Make env training
Logging to /tmp/openai-2020-06-28-08-25-06-395323
Creating dummy env object to get spaces
Updates 10, training timesteps 880, FPS 114
Last 100 training episodes: mean/median reward -62.28/-77.38, min/max -100.0/6.3
Policy entropy: 2.280, Critic Loss: 0.227, Actor Loss -0.081

Updates 20, training timesteps 1680, FPS 176
Last 100 training episodes: mean/median reward -62.50/-73.51, min/max -100.0/3.1
Policy entropy: 2.283, Critic Loss: 0.181, Actor Loss 0.621

Updates 30, training timesteps 2480, FPS 220
Last 100 training episodes: mean/median reward -59.08/-69.83, min/max -100.0/0.0
Policy entropy: 2.292, Critic Loss: 0.181, Actor Loss 0.357

Updates 40, training timesteps 3280, FPS 252
Last 100 training episodes: mean/median reward -56.26/-68.00, min/max -100.0/0.0
Policy entropy: 2.291, Critic Loss: 0.290, Actor Loss 0.160

Updates 50, training timesteps 4080, FPS 277
Last 100 training episodes: mean/median reward -63.98/-77.38, min/max -100.0/0.0
Policy entropy: 2.274, Critic Loss: 0.156, Actor Loss -0.231

Updates 60, training timesteps 4880, FPS 297
Last 100 training episodes: mean/median reward -57.12/-66.34, min/max -100.0/4.9
Policy entropy: 2.284, Critic Loss: 0.284, Actor Loss -0.260

Updates 70, training timesteps 5680, FPS 314
Last 100 training episodes: mean/median reward -60.84/-71.67, min/max -100.0/0.0
Policy entropy: 2.293, Critic Loss: 0.207, Actor Loss 0.286

Updates 80, training timesteps 6480, FPS 329
Last 100 training episodes: mean/median reward -54.10/-58.38, min/max -100.0/3.2
Policy entropy: 2.295, Critic Loss: 0.225, Actor Loss -0.301

Updates 90, training timesteps 7280, FPS 339
Last 100 training episodes: mean/median reward -57.77/-69.83, min/max -100.0/6.3
Policy entropy: 2.291, Critic Loss: 0.233, Actor Loss 0.151

Updates 100, training timesteps 8080, FPS 350
Last 100 training episodes: mean/median reward -51.09/-58.38, min/max -100.0/5.4
Policy entropy: 2.295, Critic Loss: 0.299, Actor Loss -0.077

Updates 110, training timesteps 8880, FPS 358
Last 100 training episodes: mean/median reward -63.72/-77.38, min/max -100.0/0.0
Policy entropy: 2.296, Critic Loss: 0.169, Actor Loss -0.054

Updates 120, training timesteps 9680, FPS 365
Last 100 training episodes: mean/median reward -59.07/-73.51, min/max -100.0/4.9
Policy entropy: 2.297, Critic Loss: 0.240, Actor Loss 0.010

Updates 130, training timesteps 10480, FPS 371
Last 100 training episodes: mean/median reward -59.25/-69.83, min/max -100.0/4.9
Policy entropy: 2.295, Critic Loss: 0.213, Actor Loss -0.141

Updates 140, training timesteps 11280, FPS 377
Last 100 training episodes: mean/median reward -56.36/-63.02, min/max -100.0/5.7
Policy entropy: 2.296, Critic Loss: 0.177, Actor Loss 0.071

Updates 150, training timesteps 12080, FPS 383
Last 100 training episodes: mean/median reward -61.53/-68.09, min/max -100.0/3.4
Policy entropy: 2.288, Critic Loss: 0.187, Actor Loss 0.042

Updates 160, training timesteps 12880, FPS 387
Last 100 training episodes: mean/median reward -60.53/-69.83, min/max -100.0/0.0
Policy entropy: 2.296, Critic Loss: 0.197, Actor Loss 0.020

Updates 170, training timesteps 13680, FPS 391
Last 100 training episodes: mean/median reward -60.00/-71.67, min/max -100.0/0.0
Policy entropy: 2.297, Critic Loss: 0.065, Actor Loss 0.046

Updates 180, training timesteps 14480, FPS 395
Last 100 training episodes: mean/median reward -59.65/-69.83, min/max -100.0/0.0
Policy entropy: 2.293, Critic Loss: 0.183, Actor Loss -0.116

Updates 190, training timesteps 15280, FPS 398
Last 100 training episodes: mean/median reward -56.92/-71.67, min/max -100.0/4.9
Policy entropy: 2.294, Critic Loss: 0.119, Actor Loss -0.155

Updates 200, training timesteps 16080, FPS 402
Last 100 training episodes: mean/median reward -57.38/-69.83, min/max -100.0/4.6
Policy entropy: 2.296, Critic Loss: 0.105, Actor Loss 0.329

Updates 210, training timesteps 16880, FPS 405
Last 100 training episodes: mean/median reward -57.40/-68.09, min/max -100.0/5.4
Policy entropy: 2.298, Critic Loss: 0.321, Actor Loss -0.292

Updates 220, training timesteps 17680, FPS 407
Last 100 training episodes: mean/median reward -56.29/-66.34, min/max -100.0/0.0
Policy entropy: 2.295, Critic Loss: 0.083, Actor Loss 0.192

Updates 230, training timesteps 18480, FPS 409
Last 100 training episodes: mean/median reward -58.60/-69.83, min/max -100.0/5.1
Policy entropy: 2.296, Critic Loss: 0.169, Actor Loss 0.297

Updates 240, training timesteps 19280, FPS 411
Last 100 training episodes: mean/median reward -62.07/-73.51, min/max -100.0/11.4
Policy entropy: 2.295, Critic Loss: 0.094, Actor Loss 0.036

Updates 250, training timesteps 20080, FPS 412
Last 100 training episodes: mean/median reward -60.40/-69.83, min/max -100.0/0.0
Policy entropy: 2.294, Critic Loss: 0.124, Actor Loss -0.015

Updates 260, training timesteps 20880, FPS 414
Last 100 training episodes: mean/median reward -57.90/-71.67, min/max -100.0/1.7
Policy entropy: 2.294, Critic Loss: 0.065, Actor Loss 0.189

Updates 270, training timesteps 21680, FPS 417
Last 100 training episodes: mean/median reward -57.85/-66.34, min/max -100.0/0.0
Policy entropy: 2.294, Critic Loss: 0.107, Actor Loss -0.096

Updates 280, training timesteps 22480, FPS 418
Last 100 training episodes: mean/median reward -51.75/-59.87, min/max -100.0/5.8
Policy entropy: 2.298, Critic Loss: 0.128, Actor Loss 0.157

Updates 290, training timesteps 23280, FPS 420
Last 100 training episodes: mean/median reward -54.23/-63.02, min/max -100.0/6.3
Policy entropy: 2.294, Critic Loss: 0.114, Actor Loss -0.039

Updates 300, training timesteps 24080, FPS 421
Last 100 training episodes: mean/median reward -64.36/-73.79, min/max -100.0/5.6
Policy entropy: 2.295, Critic Loss: 0.054, Actor Loss 0.302

Updates 310, training timesteps 24880, FPS 423
Last 100 training episodes: mean/median reward -63.26/-73.51, min/max -100.0/5.4
Policy entropy: 2.292, Critic Loss: 0.160, Actor Loss 0.305

Updates 320, training timesteps 25680, FPS 424
Last 100 training episodes: mean/median reward -62.38/-77.38, min/max -100.0/4.2
Policy entropy: 2.295, Critic Loss: 0.214, Actor Loss 0.250

Updates 330, training timesteps 26480, FPS 425
Last 100 training episodes: mean/median reward -59.47/-66.34, min/max -100.0/0.0
Policy entropy: 2.296, Critic Loss: 0.136, Actor Loss 0.072

Updates 340, training timesteps 27280, FPS 426
Last 100 training episodes: mean/median reward -56.80/-69.83, min/max -100.0/7.4
Policy entropy: 2.296, Critic Loss: 0.178, Actor Loss -0.033

Updates 350, training timesteps 28080, FPS 427
Last 100 training episodes: mean/median reward -53.55/-63.02, min/max -100.0/5.4
Policy entropy: 2.296, Critic Loss: 0.189, Actor Loss 0.548

Updates 360, training timesteps 28880, FPS 428
Last 100 training episodes: mean/median reward -61.30/-75.44, min/max -100.0/3.4
Policy entropy: 2.291, Critic Loss: 0.084, Actor Loss 0.273

Updates 370, training timesteps 29680, FPS 429
Last 100 training episodes: mean/median reward -61.52/-75.44, min/max -100.0/0.0
Policy entropy: 2.294, Critic Loss: 0.083, Actor Loss 0.053

Updates 380, training timesteps 30480, FPS 430
Last 100 training episodes: mean/median reward -56.32/-69.83, min/max -100.0/4.2
Policy entropy: 2.295, Critic Loss: 0.175, Actor Loss -0.199

Updates 390, training timesteps 31280, FPS 431
Last 100 training episodes: mean/median reward -55.41/-63.02, min/max -100.0/0.0
Policy entropy: 2.294, Critic Loss: 0.129, Actor Loss -0.238

Updates 400, training timesteps 32080, FPS 432
Last 100 training episodes: mean/median reward -51.45/-59.87, min/max -100.0/5.7
Policy entropy: 2.289, Critic Loss: 0.205, Actor Loss -0.211

Updates 410, training timesteps 32880, FPS 434
Last 100 training episodes: mean/median reward -61.77/-73.51, min/max -100.0/0.0
Policy entropy: 2.295, Critic Loss: 0.117, Actor Loss 0.077

Updates 420, training timesteps 33680, FPS 434
Last 100 training episodes: mean/median reward -60.51/-73.51, min/max -100.0/3.6
Policy entropy: 2.293, Critic Loss: 0.147, Actor Loss -0.056

Updates 430, training timesteps 34480, FPS 434
Last 100 training episodes: mean/median reward -52.80/-63.02, min/max -100.0/0.0
Policy entropy: 2.296, Critic Loss: 0.084, Actor Loss 0.012

Updates 440, training timesteps 35280, FPS 435
Last 100 training episodes: mean/median reward -58.91/-69.83, min/max -100.0/5.7
Policy entropy: 2.295, Critic Loss: 0.307, Actor Loss 0.772

Updates 450, training timesteps 36080, FPS 436
Last 100 training episodes: mean/median reward -54.27/-69.83, min/max -100.0/6.3
Policy entropy: 2.295, Critic Loss: 0.145, Actor Loss -0.153

Updates 460, training timesteps 36880, FPS 436
Last 100 training episodes: mean/median reward -56.95/-73.51, min/max -100.0/4.4
Policy entropy: 2.298, Critic Loss: 0.135, Actor Loss -0.249

Updates 470, training timesteps 37680, FPS 436
Last 100 training episodes: mean/median reward -63.99/-73.51, min/max -100.0/5.1
Policy entropy: 2.293, Critic Loss: 0.315, Actor Loss -0.629

Updates 480, training timesteps 38480, FPS 438
Last 100 training episodes: mean/median reward -60.90/-69.83, min/max -100.0/7.0
Policy entropy: 2.293, Critic Loss: 0.108, Actor Loss 0.006

Updates 490, training timesteps 39280, FPS 438
Last 100 training episodes: mean/median reward -62.25/-73.51, min/max -100.0/6.4
Policy entropy: 2.296, Critic Loss: 0.118, Actor Loss -0.550

Updates 500, training timesteps 40080, FPS 439
Last 100 training episodes: mean/median reward -57.62/-73.51, min/max -100.0/6.0
Policy entropy: 2.295, Critic Loss: 0.299, Actor Loss -0.308

Updates 510, training timesteps 40880, FPS 439
Last 100 training episodes: mean/median reward -59.11/-69.83, min/max -100.0/0.0
Policy entropy: 2.296, Critic Loss: 0.105, Actor Loss -0.227

Updates 520, training timesteps 41680, FPS 440
Last 100 training episodes: mean/median reward -57.78/-73.51, min/max -100.0/5.4
Policy entropy: 2.295, Critic Loss: 0.129, Actor Loss 0.417

Updates 530, training timesteps 42480, FPS 440
Last 100 training episodes: mean/median reward -57.22/-71.67, min/max -100.0/4.0
Policy entropy: 2.294, Critic Loss: 0.115, Actor Loss 0.161

Updates 540, training timesteps 43280, FPS 441
Last 100 training episodes: mean/median reward -59.93/-73.51, min/max -100.0/6.0
Policy entropy: 2.297, Critic Loss: 0.181, Actor Loss -0.564

Updates 550, training timesteps 44080, FPS 442
Last 100 training episodes: mean/median reward -60.43/-71.67, min/max -100.0/6.0
Policy entropy: 2.291, Critic Loss: 0.065, Actor Loss -0.132

Updates 560, training timesteps 44880, FPS 442
Last 100 training episodes: mean/median reward -58.56/-69.83, min/max -100.0/12.8
Policy entropy: 2.294, Critic Loss: 0.232, Actor Loss -0.580

Updates 570, training timesteps 45680, FPS 443
Last 100 training episodes: mean/median reward -54.46/-66.34, min/max -100.0/9.3
Policy entropy: 2.291, Critic Loss: 0.100, Actor Loss -0.153

Updates 580, training timesteps 46480, FPS 444
Last 100 training episodes: mean/median reward -62.29/-77.38, min/max -100.0/6.0
Policy entropy: 2.294, Critic Loss: 0.278, Actor Loss 0.341

Updates 590, training timesteps 47280, FPS 444
Last 100 training episodes: mean/median reward -58.90/-69.83, min/max -100.0/12.4
Policy entropy: 2.286, Critic Loss: 0.270, Actor Loss -0.295

Updates 600, training timesteps 48080, FPS 444
Last 100 training episodes: mean/median reward -56.39/-66.34, min/max -100.0/10.1
Policy entropy: 2.292, Critic Loss: 0.212, Actor Loss -0.611

Updates 610, training timesteps 48880, FPS 445
Last 100 training episodes: mean/median reward -59.62/-69.83, min/max -100.0/7.6
Policy entropy: 2.294, Critic Loss: 0.305, Actor Loss -0.235

Updates 620, training timesteps 49680, FPS 445
Last 100 training episodes: mean/median reward -56.51/-69.83, min/max -100.0/10.8
Policy entropy: 2.294, Critic Loss: 0.100, Actor Loss 0.014

Updates 630, training timesteps 50480, FPS 446
Last 100 training episodes: mean/median reward -57.72/-73.51, min/max -100.0/4.9
Policy entropy: 2.296, Critic Loss: 0.130, Actor Loss 0.106

Updates 640, training timesteps 51280, FPS 446
Last 100 training episodes: mean/median reward -54.07/-69.83, min/max -100.0/15.4
Policy entropy: 2.293, Critic Loss: 0.212, Actor Loss -0.602

Updates 650, training timesteps 52080, FPS 446
Last 100 training episodes: mean/median reward -59.34/-73.51, min/max -100.0/8.0
Policy entropy: 2.294, Critic Loss: 0.064, Actor Loss 0.073

Updates 660, training timesteps 52880, FPS 446
Last 100 training episodes: mean/median reward -59.86/-73.51, min/max -100.0/5.7
Policy entropy: 2.292, Critic Loss: 0.152, Actor Loss 0.097

Updates 670, training timesteps 53680, FPS 446
Last 100 training episodes: mean/median reward -54.85/-64.68, min/max -100.0/5.5
Policy entropy: 2.292, Critic Loss: 0.056, Actor Loss 0.043

Updates 680, training timesteps 54480, FPS 446
Last 100 training episodes: mean/median reward -55.81/-69.83, min/max -100.0/5.7
Policy entropy: 2.293, Critic Loss: 0.176, Actor Loss -0.071

Updates 690, training timesteps 55280, FPS 447
Last 100 training episodes: mean/median reward -51.85/-55.46, min/max -100.0/11.8
Policy entropy: 2.293, Critic Loss: 0.193, Actor Loss -0.318

Updates 700, training timesteps 56080, FPS 447
Last 100 training episodes: mean/median reward -61.56/-69.83, min/max -100.0/0.0
Policy entropy: 2.294, Critic Loss: 0.098, Actor Loss 0.190

Updates 710, training timesteps 56880, FPS 447
Last 100 training episodes: mean/median reward -63.69/-69.83, min/max -100.0/0.0
Policy entropy: 2.297, Critic Loss: 0.134, Actor Loss -0.364

Updates 720, training timesteps 57680, FPS 447
Last 100 training episodes: mean/median reward -58.92/-69.83, min/max -100.0/0.0
Policy entropy: 2.297, Critic Loss: 0.115, Actor Loss -0.038

Updates 730, training timesteps 58480, FPS 448
Last 100 training episodes: mean/median reward -51.07/-54.04, min/max -100.0/2.4
Policy entropy: 2.296, Critic Loss: 0.086, Actor Loss 0.137

Updates 740, training timesteps 59280, FPS 448
Last 100 training episodes: mean/median reward -57.26/-69.83, min/max -100.0/5.7
Policy entropy: 2.295, Critic Loss: 0.146, Actor Loss -0.011

Updates 750, training timesteps 60080, FPS 449
Last 100 training episodes: mean/median reward -50.39/-59.87, min/max -100.0/13.1
Policy entropy: 2.293, Critic Loss: 0.201, Actor Loss 0.078

Updates 760, training timesteps 60880, FPS 449
Last 100 training episodes: mean/median reward -48.69/-59.87, min/max -100.0/4.9
Policy entropy: 2.293, Critic Loss: 0.458, Actor Loss -1.043

Updates 770, training timesteps 61680, FPS 449
Last 100 training episodes: mean/median reward -59.06/-68.09, min/max -100.0/13.7
Policy entropy: 2.294, Critic Loss: 0.196, Actor Loss -0.203

Updates 780, training timesteps 62480, FPS 450
Last 100 training episodes: mean/median reward -61.56/-73.51, min/max -100.0/2.8
Policy entropy: 2.294, Critic Loss: 0.222, Actor Loss -0.399

Updates 790, training timesteps 63280, FPS 450
Last 100 training episodes: mean/median reward -67.09/-77.38, min/max -100.0/0.0
Policy entropy: 2.295, Critic Loss: 0.154, Actor Loss -0.361

Updates 800, training timesteps 64080, FPS 450
Last 100 training episodes: mean/median reward -61.51/-73.51, min/max -100.0/0.0
Policy entropy: 2.294, Critic Loss: 0.213, Actor Loss -0.234

Updates 810, training timesteps 64880, FPS 451
Last 100 training episodes: mean/median reward -52.66/-63.02, min/max -100.0/3.1
Policy entropy: 2.291, Critic Loss: 0.081, Actor Loss 0.123

Updates 820, training timesteps 65680, FPS 451
Last 100 training episodes: mean/median reward -56.76/-63.02, min/max -100.0/5.4
Policy entropy: 2.294, Critic Loss: 0.203, Actor Loss -0.081

Updates 830, training timesteps 66480, FPS 451
Last 100 training episodes: mean/median reward -59.62/-73.51, min/max -100.0/4.6
Policy entropy: 2.294, Critic Loss: 0.127, Actor Loss -0.450

Updates 840, training timesteps 67280, FPS 451
Last 100 training episodes: mean/median reward -58.68/-69.83, min/max -100.0/5.1
Policy entropy: 2.295, Critic Loss: 0.113, Actor Loss 0.028

Updates 850, training timesteps 68080, FPS 451
Last 100 training episodes: mean/median reward -65.37/-73.51, min/max -100.0/0.0
Policy entropy: 2.295, Critic Loss: 0.114, Actor Loss -0.130

Updates 860, training timesteps 68880, FPS 451
Last 100 training episodes: mean/median reward -64.90/-77.38, min/max -100.0/5.7
Policy entropy: 2.295, Critic Loss: 0.221, Actor Loss -0.719

Updates 870, training timesteps 69680, FPS 451
Last 100 training episodes: mean/median reward -55.24/-71.67, min/max -100.0/0.0
Policy entropy: 2.293, Critic Loss: 0.070, Actor Loss 0.077

Updates 880, training timesteps 70480, FPS 452
Last 100 training episodes: mean/median reward -57.89/-69.83, min/max -100.0/7.5
Policy entropy: 2.293, Critic Loss: 0.125, Actor Loss 0.152

Updates 890, training timesteps 71280, FPS 452
Last 100 training episodes: mean/median reward -57.70/-68.09, min/max -100.0/4.8
Policy entropy: 2.291, Critic Loss: 0.158, Actor Loss -0.004

Updates 900, training timesteps 72080, FPS 452
Last 100 training episodes: mean/median reward -53.15/-55.46, min/max -100.0/9.5
Policy entropy: 2.295, Critic Loss: 0.317, Actor Loss -0.610

Updates 910, training timesteps 72880, FPS 453
Last 100 training episodes: mean/median reward -57.45/-69.83, min/max -100.0/6.4
Policy entropy: 2.293, Critic Loss: 0.198, Actor Loss -0.486

Updates 920, training timesteps 73680, FPS 453
Last 100 training episodes: mean/median reward -58.04/-71.67, min/max -100.0/6.3
Policy entropy: 2.294, Critic Loss: 0.212, Actor Loss -0.243

Updates 930, training timesteps 74480, FPS 454
Last 100 training episodes: mean/median reward -59.69/-73.51, min/max -100.0/5.7
Policy entropy: 2.292, Critic Loss: 0.179, Actor Loss -0.063

Updates 940, training timesteps 75280, FPS 454
Last 100 training episodes: mean/median reward -58.04/-71.67, min/max -100.0/1.7
Policy entropy: 2.295, Critic Loss: 0.171, Actor Loss -0.007

Updates 950, training timesteps 76080, FPS 454
Last 100 training episodes: mean/median reward -62.11/-75.44, min/max -100.0/5.4
Policy entropy: 2.293, Critic Loss: 0.150, Actor Loss 0.040

Updates 960, training timesteps 76880, FPS 454
Last 100 training episodes: mean/median reward -56.67/-73.51, min/max -100.0/12.3
Policy entropy: 2.293, Critic Loss: 0.247, Actor Loss -0.302

Updates 970, training timesteps 77680, FPS 454
Last 100 training episodes: mean/median reward -53.93/-63.02, min/max -100.0/4.9
Policy entropy: 2.296, Critic Loss: 0.149, Actor Loss -0.282

Updates 980, training timesteps 78480, FPS 454
Last 100 training episodes: mean/median reward -53.31/-62.74, min/max -100.0/5.7
Policy entropy: 2.294, Critic Loss: 0.077, Actor Loss -0.124

Updates 990, training timesteps 79280, FPS 455
Last 100 training episodes: mean/median reward -48.85/-64.60, min/max -100.0/3.6
Policy entropy: 2.292, Critic Loss: 0.104, Actor Loss 0.044

Updates 1000, training timesteps 80080, FPS 455
Last 100 training episodes: mean/median reward -59.22/-66.34, min/max -100.0/7.4
Policy entropy: 2.294, Critic Loss: 0.163, Actor Loss 0.195

Updates 1010, training timesteps 80880, FPS 455
Last 100 training episodes: mean/median reward -62.06/-73.51, min/max -100.0/3.2
Policy entropy: 2.293, Critic Loss: 0.072, Actor Loss 0.272

Updates 1020, training timesteps 81680, FPS 455
Last 100 training episodes: mean/median reward -49.80/-60.34, min/max -100.0/7.4
Policy entropy: 2.293, Critic Loss: 0.252, Actor Loss -0.526

Updates 1030, training timesteps 82480, FPS 455
Last 100 training episodes: mean/median reward -51.18/-58.38, min/max -100.0/3.6
Policy entropy: 2.296, Critic Loss: 0.149, Actor Loss -0.293

Updates 1040, training timesteps 83280, FPS 455
Last 100 training episodes: mean/median reward -58.84/-64.68, min/max -100.0/0.0
Policy entropy: 2.295, Critic Loss: 0.072, Actor Loss -0.023

Updates 1050, training timesteps 84080, FPS 456
Last 100 training episodes: mean/median reward -57.89/-66.34, min/max -100.0/5.4
Policy entropy: 2.296, Critic Loss: 0.203, Actor Loss -0.489

Updates 1060, training timesteps 84880, FPS 456
Last 100 training episodes: mean/median reward -51.16/-59.95, min/max -100.0/2.8
Policy entropy: 2.293, Critic Loss: 0.070, Actor Loss 0.020

Updates 1070, training timesteps 85680, FPS 456
Last 100 training episodes: mean/median reward -57.75/-66.34, min/max -100.0/7.6
Policy entropy: 2.296, Critic Loss: 0.164, Actor Loss -0.333

Updates 1080, training timesteps 86480, FPS 456
Last 100 training episodes: mean/median reward -60.34/-73.51, min/max -100.0/9.9
Policy entropy: 2.286, Critic Loss: 0.082, Actor Loss 0.059

Updates 1090, training timesteps 87280, FPS 456
Last 100 training episodes: mean/median reward -59.93/-73.51, min/max -100.0/6.6
Policy entropy: 2.294, Critic Loss: 0.157, Actor Loss 0.158

Updates 1100, training timesteps 88080, FPS 456
Last 100 training episodes: mean/median reward -61.54/-73.51, min/max -100.0/6.3
Policy entropy: 2.295, Critic Loss: 0.179, Actor Loss -0.308

Updates 1110, training timesteps 88880, FPS 456
Last 100 training episodes: mean/median reward -58.49/-66.34, min/max -100.0/4.2
Policy entropy: 2.293, Critic Loss: 0.265, Actor Loss -0.365

Updates 1120, training timesteps 89680, FPS 456
Last 100 training episodes: mean/median reward -64.28/-77.38, min/max -100.0/4.6
Policy entropy: 2.291, Critic Loss: 0.129, Actor Loss 0.153

Updates 1130, training timesteps 90480, FPS 456
Last 100 training episodes: mean/median reward -51.25/-58.38, min/max -100.0/3.2
Policy entropy: 2.295, Critic Loss: 0.119, Actor Loss -0.326

Updates 1140, training timesteps 91280, FPS 456
Last 100 training episodes: mean/median reward -59.56/-75.44, min/max -100.0/14.0
Policy entropy: 2.292, Critic Loss: 0.148, Actor Loss -0.265

Updates 1150, training timesteps 92080, FPS 457
Last 100 training episodes: mean/median reward -59.50/-69.83, min/max -100.0/0.0
Policy entropy: 2.293, Critic Loss: 0.051, Actor Loss 0.304

Updates 1160, training timesteps 92880, FPS 457
Last 100 training episodes: mean/median reward -55.63/-68.09, min/max -100.0/2.8
Policy entropy: 2.295, Critic Loss: 0.144, Actor Loss -0.439

Updates 1170, training timesteps 93680, FPS 457
Last 100 training episodes: mean/median reward -65.55/-73.51, min/max -100.0/9.5
Policy entropy: 2.290, Critic Loss: 0.311, Actor Loss -0.492

Updates 1180, training timesteps 94480, FPS 457
Last 100 training episodes: mean/median reward -56.19/-68.09, min/max -100.0/3.1
Policy entropy: 2.293, Critic Loss: 0.053, Actor Loss 0.174

Updates 1190, training timesteps 95280, FPS 458
Last 100 training episodes: mean/median reward -52.29/-61.45, min/max -100.0/11.2
Policy entropy: 2.295, Critic Loss: 0.205, Actor Loss -0.578

Updates 1200, training timesteps 96080, FPS 458
Last 100 training episodes: mean/median reward -51.59/-63.02, min/max -100.0/5.4
Policy entropy: 2.292, Critic Loss: 0.238, Actor Loss -0.206

Updates 1210, training timesteps 96880, FPS 458
Last 100 training episodes: mean/median reward -60.09/-73.51, min/max -100.0/7.7
Policy entropy: 2.293, Critic Loss: 0.273, Actor Loss -0.528

Updates 1220, training timesteps 97680, FPS 458
Last 100 training episodes: mean/median reward -57.30/-69.83, min/max -100.0/6.3
Policy entropy: 2.290, Critic Loss: 0.126, Actor Loss -0.054

Updates 1230, training timesteps 98480, FPS 459
Last 100 training episodes: mean/median reward -61.13/-69.83, min/max -100.0/4.6
Policy entropy: 2.291, Critic Loss: 0.104, Actor Loss 0.105

Updates 1240, training timesteps 99280, FPS 459
Last 100 training episodes: mean/median reward -50.69/-56.88, min/max -100.0/0.0
Policy entropy: 2.293, Critic Loss: 0.130, Actor Loss -0.054

Updates 1250, training timesteps 100080, FPS 459
Last 100 training episodes: mean/median reward -56.00/-69.83, min/max -100.0/3.4
Policy entropy: 2.294, Critic Loss: 0.117, Actor Loss 0.023

Updates 1260, training timesteps 100880, FPS 459
Last 100 training episodes: mean/median reward -61.46/-77.38, min/max -100.0/6.0
Policy entropy: 2.293, Critic Loss: 0.175, Actor Loss -0.171

Updates 1270, training timesteps 101680, FPS 459
Last 100 training episodes: mean/median reward -58.65/-69.83, min/max -100.0/4.8
Policy entropy: 2.295, Critic Loss: 0.115, Actor Loss 0.006

Updates 1280, training timesteps 102480, FPS 459
Last 100 training episodes: mean/median reward -53.32/-66.34, min/max -100.0/0.0
Policy entropy: 2.295, Critic Loss: 0.192, Actor Loss -0.127

Updates 1290, training timesteps 103280, FPS 459
Last 100 training episodes: mean/median reward -60.32/-73.51, min/max -100.0/5.4
Policy entropy: 2.296, Critic Loss: 0.181, Actor Loss 0.041

Updates 1300, training timesteps 104080, FPS 458
Last 100 training episodes: mean/median reward -62.90/-77.38, min/max -100.0/4.6
Policy entropy: 2.296, Critic Loss: 0.141, Actor Loss 0.363

Updates 1310, training timesteps 104880, FPS 459
Last 100 training episodes: mean/median reward -60.82/-77.38, min/max -100.0/6.3
Policy entropy: 2.295, Critic Loss: 0.123, Actor Loss 0.354

Updates 1320, training timesteps 105680, FPS 459
Last 100 training episodes: mean/median reward -54.67/-68.09, min/max -100.0/5.7
Policy entropy: 2.292, Critic Loss: 0.131, Actor Loss 0.247

Updates 1330, training timesteps 106480, FPS 459
Last 100 training episodes: mean/median reward -59.30/-73.51, min/max -100.0/10.1
Policy entropy: 2.294, Critic Loss: 0.116, Actor Loss 0.367

Updates 1340, training timesteps 107280, FPS 459
Last 100 training episodes: mean/median reward -55.20/-64.68, min/max -100.0/5.7
Policy entropy: 2.293, Critic Loss: 0.101, Actor Loss -0.019

Updates 1350, training timesteps 108080, FPS 459
Last 100 training episodes: mean/median reward -57.77/-69.83, min/max -100.0/0.9
Policy entropy: 2.295, Critic Loss: 0.274, Actor Loss -0.020

Updates 1360, training timesteps 108880, FPS 459
Last 100 training episodes: mean/median reward -60.01/-68.09, min/max -100.0/5.7
Policy entropy: 2.295, Critic Loss: 0.168, Actor Loss -0.461

Updates 1370, training timesteps 109680, FPS 460
Last 100 training episodes: mean/median reward -65.34/-77.38, min/max -100.0/2.9
Policy entropy: 2.294, Critic Loss: 0.099, Actor Loss -0.158

Updates 1380, training timesteps 110480, FPS 460
Last 100 training episodes: mean/median reward -57.97/-73.51, min/max -100.0/0.0
Policy entropy: 2.291, Critic Loss: 0.137, Actor Loss -0.209

Updates 1390, training timesteps 111280, FPS 460
Last 100 training episodes: mean/median reward -56.22/-63.02, min/max -100.0/0.0
Policy entropy: 2.295, Critic Loss: 0.155, Actor Loss -0.524

Updates 1400, training timesteps 112080, FPS 460
Last 100 training episodes: mean/median reward -56.59/-66.34, min/max -100.0/5.4
Policy entropy: 2.295, Critic Loss: 0.099, Actor Loss -0.152

Updates 1410, training timesteps 112880, FPS 460
Last 100 training episodes: mean/median reward -59.72/-73.51, min/max -100.0/8.6
Policy entropy: 2.290, Critic Loss: 0.099, Actor Loss 0.065

Updates 1420, training timesteps 113680, FPS 460
Last 100 training episodes: mean/median reward -52.92/-66.34, min/max -100.0/4.0
Policy entropy: 2.291, Critic Loss: 0.248, Actor Loss -0.120

Updates 1430, training timesteps 114480, FPS 460
Last 100 training episodes: mean/median reward -53.27/-63.02, min/max -100.0/1.7
Policy entropy: 2.294, Critic Loss: 0.170, Actor Loss -0.239

Updates 1440, training timesteps 115280, FPS 460
Last 100 training episodes: mean/median reward -49.56/-59.87, min/max -100.0/6.6
Policy entropy: 2.293, Critic Loss: 0.109, Actor Loss 0.174

Updates 1450, training timesteps 116080, FPS 460
Last 100 training episodes: mean/median reward -50.41/-54.04, min/max -100.0/3.6
Policy entropy: 2.293, Critic Loss: 0.171, Actor Loss 0.273

Updates 1460, training timesteps 116880, FPS 460
Last 100 training episodes: mean/median reward -55.07/-69.83, min/max -100.0/5.4
Policy entropy: 2.291, Critic Loss: 0.105, Actor Loss 0.323

Updates 1470, training timesteps 117680, FPS 460
Last 100 training episodes: mean/median reward -61.37/-73.51, min/max -100.0/4.9
Policy entropy: 2.292, Critic Loss: 0.061, Actor Loss 0.057

Updates 1480, training timesteps 118480, FPS 460
Last 100 training episodes: mean/median reward -62.48/-73.51, min/max -100.0/4.9
Policy entropy: 2.292, Critic Loss: 0.084, Actor Loss -0.044

Updates 1490, training timesteps 119280, FPS 460
Last 100 training episodes: mean/median reward -58.64/-69.83, min/max -100.0/0.0
Policy entropy: 2.296, Critic Loss: 0.104, Actor Loss -0.139

Updates 1500, training timesteps 120080, FPS 460
Last 100 training episodes: mean/median reward -63.66/-75.44, min/max -100.0/3.2
Policy entropy: 2.297, Critic Loss: 0.104, Actor Loss 0.109

Updates 1510, training timesteps 120880, FPS 460
Last 100 training episodes: mean/median reward -58.73/-66.34, min/max -100.0/3.6
Policy entropy: 2.290, Critic Loss: 0.114, Actor Loss -0.011

Updates 1520, training timesteps 121680, FPS 460
Last 100 training episodes: mean/median reward -58.72/-69.83, min/max -100.0/5.4
Policy entropy: 2.294, Critic Loss: 0.129, Actor Loss -0.026

Updates 1530, training timesteps 122480, FPS 460
Last 100 training episodes: mean/median reward -60.63/-73.51, min/max -100.0/7.0
Policy entropy: 2.295, Critic Loss: 0.126, Actor Loss -0.172

Updates 1540, training timesteps 123280, FPS 461
Last 100 training episodes: mean/median reward -48.85/-60.19, min/max -100.0/8.2
Policy entropy: 2.296, Critic Loss: 0.200, Actor Loss -0.152

Updates 1550, training timesteps 124080, FPS 461
Last 100 training episodes: mean/median reward -59.73/-73.51, min/max -100.0/1.8
Policy entropy: 2.291, Critic Loss: 0.128, Actor Loss 0.136

Updates 1560, training timesteps 124880, FPS 461
Last 100 training episodes: mean/median reward -61.18/-73.51, min/max -100.0/2.4
Policy entropy: 2.294, Critic Loss: 0.139, Actor Loss 0.283

Updates 1570, training timesteps 125680, FPS 461
Last 100 training episodes: mean/median reward -54.86/-71.67, min/max -100.0/13.0
Policy entropy: 2.292, Critic Loss: 0.143, Actor Loss -0.082

Updates 1580, training timesteps 126480, FPS 461
Last 100 training episodes: mean/median reward -59.54/-71.67, min/max -100.0/5.1
Policy entropy: 2.291, Critic Loss: 0.243, Actor Loss -0.334

Updates 1590, training timesteps 127280, FPS 462
Last 100 training episodes: mean/median reward -44.95/-49.31, min/max -100.0/6.0
Policy entropy: 2.293, Critic Loss: 0.158, Actor Loss 0.689

Updates 1600, training timesteps 128080, FPS 462
Last 100 training episodes: mean/median reward -48.16/-52.69, min/max -100.0/6.3
Policy entropy: 2.290, Critic Loss: 0.301, Actor Loss -0.168

Updates 1610, training timesteps 128880, FPS 462
Last 100 training episodes: mean/median reward -58.67/-71.67, min/max -100.0/4.6
Policy entropy: 2.293, Critic Loss: 0.173, Actor Loss -0.405

Updates 1620, training timesteps 129680, FPS 462
Last 100 training episodes: mean/median reward -53.33/-66.34, min/max -100.0/4.0
Policy entropy: 2.292, Critic Loss: 0.140, Actor Loss -0.326

Updates 1630, training timesteps 130480, FPS 462
Last 100 training episodes: mean/median reward -56.21/-66.34, min/max -100.0/2.6
Policy entropy: 2.293, Critic Loss: 0.118, Actor Loss 0.300

Updates 1640, training timesteps 131280, FPS 462
Last 100 training episodes: mean/median reward -58.39/-69.83, min/max -100.0/3.4
Policy entropy: 2.292, Critic Loss: 0.063, Actor Loss 0.370

Updates 1650, training timesteps 132080, FPS 462
Last 100 training episodes: mean/median reward -49.80/-56.88, min/max -100.0/7.4
Policy entropy: 2.290, Critic Loss: 0.285, Actor Loss -0.639

Updates 1660, training timesteps 132880, FPS 462
Last 100 training episodes: mean/median reward -52.13/-64.68, min/max -100.0/7.7
Policy entropy: 2.293, Critic Loss: 0.137, Actor Loss -0.045

Updates 1670, training timesteps 133680, FPS 462
Last 100 training episodes: mean/median reward -60.50/-73.51, min/max -100.0/0.0
Policy entropy: 2.293, Critic Loss: 0.114, Actor Loss -0.163

Updates 1680, training timesteps 134480, FPS 462
Last 100 training episodes: mean/median reward -52.03/-61.45, min/max -100.0/5.7
Policy entropy: 2.289, Critic Loss: 0.154, Actor Loss -0.042

Updates 1690, training timesteps 135280, FPS 462
Last 100 training episodes: mean/median reward -52.50/-59.87, min/max -100.0/5.7
Policy entropy: 2.293, Critic Loss: 0.176, Actor Loss -0.132

Updates 1700, training timesteps 136080, FPS 462
Last 100 training episodes: mean/median reward -55.37/-63.98, min/max -100.0/8.1
Policy entropy: 2.289, Critic Loss: 0.125, Actor Loss -0.192

Updates 1710, training timesteps 136880, FPS 462
Last 100 training episodes: mean/median reward -55.34/-66.34, min/max -100.0/4.9
Policy entropy: 2.291, Critic Loss: 0.086, Actor Loss 0.168

Updates 1720, training timesteps 137680, FPS 463
Last 100 training episodes: mean/median reward -50.15/-54.04, min/max -100.0/5.7
Policy entropy: 2.291, Critic Loss: 0.115, Actor Loss 0.083

Updates 1730, training timesteps 138480, FPS 463
Last 100 training episodes: mean/median reward -57.10/-68.09, min/max -100.0/4.4
Policy entropy: 2.293, Critic Loss: 0.117, Actor Loss 0.108

Updates 1740, training timesteps 139280, FPS 463
Last 100 training episodes: mean/median reward -54.18/-66.34, min/max -100.0/3.6
Policy entropy: 2.295, Critic Loss: 0.195, Actor Loss -0.435

Updates 1750, training timesteps 140080, FPS 463
Last 100 training episodes: mean/median reward -61.90/-68.09, min/max -100.0/12.8
Policy entropy: 2.293, Critic Loss: 0.066, Actor Loss 0.200

Updates 1760, training timesteps 140880, FPS 463
Last 100 training episodes: mean/median reward -62.62/-73.51, min/max -100.0/8.8
Policy entropy: 2.294, Critic Loss: 0.152, Actor Loss -0.080

Updates 1770, training timesteps 141680, FPS 462
Last 100 training episodes: mean/median reward -54.01/-63.02, min/max -100.0/5.7
Policy entropy: 2.292, Critic Loss: 0.138, Actor Loss 0.351

Updates 1780, training timesteps 142480, FPS 463
Last 100 training episodes: mean/median reward -59.73/-69.83, min/max -100.0/4.9
Policy entropy: 2.293, Critic Loss: 0.100, Actor Loss -0.038

Updates 1790, training timesteps 143280, FPS 463
Last 100 training episodes: mean/median reward -67.34/-77.38, min/max -100.0/0.0
Policy entropy: 2.294, Critic Loss: 0.055, Actor Loss 0.125

Updates 1800, training timesteps 144080, FPS 463
Last 100 training episodes: mean/median reward -57.80/-69.41, min/max -100.0/2.6
Policy entropy: 2.295, Critic Loss: 0.192, Actor Loss -0.430

Updates 1810, training timesteps 144880, FPS 463
Last 100 training episodes: mean/median reward -53.99/-63.02, min/max -100.0/3.6
Policy entropy: 2.291, Critic Loss: 0.151, Actor Loss 0.055

Updates 1820, training timesteps 145680, FPS 463
Last 100 training episodes: mean/median reward -52.70/-64.68, min/max -100.0/0.0
Policy entropy: 2.295, Critic Loss: 0.105, Actor Loss -0.033

Updates 1830, training timesteps 146480, FPS 463
Last 100 training episodes: mean/median reward -58.75/-69.83, min/max -100.0/4.6
Policy entropy: 2.295, Critic Loss: 0.177, Actor Loss 0.126

Updates 1840, training timesteps 147280, FPS 463
Last 100 training episodes: mean/median reward -57.34/-63.02, min/max -100.0/4.4
Policy entropy: 2.293, Critic Loss: 0.054, Actor Loss 0.322

Updates 1850, training timesteps 148080, FPS 463
Last 100 training episodes: mean/median reward -53.46/-63.02, min/max -100.0/10.0
Policy entropy: 2.294, Critic Loss: 0.158, Actor Loss -0.322

Updates 1860, training timesteps 148880, FPS 463
Last 100 training episodes: mean/median reward -63.07/-73.51, min/max -100.0/0.0
Policy entropy: 2.292, Critic Loss: 0.053, Actor Loss 0.301

Updates 1870, training timesteps 149680, FPS 463
Last 100 training episodes: mean/median reward -62.04/-77.38, min/max -100.0/4.0
Policy entropy: 2.293, Critic Loss: 0.111, Actor Loss -0.060

Updates 1880, training timesteps 150480, FPS 463
Last 100 training episodes: mean/median reward -63.28/-75.44, min/max -100.0/4.2
Policy entropy: 2.295, Critic Loss: 0.116, Actor Loss 0.088

Updates 1890, training timesteps 151280, FPS 463
Last 100 training episodes: mean/median reward -58.22/-69.83, min/max -100.0/8.2
Policy entropy: 2.295, Critic Loss: 0.101, Actor Loss 0.182

Updates 1900, training timesteps 152080, FPS 463
Last 100 training episodes: mean/median reward -57.15/-66.34, min/max -100.0/14.4
Policy entropy: 2.296, Critic Loss: 0.251, Actor Loss -0.602

Updates 1910, training timesteps 152880, FPS 463
Last 100 training episodes: mean/median reward -51.61/-66.34, min/max -100.0/6.0
Policy entropy: 2.294, Critic Loss: 0.158, Actor Loss -0.284

Updates 1920, training timesteps 153680, FPS 463
Last 100 training episodes: mean/median reward -58.84/-73.51, min/max -100.0/6.0
Policy entropy: 2.293, Critic Loss: 0.125, Actor Loss -0.013

Updates 1930, training timesteps 154480, FPS 463
Last 100 training episodes: mean/median reward -61.49/-69.83, min/max -100.0/3.8
Policy entropy: 2.292, Critic Loss: 0.123, Actor Loss -0.169

Updates 1940, training timesteps 155280, FPS 463
Last 100 training episodes: mean/median reward -53.42/-68.09, min/max -100.0/8.1
Policy entropy: 2.293, Critic Loss: 0.134, Actor Loss 0.288

Updates 1950, training timesteps 156080, FPS 463
Last 100 training episodes: mean/median reward -57.03/-73.51, min/max -100.0/2.3
Policy entropy: 2.292, Critic Loss: 0.065, Actor Loss 0.201

Updates 1960, training timesteps 156880, FPS 463
Last 100 training episodes: mean/median reward -57.53/-66.34, min/max -100.0/2.6
Policy entropy: 2.290, Critic Loss: 0.128, Actor Loss 0.028

Updates 1970, training timesteps 157680, FPS 464
Last 100 training episodes: mean/median reward -59.44/-69.83, min/max -100.0/8.1
Policy entropy: 2.290, Critic Loss: 0.059, Actor Loss 0.077

Updates 1980, training timesteps 158480, FPS 464
Last 100 training episodes: mean/median reward -62.05/-73.51, min/max -100.0/1.6
Policy entropy: 2.290, Critic Loss: 0.159, Actor Loss -0.095

Updates 1990, training timesteps 159280, FPS 464
Last 100 training episodes: mean/median reward -51.42/-59.87, min/max -100.0/5.7
Policy entropy: 2.294, Critic Loss: 0.104, Actor Loss -0.035

Updates 2000, training timesteps 160080, FPS 464
Last 100 training episodes: mean/median reward -54.33/-66.34, min/max -100.0/4.4
Policy entropy: 2.290, Critic Loss: 0.256, Actor Loss -0.396

Updates 2010, training timesteps 160880, FPS 464
Last 100 training episodes: mean/median reward -53.94/-63.02, min/max -100.0/7.0
Policy entropy: 2.293, Critic Loss: 0.144, Actor Loss -0.312

Updates 2020, training timesteps 161680, FPS 464
Last 100 training episodes: mean/median reward -55.38/-66.34, min/max -100.0/7.0
Policy entropy: 2.291, Critic Loss: 0.225, Actor Loss -0.343

Updates 2030, training timesteps 162480, FPS 464
Last 100 training episodes: mean/median reward -55.96/-63.02, min/max -100.0/0.3
Policy entropy: 2.295, Critic Loss: 0.080, Actor Loss 0.027

Updates 2040, training timesteps 163280, FPS 464
Last 100 training episodes: mean/median reward -60.29/-73.51, min/max -103.1/5.1
Policy entropy: 2.293, Critic Loss: 0.121, Actor Loss -0.139

Updates 2050, training timesteps 164080, FPS 464
Last 100 training episodes: mean/median reward -56.10/-69.83, min/max -100.0/7.0
Policy entropy: 2.294, Critic Loss: 0.109, Actor Loss 0.086

Updates 2060, training timesteps 164880, FPS 464
Last 100 training episodes: mean/median reward -53.70/-69.83, min/max -100.0/5.7
Policy entropy: 2.291, Critic Loss: 0.148, Actor Loss 0.084

Updates 2070, training timesteps 165680, FPS 464
Last 100 training episodes: mean/median reward -55.43/-68.09, min/max -100.0/4.4
Policy entropy: 2.291, Critic Loss: 0.099, Actor Loss 0.172

Updates 2080, training timesteps 166480, FPS 464
Last 100 training episodes: mean/median reward -56.34/-69.83, min/max -100.0/4.4
Policy entropy: 2.292, Critic Loss: 0.279, Actor Loss -0.425

Updates 2090, training timesteps 167280, FPS 464
Last 100 training episodes: mean/median reward -55.14/-66.34, min/max -100.0/7.4
Policy entropy: 2.294, Critic Loss: 0.109, Actor Loss -0.060

Updates 2100, training timesteps 168080, FPS 464
Last 100 training episodes: mean/median reward -59.45/-66.34, min/max -100.0/6.6
Policy entropy: 2.291, Critic Loss: 0.118, Actor Loss -0.003

Updates 2110, training timesteps 168880, FPS 465
Last 100 training episodes: mean/median reward -54.04/-64.68, min/max -100.0/11.9
Policy entropy: 2.292, Critic Loss: 0.136, Actor Loss -0.207

Updates 2120, training timesteps 169680, FPS 465
Last 100 training episodes: mean/median reward -55.37/-69.83, min/max -100.0/0.0
Policy entropy: 2.293, Critic Loss: 0.051, Actor Loss 0.169

Updates 2130, training timesteps 170480, FPS 465
Last 100 training episodes: mean/median reward -65.07/-69.83, min/max -100.0/0.0
Policy entropy: 2.293, Critic Loss: 0.126, Actor Loss -0.153

Updates 2140, training timesteps 171280, FPS 465
Last 100 training episodes: mean/median reward -57.04/-66.34, min/max -100.0/3.8
Policy entropy: 2.291, Critic Loss: 0.156, Actor Loss 0.004

Updates 2150, training timesteps 172080, FPS 465
Last 100 training episodes: mean/median reward -57.28/-71.67, min/max -100.0/7.0
Policy entropy: 2.292, Critic Loss: 0.106, Actor Loss 0.369

Updates 2160, training timesteps 172880, FPS 465
Last 100 training episodes: mean/median reward -58.58/-64.68, min/max -100.0/7.0
Policy entropy: 2.294, Critic Loss: 0.164, Actor Loss -0.148

Updates 2170, training timesteps 173680, FPS 465
Last 100 training episodes: mean/median reward -64.39/-77.38, min/max -100.0/5.7
Policy entropy: 2.293, Critic Loss: 0.129, Actor Loss 0.121

Updates 2180, training timesteps 174480, FPS 465
Last 100 training episodes: mean/median reward -50.17/-55.46, min/max -100.0/5.4
Policy entropy: 2.290, Critic Loss: 0.077, Actor Loss 0.037

Updates 2190, training timesteps 175280, FPS 465
Last 100 training episodes: mean/median reward -55.30/-66.34, min/max -100.0/7.7
Policy entropy: 2.291, Critic Loss: 0.101, Actor Loss -0.043

Updates 2200, training timesteps 176080, FPS 465
Last 100 training episodes: mean/median reward -58.99/-66.34, min/max -100.0/0.0
Policy entropy: 2.295, Critic Loss: 0.152, Actor Loss -0.291

Updates 2210, training timesteps 176880, FPS 465
Last 100 training episodes: mean/median reward -57.36/-69.83, min/max -100.0/7.7
Policy entropy: 2.294, Critic Loss: 0.176, Actor Loss 0.214

Updates 2220, training timesteps 177680, FPS 465
Last 100 training episodes: mean/median reward -58.37/-68.09, min/max -100.0/4.2
Policy entropy: 2.287, Critic Loss: 0.136, Actor Loss 0.252

Updates 2230, training timesteps 178480, FPS 465
Last 100 training episodes: mean/median reward -52.31/-61.45, min/max -100.0/4.9
Policy entropy: 2.294, Critic Loss: 0.105, Actor Loss -0.125

Updates 2240, training timesteps 179280, FPS 465
Last 100 training episodes: mean/median reward -51.03/-56.88, min/max -100.0/5.4
Policy entropy: 2.290, Critic Loss: 0.145, Actor Loss -0.103

Updates 2250, training timesteps 180080, FPS 465
Last 100 training episodes: mean/median reward -57.39/-68.09, min/max -100.0/4.0
Policy entropy: 2.294, Critic Loss: 0.101, Actor Loss 0.121

Updates 2260, training timesteps 180880, FPS 466
Last 100 training episodes: mean/median reward -54.98/-66.34, min/max -100.0/12.4
Policy entropy: 2.292, Critic Loss: 0.117, Actor Loss -0.034

Updates 2270, training timesteps 181680, FPS 466
Last 100 training episodes: mean/median reward -55.88/-63.02, min/max -100.0/4.6
Policy entropy: 2.294, Critic Loss: 0.123, Actor Loss 0.193

Updates 2280, training timesteps 182480, FPS 466
Last 100 training episodes: mean/median reward -55.37/-66.34, min/max -100.0/4.6
Policy entropy: 2.293, Critic Loss: 0.090, Actor Loss 0.041

Updates 2290, training timesteps 183280, FPS 466
Last 100 training episodes: mean/median reward -55.56/-68.09, min/max -100.0/6.4
Policy entropy: 2.295, Critic Loss: 0.138, Actor Loss 0.177

Updates 2300, training timesteps 184080, FPS 465
Last 100 training episodes: mean/median reward -61.67/-77.38, min/max -100.0/9.5
Policy entropy: 2.294, Critic Loss: 0.163, Actor Loss -0.087

Updates 2310, training timesteps 184880, FPS 465
Last 100 training episodes: mean/median reward -53.05/-58.64, min/max -100.0/9.5
Policy entropy: 2.287, Critic Loss: 0.165, Actor Loss 0.013

Updates 2320, training timesteps 185680, FPS 465
Last 100 training episodes: mean/median reward -49.16/-59.87, min/max -100.0/5.1
Policy entropy: 2.290, Critic Loss: 0.182, Actor Loss -0.034

Updates 2330, training timesteps 186480, FPS 466
Last 100 training episodes: mean/median reward -55.45/-69.83, min/max -100.0/14.2
Policy entropy: 2.292, Critic Loss: 0.155, Actor Loss -0.385

Updates 2340, training timesteps 187280, FPS 466
Last 100 training episodes: mean/median reward -58.18/-71.67, min/max -100.0/1.6
Policy entropy: 2.293, Critic Loss: 0.144, Actor Loss -0.189

Updates 2350, training timesteps 188080, FPS 466
Last 100 training episodes: mean/median reward -55.34/-66.34, min/max -100.0/6.0
Policy entropy: 2.294, Critic Loss: 0.093, Actor Loss 0.339

Updates 2360, training timesteps 188880, FPS 466
Last 100 training episodes: mean/median reward -58.70/-72.14, min/max -100.0/5.1
Policy entropy: 2.292, Critic Loss: 0.167, Actor Loss -0.152

Updates 2370, training timesteps 189680, FPS 466
Last 100 training episodes: mean/median reward -55.34/-59.87, min/max -100.0/5.1
Policy entropy: 2.293, Critic Loss: 0.196, Actor Loss -0.276

Updates 2380, training timesteps 190480, FPS 466
Last 100 training episodes: mean/median reward -54.32/-61.45, min/max -100.0/3.8
Policy entropy: 2.295, Critic Loss: 0.087, Actor Loss 0.045

Updates 2390, training timesteps 191280, FPS 466
Last 100 training episodes: mean/median reward -53.43/-63.02, min/max -100.0/0.0
Policy entropy: 2.293, Critic Loss: 0.083, Actor Loss 0.082

Updates 2400, training timesteps 192080, FPS 466
Last 100 training episodes: mean/median reward -54.59/-69.83, min/max -100.0/0.0
Policy entropy: 2.293, Critic Loss: 0.105, Actor Loss -0.239

Updates 2410, training timesteps 192880, FPS 465
Last 100 training episodes: mean/median reward -61.69/-77.38, min/max -100.0/8.8
Policy entropy: 2.293, Critic Loss: 0.093, Actor Loss 0.119

Updates 2420, training timesteps 193680, FPS 466
Last 100 training episodes: mean/median reward -53.38/-66.34, min/max -100.0/4.2
Policy entropy: 2.295, Critic Loss: 0.156, Actor Loss -0.147

Updates 2430, training timesteps 194480, FPS 466
Last 100 training episodes: mean/median reward -57.46/-67.79, min/max -100.0/5.1
Policy entropy: 2.293, Critic Loss: 0.070, Actor Loss 0.090

Updates 2440, training timesteps 195280, FPS 466
Last 100 training episodes: mean/median reward -52.89/-64.68, min/max -100.0/7.0
Policy entropy: 2.295, Critic Loss: 0.166, Actor Loss -0.339

Updates 2450, training timesteps 196080, FPS 466
Last 100 training episodes: mean/median reward -54.82/-68.09, min/max -100.0/6.6
Policy entropy: 2.293, Critic Loss: 0.124, Actor Loss 0.362

Updates 2460, training timesteps 196880, FPS 466
Last 100 training episodes: mean/median reward -54.07/-63.02, min/max -100.0/6.6
Policy entropy: 2.294, Critic Loss: 0.126, Actor Loss -0.345

Updates 2470, training timesteps 197680, FPS 466
Last 100 training episodes: mean/median reward -47.07/-55.46, min/max -100.0/4.0
Policy entropy: 2.293, Critic Loss: 0.120, Actor Loss 0.096

Updates 2480, training timesteps 198480, FPS 466
Last 100 training episodes: mean/median reward -56.36/-73.51, min/max -100.0/3.8
Policy entropy: 2.291, Critic Loss: 0.056, Actor Loss 0.142

Updates 2490, training timesteps 199280, FPS 466
Last 100 training episodes: mean/median reward -60.42/-69.83, min/max -100.0/9.2
Policy entropy: 2.294, Critic Loss: 0.070, Actor Loss 0.072

Updates 2500, training timesteps 200080, FPS 466
Last 100 training episodes: mean/median reward -50.04/-58.38, min/max -100.0/7.0
Policy entropy: 2.292, Critic Loss: 0.064, Actor Loss 0.095

Updates 2510, training timesteps 200880, FPS 466
Last 100 training episodes: mean/median reward -57.62/-69.83, min/max -100.0/3.1
Policy entropy: 2.290, Critic Loss: 0.110, Actor Loss 0.003

Updates 2520, training timesteps 201680, FPS 466
Last 100 training episodes: mean/median reward -55.06/-63.02, min/max -100.0/6.3
Policy entropy: 2.294, Critic Loss: 0.100, Actor Loss -0.182

Updates 2530, training timesteps 202480, FPS 466
Last 100 training episodes: mean/median reward -56.96/-66.34, min/max -100.0/3.1
Policy entropy: 2.295, Critic Loss: 0.141, Actor Loss 0.111

Updates 2540, training timesteps 203280, FPS 466
Last 100 training episodes: mean/median reward -58.47/-73.51, min/max -100.0/4.0
Policy entropy: 2.291, Critic Loss: 0.097, Actor Loss -0.058

Updates 2550, training timesteps 204080, FPS 466
Last 100 training episodes: mean/median reward -56.75/-69.83, min/max -100.0/0.0
Policy entropy: 2.295, Critic Loss: 0.132, Actor Loss 0.082

Updates 2560, training timesteps 204880, FPS 466
Last 100 training episodes: mean/median reward -57.12/-69.83, min/max -100.0/7.7
Policy entropy: 2.296, Critic Loss: 0.080, Actor Loss 0.040

Updates 2570, training timesteps 205680, FPS 466
Last 100 training episodes: mean/median reward -55.84/-66.34, min/max -100.0/3.6
Policy entropy: 2.292, Critic Loss: 0.084, Actor Loss 0.321

Updates 2580, training timesteps 206480, FPS 466
Last 100 training episodes: mean/median reward -56.19/-72.02, min/max -100.0/6.0
Policy entropy: 2.294, Critic Loss: 0.224, Actor Loss -0.266

Updates 2590, training timesteps 207280, FPS 466
Last 100 training episodes: mean/median reward -54.77/-69.83, min/max -100.0/12.5
Policy entropy: 2.294, Critic Loss: 0.097, Actor Loss -0.104

Updates 2600, training timesteps 208080, FPS 466
Last 100 training episodes: mean/median reward -55.02/-64.68, min/max -100.0/7.4
Policy entropy: 2.289, Critic Loss: 0.188, Actor Loss -0.279

Updates 2610, training timesteps 208880, FPS 466
Last 100 training episodes: mean/median reward -53.55/-66.34, min/max -100.0/1.9
Policy entropy: 2.294, Critic Loss: 0.114, Actor Loss -0.007

Updates 2620, training timesteps 209680, FPS 466
Last 100 training episodes: mean/median reward -57.59/-69.83, min/max -100.0/3.4
Policy entropy: 2.295, Critic Loss: 0.066, Actor Loss 0.071

Updates 2630, training timesteps 210480, FPS 466
Last 100 training episodes: mean/median reward -59.47/-69.83, min/max -100.0/6.6
Policy entropy: 2.292, Critic Loss: 0.111, Actor Loss -0.063

Updates 2640, training timesteps 211280, FPS 466
Last 100 training episodes: mean/median reward -62.62/-73.51, min/max -100.0/12.1
Policy entropy: 2.292, Critic Loss: 0.191, Actor Loss -0.284

Updates 2650, training timesteps 212080, FPS 466
Last 100 training episodes: mean/median reward -56.44/-66.34, min/max -100.0/6.0
Policy entropy: 2.296, Critic Loss: 0.301, Actor Loss -0.840

Updates 2660, training timesteps 212880, FPS 466
Last 100 training episodes: mean/median reward -60.60/-75.44, min/max -100.0/0.0
Policy entropy: 2.293, Critic Loss: 0.176, Actor Loss 0.043

Updates 2670, training timesteps 213680, FPS 466
Last 100 training episodes: mean/median reward -50.83/-66.34, min/max -100.0/0.4
Policy entropy: 2.296, Critic Loss: 0.118, Actor Loss -0.116

Updates 2680, training timesteps 214480, FPS 466
Last 100 training episodes: mean/median reward -54.18/-63.02, min/max -100.0/4.4
Policy entropy: 2.296, Critic Loss: 0.125, Actor Loss 0.361

Updates 2690, training timesteps 215280, FPS 466
Last 100 training episodes: mean/median reward -47.25/-51.40, min/max -100.0/2.9
Policy entropy: 2.293, Critic Loss: 0.190, Actor Loss -0.005

Updates 2700, training timesteps 216080, FPS 466
Last 100 training episodes: mean/median reward -51.52/-59.87, min/max -100.0/5.4
Policy entropy: 2.295, Critic Loss: 0.122, Actor Loss -0.125

Updates 2710, training timesteps 216880, FPS 466
Last 100 training episodes: mean/median reward -54.61/-69.83, min/max -100.0/6.3
Policy entropy: 2.290, Critic Loss: 0.087, Actor Loss 0.259

Updates 2720, training timesteps 217680, FPS 466
Last 100 training episodes: mean/median reward -55.87/-69.83, min/max -100.0/7.0
Policy entropy: 2.291, Critic Loss: 0.147, Actor Loss 0.491

Updates 2730, training timesteps 218480, FPS 467
Last 100 training episodes: mean/median reward -56.40/-73.51, min/max -100.0/8.7
Policy entropy: 2.293, Critic Loss: 0.192, Actor Loss -0.248

Updates 2740, training timesteps 219280, FPS 467
Last 100 training episodes: mean/median reward -56.13/-64.68, min/max -100.0/12.8
Policy entropy: 2.290, Critic Loss: 0.138, Actor Loss -0.192

Updates 2750, training timesteps 220080, FPS 467
Last 100 training episodes: mean/median reward -52.29/-63.02, min/max -100.0/8.6
Policy entropy: 2.291, Critic Loss: 0.152, Actor Loss -0.148

Updates 2760, training timesteps 220880, FPS 467
Last 100 training episodes: mean/median reward -58.24/-69.83, min/max -100.0/6.0
Policy entropy: 2.291, Critic Loss: 0.073, Actor Loss 0.199

Updates 2770, training timesteps 221680, FPS 467
Last 100 training episodes: mean/median reward -56.40/-68.09, min/max -100.0/0.0
Policy entropy: 2.291, Critic Loss: 0.103, Actor Loss 0.160

Updates 2780, training timesteps 222480, FPS 467
Last 100 training episodes: mean/median reward -60.37/-73.51, min/max -100.0/4.2
Policy entropy: 2.293, Critic Loss: 0.133, Actor Loss -0.215

Updates 2790, training timesteps 223280, FPS 467
Last 100 training episodes: mean/median reward -55.79/-69.83, min/max -100.0/0.0
Policy entropy: 2.295, Critic Loss: 0.109, Actor Loss -0.150

Updates 2800, training timesteps 224080, FPS 467
Last 100 training episodes: mean/median reward -58.54/-71.67, min/max -100.0/5.7
Policy entropy: 2.293, Critic Loss: 0.202, Actor Loss -0.145

Updates 2810, training timesteps 224880, FPS 467
Last 100 training episodes: mean/median reward -65.55/-77.38, min/max -100.0/0.0
Policy entropy: 2.294, Critic Loss: 0.071, Actor Loss -0.107

Updates 2820, training timesteps 225680, FPS 467
Last 100 training episodes: mean/median reward -57.85/-75.44, min/max -100.0/10.1
Policy entropy: 2.293, Critic Loss: 0.196, Actor Loss -0.129

Updates 2830, training timesteps 226480, FPS 467
Last 100 training episodes: mean/median reward -51.28/-64.68, min/max -100.0/4.6
Policy entropy: 2.294, Critic Loss: 0.087, Actor Loss 0.074

Updates 2840, training timesteps 227280, FPS 467
Last 100 training episodes: mean/median reward -58.04/-71.67, min/max -100.0/4.2
Policy entropy: 2.292, Critic Loss: 0.142, Actor Loss 0.018

Updates 2850, training timesteps 228080, FPS 467
Last 100 training episodes: mean/median reward -53.09/-64.68, min/max -100.0/5.4
Policy entropy: 2.292, Critic Loss: 0.089, Actor Loss 0.022

Updates 2860, training timesteps 228880, FPS 467
Last 100 training episodes: mean/median reward -61.38/-73.51, min/max -100.0/5.7
Policy entropy: 2.293, Critic Loss: 0.196, Actor Loss -0.343

Updates 2870, training timesteps 229680, FPS 467
Last 100 training episodes: mean/median reward -56.29/-69.83, min/max -100.0/7.4
Policy entropy: 2.293, Critic Loss: 0.144, Actor Loss -0.051

Updates 2880, training timesteps 230480, FPS 467
Last 100 training episodes: mean/median reward -53.12/-63.02, min/max -100.0/7.4
Policy entropy: 2.294, Critic Loss: 0.140, Actor Loss -0.190

Updates 2890, training timesteps 231280, FPS 467
Last 100 training episodes: mean/median reward -49.41/-51.33, min/max -100.0/0.0
Policy entropy: 2.293, Critic Loss: 0.180, Actor Loss -0.628

Updates 2900, training timesteps 232080, FPS 467
Last 100 training episodes: mean/median reward -58.60/-69.83, min/max -100.0/6.0
Policy entropy: 2.292, Critic Loss: 0.144, Actor Loss -0.107

Updates 2910, training timesteps 232880, FPS 467
Last 100 training episodes: mean/median reward -62.89/-73.51, min/max -100.0/2.8
Policy entropy: 2.293, Critic Loss: 0.068, Actor Loss 0.286

Updates 2920, training timesteps 233680, FPS 467
Last 100 training episodes: mean/median reward -52.10/-61.45, min/max -100.0/4.0
Policy entropy: 2.292, Critic Loss: 0.122, Actor Loss -0.019

Updates 2930, training timesteps 234480, FPS 467
Last 100 training episodes: mean/median reward -54.51/-63.02, min/max -100.0/1.7
Policy entropy: 2.293, Critic Loss: 0.096, Actor Loss -0.037

Updates 2940, training timesteps 235280, FPS 467
Last 100 training episodes: mean/median reward -58.61/-69.83, min/max -100.0/4.4
Policy entropy: 2.294, Critic Loss: 0.105, Actor Loss 0.148

Updates 2950, training timesteps 236080, FPS 467
Last 100 training episodes: mean/median reward -56.64/-66.34, min/max -100.0/4.4
Policy entropy: 2.294, Critic Loss: 0.215, Actor Loss -0.290

Updates 2960, training timesteps 236880, FPS 467
Last 100 training episodes: mean/median reward -58.37/-77.38, min/max -100.0/4.2
Policy entropy: 2.293, Critic Loss: 0.094, Actor Loss 0.061

Updates 2970, training timesteps 237680, FPS 467
Last 100 training episodes: mean/median reward -57.23/-67.89, min/max -100.0/5.7
Policy entropy: 2.293, Critic Loss: 0.089, Actor Loss -0.152

Updates 2980, training timesteps 238480, FPS 467
Last 100 training episodes: mean/median reward -54.61/-63.02, min/max -100.0/11.7
Policy entropy: 2.292, Critic Loss: 0.226, Actor Loss -0.282

Updates 2990, training timesteps 239280, FPS 467
Last 100 training episodes: mean/median reward -54.22/-66.34, min/max -100.0/9.9
Policy entropy: 2.291, Critic Loss: 0.227, Actor Loss -0.080

Updates 3000, training timesteps 240080, FPS 467
Last 100 training episodes: mean/median reward -53.62/-63.02, min/max -100.0/6.0
Policy entropy: 2.291, Critic Loss: 0.285, Actor Loss -0.519

Updates 3010, training timesteps 240880, FPS 467
Last 100 training episodes: mean/median reward -58.80/-69.83, min/max -100.0/3.1
Policy entropy: 2.291, Critic Loss: 0.170, Actor Loss -0.223

Updates 3020, training timesteps 241680, FPS 467
Last 100 training episodes: mean/median reward -54.65/-66.34, min/max -100.0/2.8
Policy entropy: 2.292, Critic Loss: 0.087, Actor Loss 0.171

Updates 3030, training timesteps 242480, FPS 467
Last 100 training episodes: mean/median reward -59.43/-69.83, min/max -100.0/2.6
Policy entropy: 2.292, Critic Loss: 0.143, Actor Loss 0.689

Updates 3040, training timesteps 243280, FPS 467
Last 100 training episodes: mean/median reward -58.43/-64.68, min/max -100.0/4.2
Policy entropy: 2.294, Critic Loss: 0.105, Actor Loss -0.121

Updates 3050, training timesteps 244080, FPS 467
Last 100 training episodes: mean/median reward -60.02/-69.83, min/max -100.0/5.1
Policy entropy: 2.290, Critic Loss: 0.173, Actor Loss -0.076

Updates 3060, training timesteps 244880, FPS 467
Last 100 training episodes: mean/median reward -58.05/-73.51, min/max -100.0/6.0
Policy entropy: 2.295, Critic Loss: 0.104, Actor Loss -0.170

Updates 3070, training timesteps 245680, FPS 467
Last 100 training episodes: mean/median reward -52.01/-64.68, min/max -100.0/4.7
Policy entropy: 2.292, Critic Loss: 0.153, Actor Loss -0.052

Updates 3080, training timesteps 246480, FPS 467
Last 100 training episodes: mean/median reward -57.43/-73.51, min/max -100.0/11.8
Policy entropy: 2.292, Critic Loss: 0.104, Actor Loss -0.046

Updates 3090, training timesteps 247280, FPS 467
Last 100 training episodes: mean/median reward -58.10/-66.34, min/max -100.0/6.6
Policy entropy: 2.293, Critic Loss: 0.231, Actor Loss -0.403

Updates 3100, training timesteps 248080, FPS 467
Last 100 training episodes: mean/median reward -53.44/-63.02, min/max -100.0/4.2
Policy entropy: 2.290, Critic Loss: 0.134, Actor Loss -0.037

Updates 3110, training timesteps 248880, FPS 467
Last 100 training episodes: mean/median reward -52.37/-64.68, min/max -100.0/4.2
Policy entropy: 2.293, Critic Loss: 0.096, Actor Loss 0.404

Updates 3120, training timesteps 249680, FPS 467
Last 100 training episodes: mean/median reward -56.89/-68.09, min/max -100.0/4.2
Policy entropy: 2.291, Critic Loss: 0.158, Actor Loss 0.130

Updates 3130, training timesteps 250480, FPS 467
Last 100 training episodes: mean/median reward -53.82/-59.87, min/max -100.0/5.1
Policy entropy: 2.290, Critic Loss: 0.251, Actor Loss -0.060

Updates 3140, training timesteps 251280, FPS 467
Last 100 training episodes: mean/median reward -57.54/-73.51, min/max -100.0/9.9
Policy entropy: 2.293, Critic Loss: 0.179, Actor Loss 0.563

Updates 3150, training timesteps 252080, FPS 467
Last 100 training episodes: mean/median reward -58.99/-71.67, min/max -100.0/8.2
Policy entropy: 2.290, Critic Loss: 0.119, Actor Loss 0.262

Updates 3160, training timesteps 252880, FPS 467
Last 100 training episodes: mean/median reward -66.14/-73.51, min/max -100.0/4.4
Policy entropy: 2.293, Critic Loss: 0.110, Actor Loss 0.514

Updates 3170, training timesteps 253680, FPS 467
Last 100 training episodes: mean/median reward -53.74/-69.83, min/max -100.0/5.4
Policy entropy: 2.292, Critic Loss: 0.080, Actor Loss 0.175

Updates 3180, training timesteps 254480, FPS 467
Last 100 training episodes: mean/median reward -53.18/-63.02, min/max -100.0/3.2
Policy entropy: 2.293, Critic Loss: 0.213, Actor Loss -0.408

Updates 3190, training timesteps 255280, FPS 467
Last 100 training episodes: mean/median reward -53.96/-66.60, min/max -100.0/5.4
Policy entropy: 2.293, Critic Loss: 0.247, Actor Loss -0.415

Updates 3200, training timesteps 256080, FPS 467
Last 100 training episodes: mean/median reward -54.40/-68.17, min/max -100.0/4.4
Policy entropy: 2.291, Critic Loss: 0.100, Actor Loss -0.132

Updates 3210, training timesteps 256880, FPS 467
Last 100 training episodes: mean/median reward -55.22/-66.34, min/max -100.0/6.0
Policy entropy: 2.293, Critic Loss: 0.104, Actor Loss -0.018

Updates 3220, training timesteps 257680, FPS 467
Last 100 training episodes: mean/median reward -53.27/-56.88, min/max -100.0/3.6
Policy entropy: 2.293, Critic Loss: 0.088, Actor Loss -0.019

Updates 3230, training timesteps 258480, FPS 467
Last 100 training episodes: mean/median reward -54.72/-66.34, min/max -100.0/1.1
Policy entropy: 2.293, Critic Loss: 0.067, Actor Loss 0.405

Updates 3240, training timesteps 259280, FPS 468
Last 100 training episodes: mean/median reward -57.52/-71.67, min/max -100.0/0.0
Policy entropy: 2.291, Critic Loss: 0.061, Actor Loss 0.347

Updates 3250, training timesteps 260080, FPS 468
Last 100 training episodes: mean/median reward -60.13/-71.67, min/max -100.0/2.5
Policy entropy: 2.292, Critic Loss: 0.076, Actor Loss 0.222

Updates 3260, training timesteps 260880, FPS 468
Last 100 training episodes: mean/median reward -58.91/-64.68, min/max -100.0/3.2
Policy entropy: 2.294, Critic Loss: 0.061, Actor Loss 0.278

Updates 3270, training timesteps 261680, FPS 467
Last 100 training episodes: mean/median reward -54.13/-63.02, min/max -100.0/5.7
Policy entropy: 2.295, Critic Loss: 0.117, Actor Loss -0.071

Updates 3280, training timesteps 262480, FPS 468
Last 100 training episodes: mean/median reward -51.40/-63.02, min/max -100.0/7.0
Policy entropy: 2.294, Critic Loss: 0.071, Actor Loss 0.161

Updates 3290, training timesteps 263280, FPS 467
Last 100 training episodes: mean/median reward -57.07/-73.51, min/max -100.0/6.0
Policy entropy: 2.293, Critic Loss: 0.253, Actor Loss -0.453

Updates 3300, training timesteps 264080, FPS 467
Last 100 training episodes: mean/median reward -51.67/-59.87, min/max -100.0/12.7
Policy entropy: 2.291, Critic Loss: 0.198, Actor Loss -0.172

Updates 3310, training timesteps 264880, FPS 467
Last 100 training episodes: mean/median reward -61.77/-77.38, min/max -100.0/9.8
Policy entropy: 2.292, Critic Loss: 0.084, Actor Loss -0.067

Updates 3320, training timesteps 265680, FPS 468
Last 100 training episodes: mean/median reward -64.09/-79.41, min/max -100.0/11.6
Policy entropy: 2.292, Critic Loss: 0.086, Actor Loss 0.239

Updates 3330, training timesteps 266480, FPS 467
Last 100 training episodes: mean/median reward -55.25/-69.83, min/max -100.0/0.0
Policy entropy: 2.293, Critic Loss: 0.116, Actor Loss -0.012

Updates 3340, training timesteps 267280, FPS 467
Last 100 training episodes: mean/median reward -54.49/-66.34, min/max -100.0/7.0
Policy entropy: 2.293, Critic Loss: 0.097, Actor Loss 0.070

Updates 3350, training timesteps 268080, FPS 468
Last 100 training episodes: mean/median reward -58.07/-69.83, min/max -100.0/5.1
Policy entropy: 2.291, Critic Loss: 0.120, Actor Loss 0.084

Updates 3360, training timesteps 268880, FPS 468
Last 100 training episodes: mean/median reward -54.37/-63.02, min/max -100.0/5.3
Policy entropy: 2.289, Critic Loss: 0.149, Actor Loss -0.010

Updates 3370, training timesteps 269680, FPS 468
Last 100 training episodes: mean/median reward -62.06/-69.93, min/max -100.0/2.8
Policy entropy: 2.293, Critic Loss: 0.072, Actor Loss 0.053

Updates 3380, training timesteps 270480, FPS 468
Last 100 training episodes: mean/median reward -56.38/-69.83, min/max -100.0/5.4
Policy entropy: 2.291, Critic Loss: 0.049, Actor Loss 0.251

Updates 3390, training timesteps 271280, FPS 468
Last 100 training episodes: mean/median reward -55.76/-66.34, min/max -100.0/6.3
Policy entropy: 2.292, Critic Loss: 0.171, Actor Loss -0.030

Updates 3400, training timesteps 272080, FPS 468
Last 100 training episodes: mean/median reward -51.10/-64.68, min/max -100.0/7.6
Policy entropy: 2.292, Critic Loss: 0.267, Actor Loss -0.550

Updates 3410, training timesteps 272880, FPS 468
Last 100 training episodes: mean/median reward -59.39/-69.83, min/max -100.0/0.0
Policy entropy: 2.292, Critic Loss: 0.092, Actor Loss 0.097

Updates 3420, training timesteps 273680, FPS 468
Last 100 training episodes: mean/median reward -57.21/-69.14, min/max -100.0/12.3
Policy entropy: 2.292, Critic Loss: 0.111, Actor Loss -0.006

Updates 3430, training timesteps 274480, FPS 468
Last 100 training episodes: mean/median reward -55.35/-69.75, min/max -100.0/8.9
Policy entropy: 2.292, Critic Loss: 0.324, Actor Loss -0.862

Updates 3440, training timesteps 275280, FPS 468
Last 100 training episodes: mean/median reward -59.80/-69.83, min/max -100.0/0.0
Policy entropy: 2.292, Critic Loss: 0.190, Actor Loss -0.334

Updates 3450, training timesteps 276080, FPS 468
Last 100 training episodes: mean/median reward -59.30/-73.51, min/max -100.0/4.2
Policy entropy: 2.292, Critic Loss: 0.069, Actor Loss 0.061

Updates 3460, training timesteps 276880, FPS 468
Last 100 training episodes: mean/median reward -59.26/-69.83, min/max -100.0/5.1
Policy entropy: 2.292, Critic Loss: 0.159, Actor Loss 0.071

Updates 3470, training timesteps 277680, FPS 468
Last 100 training episodes: mean/median reward -64.61/-77.38, min/max -100.0/6.3
Policy entropy: 2.291, Critic Loss: 0.118, Actor Loss 0.264

Updates 3480, training timesteps 278480, FPS 468
Last 100 training episodes: mean/median reward -51.47/-63.02, min/max -100.0/0.5
Policy entropy: 2.292, Critic Loss: 0.210, Actor Loss -0.208

Updates 3490, training timesteps 279280, FPS 468
Last 100 training episodes: mean/median reward -51.09/-56.88, min/max -100.0/5.4
Policy entropy: 2.292, Critic Loss: 0.175, Actor Loss -0.247

Updates 3500, training timesteps 280080, FPS 468
Last 100 training episodes: mean/median reward -56.05/-66.34, min/max -100.0/5.7
Policy entropy: 2.293, Critic Loss: 0.078, Actor Loss 0.063

Updates 3510, training timesteps 280880, FPS 468
Last 100 training episodes: mean/median reward -53.12/-66.34, min/max -100.0/4.4
Policy entropy: 2.292, Critic Loss: 0.089, Actor Loss -0.007

Updates 3520, training timesteps 281680, FPS 468
Last 100 training episodes: mean/median reward -55.86/-63.02, min/max -100.0/3.4
Policy entropy: 2.292, Critic Loss: 0.158, Actor Loss -0.395

Updates 3530, training timesteps 282480, FPS 468
Last 100 training episodes: mean/median reward -58.99/-68.09, min/max -100.0/7.0
Policy entropy: 2.293, Critic Loss: 0.202, Actor Loss -0.579

Updates 3540, training timesteps 283280, FPS 468
Last 100 training episodes: mean/median reward -55.54/-68.09, min/max -100.0/4.6
Policy entropy: 2.293, Critic Loss: 0.049, Actor Loss 0.483

Updates 3550, training timesteps 284080, FPS 468
Last 100 training episodes: mean/median reward -60.39/-69.83, min/max -100.0/4.4
Policy entropy: 2.292, Critic Loss: 0.149, Actor Loss 0.053

Updates 3560, training timesteps 284880, FPS 468
Last 100 training episodes: mean/median reward -57.34/-66.34, min/max -100.0/5.7
Policy entropy: 2.292, Critic Loss: 0.206, Actor Loss -0.445

Updates 3570, training timesteps 285680, FPS 468
Last 100 training episodes: mean/median reward -57.69/-63.02, min/max -100.0/5.7
Policy entropy: 2.292, Critic Loss: 0.317, Actor Loss -1.003

Updates 3580, training timesteps 286480, FPS 468
Last 100 training episodes: mean/median reward -62.87/-73.51, min/max -100.0/7.0
Policy entropy: 2.289, Critic Loss: 0.182, Actor Loss -0.271

Updates 3590, training timesteps 287280, FPS 468
Last 100 training episodes: mean/median reward -55.56/-68.09, min/max -100.0/10.8
Policy entropy: 2.285, Critic Loss: 0.156, Actor Loss 0.073

Updates 3600, training timesteps 288080, FPS 468
Last 100 training episodes: mean/median reward -52.95/-59.87, min/max -100.0/7.4
Policy entropy: 2.288, Critic Loss: 0.095, Actor Loss -0.278

Updates 3610, training timesteps 288880, FPS 468
Last 100 training episodes: mean/median reward -52.44/-66.34, min/max -100.0/3.2
Policy entropy: 2.292, Critic Loss: 0.178, Actor Loss -0.234

Updates 3620, training timesteps 289680, FPS 468
Last 100 training episodes: mean/median reward -55.21/-66.34, min/max -100.0/13.3
Policy entropy: 2.291, Critic Loss: 0.129, Actor Loss -0.111

Updates 3630, training timesteps 290480, FPS 468
Last 100 training episodes: mean/median reward -51.71/-59.87, min/max -100.0/5.4
Policy entropy: 2.292, Critic Loss: 0.220, Actor Loss 0.022

Updates 3640, training timesteps 291280, FPS 468
Last 100 training episodes: mean/median reward -62.16/-75.44, min/max -100.0/5.1
Policy entropy: 2.292, Critic Loss: 0.164, Actor Loss -0.054

Updates 3650, training timesteps 292080, FPS 468
Last 100 training episodes: mean/median reward -49.47/-63.02, min/max -100.0/0.0
Policy entropy: 2.296, Critic Loss: 0.206, Actor Loss 0.146

Updates 3660, training timesteps 292880, FPS 468
Last 100 training episodes: mean/median reward -59.15/-68.00, min/max -100.0/9.6
Policy entropy: 2.293, Critic Loss: 0.165, Actor Loss -0.263

Updates 3670, training timesteps 293680, FPS 468
Last 100 training episodes: mean/median reward -48.71/-56.88, min/max -100.0/7.1
Policy entropy: 2.294, Critic Loss: 0.200, Actor Loss -0.192

Updates 3680, training timesteps 294480, FPS 468
Last 100 training episodes: mean/median reward -54.87/-71.67, min/max -100.0/5.4
Policy entropy: 2.292, Critic Loss: 0.196, Actor Loss -0.046

Updates 3690, training timesteps 295280, FPS 468
Last 100 training episodes: mean/median reward -54.93/-68.09, min/max -102.4/0.0
Policy entropy: 2.295, Critic Loss: 0.107, Actor Loss 0.121

Updates 3700, training timesteps 296080, FPS 468
Last 100 training episodes: mean/median reward -55.66/-64.68, min/max -100.0/5.4
Policy entropy: 2.295, Critic Loss: 0.087, Actor Loss 0.032

Updates 3710, training timesteps 296880, FPS 468
Last 100 training episodes: mean/median reward -56.01/-66.34, min/max -100.0/5.1
Policy entropy: 2.294, Critic Loss: 0.098, Actor Loss 0.495

Updates 3720, training timesteps 297680, FPS 468
Last 100 training episodes: mean/median reward -51.52/-59.87, min/max -100.0/6.0
Policy entropy: 2.291, Critic Loss: 0.116, Actor Loss 0.166

Updates 3730, training timesteps 298480, FPS 468
Last 100 training episodes: mean/median reward -54.84/-61.45, min/max -100.0/6.0
Policy entropy: 2.291, Critic Loss: 0.346, Actor Loss -0.727

Updates 3740, training timesteps 299280, FPS 468
Last 100 training episodes: mean/median reward -57.79/-73.51, min/max -100.0/4.9
Policy entropy: 2.293, Critic Loss: 0.180, Actor Loss -0.140

Updates 3750, training timesteps 300080, FPS 468
Last 100 training episodes: mean/median reward -54.87/-64.68, min/max -100.0/5.4
Policy entropy: 2.292, Critic Loss: 0.176, Actor Loss 0.032

Updates 3760, training timesteps 300880, FPS 468
Last 100 training episodes: mean/median reward -53.37/-64.68, min/max -100.0/4.4
Policy entropy: 2.289, Critic Loss: 0.084, Actor Loss 0.423

Updates 3770, training timesteps 301680, FPS 468
Last 100 training episodes: mean/median reward -58.59/-75.44, min/max -100.0/4.6
Policy entropy: 2.292, Critic Loss: 0.117, Actor Loss 0.012

Updates 3780, training timesteps 302480, FPS 468
Last 100 training episodes: mean/median reward -55.85/-66.34, min/max -100.0/4.4
Policy entropy: 2.293, Critic Loss: 0.088, Actor Loss 0.136

Updates 3790, training timesteps 303280, FPS 468
Last 100 training episodes: mean/median reward -53.55/-61.45, min/max -100.0/7.7
Policy entropy: 2.291, Critic Loss: 0.074, Actor Loss 0.180

Updates 3800, training timesteps 304080, FPS 469
Last 100 training episodes: mean/median reward -60.70/-68.09, min/max -100.0/2.9
Policy entropy: 2.295, Critic Loss: 0.083, Actor Loss 0.145

Updates 3810, training timesteps 304880, FPS 469
Last 100 training episodes: mean/median reward -59.37/-68.35, min/max -100.0/6.6
Policy entropy: 2.292, Critic Loss: 0.094, Actor Loss -0.132

Updates 3820, training timesteps 305680, FPS 469
Last 100 training episodes: mean/median reward -60.54/-75.44, min/max -100.0/4.4
Policy entropy: 2.293, Critic Loss: 0.063, Actor Loss 0.377

Updates 3830, training timesteps 306480, FPS 469
Last 100 training episodes: mean/median reward -56.11/-63.02, min/max -100.0/3.4
Policy entropy: 2.293, Critic Loss: 0.163, Actor Loss -0.282

Updates 3840, training timesteps 307280, FPS 469
Last 100 training episodes: mean/median reward -58.78/-69.83, min/max -100.0/0.0
Policy entropy: 2.294, Critic Loss: 0.099, Actor Loss 0.030

Updates 3850, training timesteps 308080, FPS 469
Last 100 training episodes: mean/median reward -64.08/-81.45, min/max -100.0/2.9
Policy entropy: 2.292, Critic Loss: 0.130, Actor Loss 0.058

Updates 3860, training timesteps 308880, FPS 469
Last 100 training episodes: mean/median reward -56.73/-63.02, min/max -100.0/6.2
Policy entropy: 2.291, Critic Loss: 0.123, Actor Loss 0.042

Updates 3870, training timesteps 309680, FPS 469
Last 100 training episodes: mean/median reward -60.08/-69.83, min/max -100.0/10.6
Policy entropy: 2.292, Critic Loss: 0.162, Actor Loss -0.442

Updates 3880, training timesteps 310480, FPS 469
Last 100 training episodes: mean/median reward -57.45/-66.34, min/max -100.0/1.8
Policy entropy: 2.291, Critic Loss: 0.160, Actor Loss -0.254

Updates 3890, training timesteps 311280, FPS 469
Last 100 training episodes: mean/median reward -55.05/-66.34, min/max -100.0/6.0
Policy entropy: 2.293, Critic Loss: 0.056, Actor Loss 0.117

Updates 3900, training timesteps 312080, FPS 469
Last 100 training episodes: mean/median reward -54.98/-66.34, min/max -100.0/4.6
Policy entropy: 2.294, Critic Loss: 0.111, Actor Loss -0.022

Updates 3910, training timesteps 312880, FPS 469
Last 100 training episodes: mean/median reward -56.76/-69.83, min/max -100.0/6.0
Policy entropy: 2.292, Critic Loss: 0.098, Actor Loss 0.113

Updates 3920, training timesteps 313680, FPS 469
Last 100 training episodes: mean/median reward -53.51/-66.34, min/max -100.0/6.0
Policy entropy: 2.292, Critic Loss: 0.293, Actor Loss -0.169

Updates 3930, training timesteps 314480, FPS 469
Last 100 training episodes: mean/median reward -58.03/-71.67, min/max -100.0/5.7
Policy entropy: 2.291, Critic Loss: 0.157, Actor Loss -0.368

Updates 3940, training timesteps 315280, FPS 469
Last 100 training episodes: mean/median reward -61.23/-73.51, min/max -100.0/7.0
Policy entropy: 2.292, Critic Loss: 0.209, Actor Loss -0.194

Updates 3950, training timesteps 316080, FPS 469
Last 100 training episodes: mean/median reward -55.35/-69.83, min/max -100.0/5.7
Policy entropy: 2.293, Critic Loss: 0.078, Actor Loss -0.059

Updates 3960, training timesteps 316880, FPS 469
Last 100 training episodes: mean/median reward -56.50/-69.83, min/max -100.0/7.0
Policy entropy: 2.294, Critic Loss: 0.159, Actor Loss -0.484

Updates 3970, training timesteps 317680, FPS 469
Last 100 training episodes: mean/median reward -56.72/-73.51, min/max -100.0/3.8
Policy entropy: 2.292, Critic Loss: 0.081, Actor Loss 0.005

Updates 3980, training timesteps 318480, FPS 469
Last 100 training episodes: mean/median reward -52.42/-69.83, min/max -100.0/6.0
Policy entropy: 2.294, Critic Loss: 0.120, Actor Loss -0.266

Updates 3990, training timesteps 319280, FPS 469
Last 100 training episodes: mean/median reward -53.96/-61.45, min/max -100.0/5.7
Policy entropy: 2.295, Critic Loss: 0.243, Actor Loss 0.820

Updates 4000, training timesteps 320080, FPS 469
Last 100 training episodes: mean/median reward -60.03/-73.51, min/max -100.0/6.0
Policy entropy: 2.292, Critic Loss: 0.058, Actor Loss 0.223

Updates 4010, training timesteps 320880, FPS 469
Last 100 training episodes: mean/median reward -53.58/-63.82, min/max -100.0/7.0
Policy entropy: 2.293, Critic Loss: 0.056, Actor Loss 0.255

Updates 4020, training timesteps 321680, FPS 469
Last 100 training episodes: mean/median reward -53.84/-63.02, min/max -100.0/11.2
Policy entropy: 2.295, Critic Loss: 0.118, Actor Loss -0.204

Updates 4030, training timesteps 322480, FPS 469
Last 100 training episodes: mean/median reward -60.13/-71.67, min/max -100.0/0.0
Policy entropy: 2.293, Critic Loss: 0.135, Actor Loss -0.047

Updates 4040, training timesteps 323280, FPS 469
Last 100 training episodes: mean/median reward -63.82/-77.38, min/max -100.0/5.7
Policy entropy: 2.292, Critic Loss: 0.127, Actor Loss -0.125

Updates 4050, training timesteps 324080, FPS 469
Last 100 training episodes: mean/median reward -66.93/-81.45, min/max -100.0/3.1
Policy entropy: 2.293, Critic Loss: 0.136, Actor Loss 0.127

Updates 4060, training timesteps 324880, FPS 469
Last 100 training episodes: mean/median reward -60.56/-68.09, min/max -100.0/4.2
Policy entropy: 2.292, Critic Loss: 0.089, Actor Loss 0.066

Updates 4070, training timesteps 325680, FPS 469
Last 100 training episodes: mean/median reward -57.69/-66.34, min/max -100.0/5.7
Policy entropy: 2.292, Critic Loss: 0.117, Actor Loss -0.287

Updates 4080, training timesteps 326480, FPS 469
Last 100 training episodes: mean/median reward -56.95/-69.83, min/max -100.0/3.6
Policy entropy: 2.292, Critic Loss: 0.097, Actor Loss 0.001

Updates 4090, training timesteps 327280, FPS 469
Last 100 training episodes: mean/median reward -53.62/-59.87, min/max -100.0/3.6
Policy entropy: 2.294, Critic Loss: 0.157, Actor Loss -0.315

Updates 4100, training timesteps 328080, FPS 469
Last 100 training episodes: mean/median reward -56.28/-64.68, min/max -100.0/4.9
Policy entropy: 2.293, Critic Loss: 0.223, Actor Loss -0.686

Updates 4110, training timesteps 328880, FPS 469
Last 100 training episodes: mean/median reward -53.96/-60.86, min/max -100.0/1.7
Policy entropy: 2.292, Critic Loss: 0.098, Actor Loss -0.131

Updates 4120, training timesteps 329680, FPS 469
Last 100 training episodes: mean/median reward -58.93/-64.68, min/max -100.0/7.0
Policy entropy: 2.292, Critic Loss: 0.173, Actor Loss 0.072

Updates 4130, training timesteps 330480, FPS 469
Last 100 training episodes: mean/median reward -56.21/-61.45, min/max -100.0/6.3
Policy entropy: 2.292, Critic Loss: 0.171, Actor Loss -0.364

Updates 4140, training timesteps 331280, FPS 469
Last 100 training episodes: mean/median reward -58.01/-69.83, min/max -100.0/2.6
Policy entropy: 2.292, Critic Loss: 0.154, Actor Loss 0.214

Updates 4150, training timesteps 332080, FPS 469
Last 100 training episodes: mean/median reward -54.95/-61.45, min/max -100.0/6.0
Policy entropy: 2.293, Critic Loss: 0.094, Actor Loss -0.154

Updates 4160, training timesteps 332880, FPS 469
Last 100 training episodes: mean/median reward -50.41/-61.45, min/max -100.0/0.0
Policy entropy: 2.294, Critic Loss: 0.077, Actor Loss 0.110

Updates 4170, training timesteps 333680, FPS 469
Last 100 training episodes: mean/median reward -59.36/-73.51, min/max -100.0/0.0
Policy entropy: 2.292, Critic Loss: 0.045, Actor Loss 0.267

Updates 4180, training timesteps 334480, FPS 469
Last 100 training episodes: mean/median reward -58.96/-69.83, min/max -100.0/3.8
Policy entropy: 2.294, Critic Loss: 0.103, Actor Loss -0.154

Updates 4190, training timesteps 335280, FPS 469
Last 100 training episodes: mean/median reward -53.61/-63.02, min/max -100.0/10.3
Policy entropy: 2.294, Critic Loss: 0.136, Actor Loss -0.019

Updates 4200, training timesteps 336080, FPS 469
Last 100 training episodes: mean/median reward -50.71/-61.45, min/max -100.0/7.7
Policy entropy: 2.290, Critic Loss: 0.212, Actor Loss -0.249

Updates 4210, training timesteps 336880, FPS 469
Last 100 training episodes: mean/median reward -49.81/-56.88, min/max -100.0/7.0
Policy entropy: 2.291, Critic Loss: 0.126, Actor Loss 0.130

Updates 4220, training timesteps 337680, FPS 469
Last 100 training episodes: mean/median reward -57.65/-77.38, min/max -100.0/7.4
Policy entropy: 2.290, Critic Loss: 0.162, Actor Loss 0.301

Updates 4230, training timesteps 338480, FPS 469
Last 100 training episodes: mean/median reward -60.41/-73.51, min/max -100.0/6.6
Policy entropy: 2.289, Critic Loss: 0.124, Actor Loss 0.042

Updates 4240, training timesteps 339280, FPS 469
Last 100 training episodes: mean/median reward -55.92/-69.83, min/max -100.0/6.0
Policy entropy: 2.287, Critic Loss: 0.166, Actor Loss -0.260

Updates 4250, training timesteps 340080, FPS 469
Last 100 training episodes: mean/median reward -52.23/-66.34, min/max -100.0/6.6
Policy entropy: 2.293, Critic Loss: 0.056, Actor Loss 0.088

Updates 4260, training timesteps 340880, FPS 469
Last 100 training episodes: mean/median reward -56.27/-68.09, min/max -100.0/5.4
Policy entropy: 2.293, Critic Loss: 0.101, Actor Loss 0.028

Updates 4270, training timesteps 341680, FPS 469
Last 100 training episodes: mean/median reward -59.04/-75.44, min/max -100.0/4.9
Policy entropy: 2.289, Critic Loss: 0.070, Actor Loss 0.214

Updates 4280, training timesteps 342480, FPS 469
Last 100 training episodes: mean/median reward -50.83/-58.38, min/max -100.0/5.1
Policy entropy: 2.291, Critic Loss: 0.122, Actor Loss -0.061

Updates 4290, training timesteps 343280, FPS 469
Last 100 training episodes: mean/median reward -56.77/-66.78, min/max -100.0/5.7
Policy entropy: 2.293, Critic Loss: 0.275, Actor Loss -0.629

Updates 4300, training timesteps 344080, FPS 469
Last 100 training episodes: mean/median reward -50.57/-54.11, min/max -100.0/1.3
Policy entropy: 2.292, Critic Loss: 0.181, Actor Loss -0.434

Updates 4310, training timesteps 344880, FPS 469
Last 100 training episodes: mean/median reward -53.60/-64.68, min/max -100.0/4.9
Policy entropy: 2.292, Critic Loss: 0.120, Actor Loss -0.235

Updates 4320, training timesteps 345680, FPS 469
Last 100 training episodes: mean/median reward -50.77/-59.87, min/max -100.0/6.7
Policy entropy: 2.290, Critic Loss: 0.095, Actor Loss 0.056

Updates 4330, training timesteps 346480, FPS 469
Last 100 training episodes: mean/median reward -48.60/-54.04, min/max -100.0/6.6
Policy entropy: 2.292, Critic Loss: 0.193, Actor Loss -0.215

Updates 4340, training timesteps 347280, FPS 469
Last 100 training episodes: mean/median reward -51.37/-61.45, min/max -100.0/5.1
Policy entropy: 2.291, Critic Loss: 0.153, Actor Loss 0.024

Updates 4350, training timesteps 348080, FPS 469
Last 100 training episodes: mean/median reward -47.90/-51.33, min/max -100.0/12.1
Policy entropy: 2.290, Critic Loss: 0.106, Actor Loss -0.158

Updates 4360, training timesteps 348880, FPS 469
Last 100 training episodes: mean/median reward -49.62/-64.68, min/max -100.0/6.6
Policy entropy: 2.291, Critic Loss: 0.155, Actor Loss -0.048

Updates 4370, training timesteps 349680, FPS 469
Last 100 training episodes: mean/median reward -59.99/-73.51, min/max -100.0/9.7
Policy entropy: 2.290, Critic Loss: 0.136, Actor Loss -0.023

Updates 4380, training timesteps 350480, FPS 469
Last 100 training episodes: mean/median reward -56.21/-68.09, min/max -100.0/6.6
Policy entropy: 2.291, Critic Loss: 0.089, Actor Loss -0.004

Updates 4390, training timesteps 351280, FPS 469
Last 100 training episodes: mean/median reward -51.53/-58.38, min/max -100.0/3.6
Policy entropy: 2.291, Critic Loss: 0.221, Actor Loss 0.021

Updates 4400, training timesteps 352080, FPS 469
Last 100 training episodes: mean/median reward -50.32/-51.33, min/max -100.0/4.6
Policy entropy: 2.292, Critic Loss: 0.151, Actor Loss -0.359

Updates 4410, training timesteps 352880, FPS 469
Last 100 training episodes: mean/median reward -58.50/-71.67, min/max -100.0/8.1
Policy entropy: 2.292, Critic Loss: 0.115, Actor Loss -0.036

Updates 4420, training timesteps 353680, FPS 469
Last 100 training episodes: mean/median reward -56.03/-68.09, min/max -100.0/14.3
Policy entropy: 2.290, Critic Loss: 0.215, Actor Loss -0.400

Updates 4430, training timesteps 354480, FPS 469
Last 100 training episodes: mean/median reward -55.57/-69.83, min/max -100.0/4.6
Policy entropy: 2.293, Critic Loss: 0.203, Actor Loss -0.292

Updates 4440, training timesteps 355280, FPS 469
Last 100 training episodes: mean/median reward -62.33/-81.45, min/max -100.0/6.3
Policy entropy: 2.291, Critic Loss: 0.072, Actor Loss 0.280

Updates 4450, training timesteps 356080, FPS 469
Last 100 training episodes: mean/median reward -43.46/-48.77, min/max -100.0/8.6
Policy entropy: 2.293, Critic Loss: 0.128, Actor Loss 0.236

Updates 4460, training timesteps 356880, FPS 469
Last 100 training episodes: mean/median reward -49.42/-63.98, min/max -100.0/5.1
Policy entropy: 2.289, Critic Loss: 0.113, Actor Loss 0.383

Updates 4470, training timesteps 357680, FPS 469
Last 100 training episodes: mean/median reward -52.66/-63.02, min/max -100.0/4.6
Policy entropy: 2.292, Critic Loss: 0.108, Actor Loss -0.128

Updates 4480, training timesteps 358480, FPS 469
Last 100 training episodes: mean/median reward -55.86/-66.34, min/max -100.0/5.1
Policy entropy: 2.292, Critic Loss: 0.190, Actor Loss -0.315

Updates 4490, training timesteps 359280, FPS 469
Last 100 training episodes: mean/median reward -60.13/-73.51, min/max -100.0/7.7
Policy entropy: 2.292, Critic Loss: 0.259, Actor Loss -0.401

Updates 4500, training timesteps 360080, FPS 469
Last 100 training episodes: mean/median reward -57.08/-67.39, min/max -100.0/1.3
Policy entropy: 2.293, Critic Loss: 0.117, Actor Loss 0.047

Updates 4510, training timesteps 360880, FPS 469
Last 100 training episodes: mean/median reward -54.75/-69.83, min/max -100.0/4.9
Policy entropy: 2.293, Critic Loss: 0.141, Actor Loss 0.300

Updates 4520, training timesteps 361680, FPS 469
Last 100 training episodes: mean/median reward -60.32/-69.83, min/max -100.0/1.9
Policy entropy: 2.293, Critic Loss: 0.108, Actor Loss -0.033

Updates 4530, training timesteps 362480, FPS 469
Last 100 training episodes: mean/median reward -51.50/-59.87, min/max -100.0/9.9
Policy entropy: 2.293, Critic Loss: 0.064, Actor Loss 0.228

Updates 4540, training timesteps 363280, FPS 469
Last 100 training episodes: mean/median reward -60.81/-69.83, min/max -100.0/5.1
Policy entropy: 2.294, Critic Loss: 0.116, Actor Loss -0.299

Updates 4550, training timesteps 364080, FPS 469
Last 100 training episodes: mean/median reward -59.66/-73.51, min/max -100.0/7.0
Policy entropy: 2.293, Critic Loss: 0.060, Actor Loss 0.005

Updates 4560, training timesteps 364880, FPS 469
Last 100 training episodes: mean/median reward -55.88/-73.51, min/max -100.0/5.1
Policy entropy: 2.291, Critic Loss: 0.216, Actor Loss -0.626

Updates 4570, training timesteps 365680, FPS 469
Last 100 training episodes: mean/median reward -57.55/-68.09, min/max -100.0/6.6
Policy entropy: 2.293, Critic Loss: 0.127, Actor Loss 0.007

Updates 4580, training timesteps 366480, FPS 469
Last 100 training episodes: mean/median reward -55.42/-71.67, min/max -100.0/9.2
Policy entropy: 2.290, Critic Loss: 0.094, Actor Loss -0.008

Updates 4590, training timesteps 367280, FPS 469
Last 100 training episodes: mean/median reward -60.00/-69.83, min/max -100.0/6.6
Policy entropy: 2.290, Critic Loss: 0.139, Actor Loss 0.002

Updates 4600, training timesteps 368080, FPS 469
Last 100 training episodes: mean/median reward -54.65/-64.68, min/max -100.0/4.4
Policy entropy: 2.289, Critic Loss: 0.140, Actor Loss -0.301

Updates 4610, training timesteps 368880, FPS 469
Last 100 training episodes: mean/median reward -51.96/-63.02, min/max -100.0/8.7
Policy entropy: 2.291, Critic Loss: 0.166, Actor Loss -0.264

Updates 4620, training timesteps 369680, FPS 469
Last 100 training episodes: mean/median reward -53.62/-63.02, min/max -100.0/6.0
Policy entropy: 2.286, Critic Loss: 0.198, Actor Loss -0.283

Updates 4630, training timesteps 370480, FPS 469
Last 100 training episodes: mean/median reward -55.91/-66.34, min/max -100.0/5.7
Policy entropy: 2.294, Critic Loss: 0.153, Actor Loss 0.373

Updates 4640, training timesteps 371280, FPS 469
Last 100 training episodes: mean/median reward -55.95/-61.45, min/max -100.0/5.7
Policy entropy: 2.294, Critic Loss: 0.197, Actor Loss -0.274

Updates 4650, training timesteps 372080, FPS 469
Last 100 training episodes: mean/median reward -54.36/-63.27, min/max -100.0/3.8
Policy entropy: 2.294, Critic Loss: 0.151, Actor Loss -0.219

Updates 4660, training timesteps 372880, FPS 469
Last 100 training episodes: mean/median reward -53.56/-63.27, min/max -100.0/6.9
Policy entropy: 2.291, Critic Loss: 0.127, Actor Loss -0.200

Updates 4670, training timesteps 373680, FPS 469
Last 100 training episodes: mean/median reward -56.20/-66.34, min/max -100.0/5.4
Policy entropy: 2.294, Critic Loss: 0.204, Actor Loss -0.540

Updates 4680, training timesteps 374480, FPS 469
Last 100 training episodes: mean/median reward -60.70/-73.51, min/max -100.0/5.4
Policy entropy: 2.291, Critic Loss: 0.262, Actor Loss -0.713

Updates 4690, training timesteps 375280, FPS 470
Last 100 training episodes: mean/median reward -50.92/-59.87, min/max -100.0/6.3
Policy entropy: 2.291, Critic Loss: 0.108, Actor Loss -0.000

Updates 4700, training timesteps 376080, FPS 469
Last 100 training episodes: mean/median reward -54.69/-66.42, min/max -100.0/5.1
Policy entropy: 2.289, Critic Loss: 0.136, Actor Loss -0.217

Updates 4710, training timesteps 376880, FPS 469
Last 100 training episodes: mean/median reward -59.83/-73.51, min/max -100.0/4.6
Policy entropy: 2.291, Critic Loss: 0.219, Actor Loss 0.148

Updates 4720, training timesteps 377680, FPS 469
Last 100 training episodes: mean/median reward -60.85/-71.67, min/max -100.0/4.2
Policy entropy: 2.294, Critic Loss: 0.236, Actor Loss -0.354

Updates 4730, training timesteps 378480, FPS 469
Last 100 training episodes: mean/median reward -56.56/-64.68, min/max -100.0/11.4
Policy entropy: 2.293, Critic Loss: 0.106, Actor Loss 0.088

Updates 4740, training timesteps 379280, FPS 469
Last 100 training episodes: mean/median reward -55.95/-63.02, min/max -100.0/6.3
Policy entropy: 2.292, Critic Loss: 0.199, Actor Loss -0.726

Updates 4750, training timesteps 380080, FPS 469
Last 100 training episodes: mean/median reward -52.46/-61.45, min/max -100.0/4.6
Policy entropy: 2.292, Critic Loss: 0.056, Actor Loss 0.225

Updates 4760, training timesteps 380880, FPS 469
Last 100 training episodes: mean/median reward -53.95/-68.09, min/max -100.0/7.7
Policy entropy: 2.291, Critic Loss: 0.143, Actor Loss 0.127

Updates 4770, training timesteps 381680, FPS 469
Last 100 training episodes: mean/median reward -54.41/-64.68, min/max -100.0/5.9
Policy entropy: 2.291, Critic Loss: 0.158, Actor Loss -0.024

Updates 4780, training timesteps 382480, FPS 469
Last 100 training episodes: mean/median reward -56.41/-69.83, min/max -100.0/4.0
Policy entropy: 2.292, Critic Loss: 0.176, Actor Loss -0.184

Updates 4790, training timesteps 383280, FPS 469
Last 100 training episodes: mean/median reward -56.81/-72.78, min/max -100.0/5.4
Policy entropy: 2.292, Critic Loss: 0.146, Actor Loss -0.315

Updates 4800, training timesteps 384080, FPS 469
Last 100 training episodes: mean/median reward -51.66/-63.02, min/max -100.0/4.4
Policy entropy: 2.292, Critic Loss: 0.173, Actor Loss -0.374

Updates 4810, training timesteps 384880, FPS 469
Last 100 training episodes: mean/median reward -49.63/-56.88, min/max -100.0/0.9
Policy entropy: 2.291, Critic Loss: 0.066, Actor Loss 0.115

Updates 4820, training timesteps 385680, FPS 469
Last 100 training episodes: mean/median reward -51.44/-56.88, min/max -100.0/5.1
Policy entropy: 2.289, Critic Loss: 0.102, Actor Loss -0.315

Updates 4830, training timesteps 386480, FPS 470
Last 100 training episodes: mean/median reward -53.21/-61.45, min/max -100.0/7.0
Policy entropy: 2.292, Critic Loss: 0.213, Actor Loss -0.438

Updates 4840, training timesteps 387280, FPS 470
Last 100 training episodes: mean/median reward -60.81/-72.34, min/max -100.0/6.6
Policy entropy: 2.292, Critic Loss: 0.146, Actor Loss 0.273

Updates 4850, training timesteps 388080, FPS 470
Last 100 training episodes: mean/median reward -51.67/-63.02, min/max -100.0/5.4
Policy entropy: 2.292, Critic Loss: 0.064, Actor Loss 0.359

Updates 4860, training timesteps 388880, FPS 470
Last 100 training episodes: mean/median reward -49.97/-64.68, min/max -100.0/4.9
Policy entropy: 2.293, Critic Loss: 0.162, Actor Loss -0.206

Updates 4870, training timesteps 389680, FPS 469
Last 100 training episodes: mean/median reward -49.02/-59.87, min/max -100.0/4.9
Policy entropy: 2.292, Critic Loss: 0.176, Actor Loss -0.392

Updates 4880, training timesteps 390480, FPS 469
Last 100 training episodes: mean/median reward -52.48/-64.68, min/max -100.0/13.9
Policy entropy: 2.292, Critic Loss: 0.140, Actor Loss 0.154

Updates 4890, training timesteps 391280, FPS 469
Last 100 training episodes: mean/median reward -54.09/-66.34, min/max -100.0/4.9
Policy entropy: 2.290, Critic Loss: 0.114, Actor Loss -0.152

Updates 4900, training timesteps 392080, FPS 469
Last 100 training episodes: mean/median reward -58.75/-71.67, min/max -100.0/6.9
Policy entropy: 2.291, Critic Loss: 0.085, Actor Loss -0.057

Updates 4910, training timesteps 392880, FPS 469
Last 100 training episodes: mean/median reward -62.62/-73.51, min/max -100.0/7.9
Policy entropy: 2.291, Critic Loss: 0.176, Actor Loss -0.192

Updates 4920, training timesteps 393680, FPS 469
Last 100 training episodes: mean/median reward -53.88/-69.37, min/max -100.0/4.9
Policy entropy: 2.291, Critic Loss: 0.118, Actor Loss -0.037

Updates 4930, training timesteps 394480, FPS 470
Last 100 training episodes: mean/median reward -54.96/-63.66, min/max -100.0/7.4
Policy entropy: 2.290, Critic Loss: 0.089, Actor Loss 0.048

Updates 4940, training timesteps 395280, FPS 470
Last 100 training episodes: mean/median reward -52.80/-63.02, min/max -100.0/3.1
Policy entropy: 2.289, Critic Loss: 0.131, Actor Loss 0.002

Updates 4950, training timesteps 396080, FPS 470
Last 100 training episodes: mean/median reward -55.50/-66.34, min/max -100.0/6.3
Policy entropy: 2.290, Critic Loss: 0.191, Actor Loss -0.129

Updates 4960, training timesteps 396880, FPS 470
Last 100 training episodes: mean/median reward -47.08/-54.04, min/max -100.0/6.3
Policy entropy: 2.290, Critic Loss: 0.064, Actor Loss 0.107

Updates 4970, training timesteps 397680, FPS 470
Last 100 training episodes: mean/median reward -51.18/-60.98, min/max -100.0/4.4
Policy entropy: 2.292, Critic Loss: 0.144, Actor Loss -0.199

Updates 4980, training timesteps 398480, FPS 470
Last 100 training episodes: mean/median reward -60.96/-69.83, min/max -100.0/4.2
Policy entropy: 2.291, Critic Loss: 0.175, Actor Loss -0.012

Updates 4990, training timesteps 399280, FPS 470
Last 100 training episodes: mean/median reward -52.91/-59.87, min/max -100.0/3.1
Policy entropy: 2.291, Critic Loss: 0.148, Actor Loss -0.061

Updates 5000, training timesteps 400080, FPS 470
Last 100 training episodes: mean/median reward -60.44/-73.51, min/max -100.0/2.8
Policy entropy: 2.291, Critic Loss: 0.188, Actor Loss -0.340

Updates 5010, training timesteps 400880, FPS 470
Last 100 training episodes: mean/median reward -57.64/-66.34, min/max -100.0/7.7
Policy entropy: 2.291, Critic Loss: 0.054, Actor Loss 0.206

Updates 5020, training timesteps 401680, FPS 470
Last 100 training episodes: mean/median reward -55.51/-71.67, min/max -100.0/11.6
Policy entropy: 2.291, Critic Loss: 0.187, Actor Loss 0.373

Updates 5030, training timesteps 402480, FPS 470
Last 100 training episodes: mean/median reward -56.67/-63.02, min/max -100.0/11.8
Policy entropy: 2.293, Critic Loss: 0.337, Actor Loss -0.909

Updates 5040, training timesteps 403280, FPS 470
Last 100 training episodes: mean/median reward -50.61/-63.02, min/max -100.0/9.9
Policy entropy: 2.291, Critic Loss: 0.304, Actor Loss -0.309

Updates 5050, training timesteps 404080, FPS 470
Last 100 training episodes: mean/median reward -54.64/-63.02, min/max -100.0/8.6
Policy entropy: 2.292, Critic Loss: 0.059, Actor Loss 0.254

Updates 5060, training timesteps 404880, FPS 470
Last 100 training episodes: mean/median reward -51.12/-59.87, min/max -100.0/6.3
Policy entropy: 2.287, Critic Loss: 0.215, Actor Loss -0.296

Updates 5070, training timesteps 405680, FPS 470
Last 100 training episodes: mean/median reward -46.94/-56.88, min/max -100.0/3.8
Policy entropy: 2.290, Critic Loss: 0.154, Actor Loss -0.145

Updates 5080, training timesteps 406480, FPS 470
Last 100 training episodes: mean/median reward -55.94/-73.51, min/max -100.0/6.1
Policy entropy: 2.290, Critic Loss: 0.162, Actor Loss 0.068

Updates 5090, training timesteps 407280, FPS 470
Last 100 training episodes: mean/median reward -55.84/-64.68, min/max -100.0/6.3
Policy entropy: 2.289, Critic Loss: 0.135, Actor Loss 0.004

Updates 5100, training timesteps 408080, FPS 470
Last 100 training episodes: mean/median reward -58.26/-66.34, min/max -100.0/3.8
Policy entropy: 2.289, Critic Loss: 0.133, Actor Loss 0.098

Updates 5110, training timesteps 408880, FPS 470
Last 100 training episodes: mean/median reward -53.24/-62.74, min/max -100.0/5.4
Policy entropy: 2.288, Critic Loss: 0.112, Actor Loss -0.016

Updates 5120, training timesteps 409680, FPS 470
Last 100 training episodes: mean/median reward -52.43/-62.74, min/max -100.0/9.1
Policy entropy: 2.292, Critic Loss: 0.130, Actor Loss -0.057

Updates 5130, training timesteps 410480, FPS 470
Last 100 training episodes: mean/median reward -53.41/-66.34, min/max -100.0/6.6
Policy entropy: 2.288, Critic Loss: 0.145, Actor Loss -0.018

Updates 5140, training timesteps 411280, FPS 470
Last 100 training episodes: mean/median reward -58.37/-69.83, min/max -100.0/5.2
Policy entropy: 2.288, Critic Loss: 0.096, Actor Loss 0.185

Updates 5150, training timesteps 412080, FPS 470
Last 100 training episodes: mean/median reward -54.50/-68.09, min/max -100.0/4.9
Policy entropy: 2.289, Critic Loss: 0.178, Actor Loss -0.254

Updates 5160, training timesteps 412880, FPS 470
Last 100 training episodes: mean/median reward -60.18/-69.83, min/max -100.0/2.6
Policy entropy: 2.289, Critic Loss: 0.071, Actor Loss 0.233

Updates 5170, training timesteps 413680, FPS 470
Last 100 training episodes: mean/median reward -58.40/-73.51, min/max -100.0/6.0
Policy entropy: 2.290, Critic Loss: 0.089, Actor Loss 0.159

Updates 5180, training timesteps 414480, FPS 470
Last 100 training episodes: mean/median reward -58.46/-69.83, min/max -100.0/0.0
Policy entropy: 2.292, Critic Loss: 0.068, Actor Loss 0.243

Updates 5190, training timesteps 415280, FPS 470
Last 100 training episodes: mean/median reward -53.48/-66.34, min/max -100.0/6.0
Policy entropy: 2.292, Critic Loss: 0.194, Actor Loss -0.201

Updates 5200, training timesteps 416080, FPS 470
Last 100 training episodes: mean/median reward -51.30/-55.92, min/max -100.0/5.1
Policy entropy: 2.291, Critic Loss: 0.104, Actor Loss 0.074

Updates 5210, training timesteps 416880, FPS 470
Last 100 training episodes: mean/median reward -58.49/-69.83, min/max -100.0/7.0
Policy entropy: 2.291, Critic Loss: 0.070, Actor Loss 0.137

Updates 5220, training timesteps 417680, FPS 470
Last 100 training episodes: mean/median reward -60.89/-73.51, min/max -100.0/0.0
Policy entropy: 2.289, Critic Loss: 0.079, Actor Loss 0.240

Updates 5230, training timesteps 418480, FPS 470
Last 100 training episodes: mean/median reward -58.42/-66.34, min/max -100.0/0.0
Policy entropy: 2.291, Critic Loss: 0.085, Actor Loss 0.164

Updates 5240, training timesteps 419280, FPS 470
Last 100 training episodes: mean/median reward -52.23/-66.34, min/max -100.0/4.0
Policy entropy: 2.289, Critic Loss: 0.124, Actor Loss -0.145

Updates 5250, training timesteps 420080, FPS 470
Last 100 training episodes: mean/median reward -54.60/-64.68, min/max -100.0/7.0
Policy entropy: 2.292, Critic Loss: 0.216, Actor Loss -0.621

Updates 5260, training timesteps 420880, FPS 470
Last 100 training episodes: mean/median reward -51.90/-59.67, min/max -100.0/3.2
Policy entropy: 2.289, Critic Loss: 0.181, Actor Loss -0.177

Updates 5270, training timesteps 421680, FPS 470
Last 100 training episodes: mean/median reward -50.19/-63.11, min/max -100.0/4.4
Policy entropy: 2.290, Critic Loss: 0.158, Actor Loss 0.107

Updates 5280, training timesteps 422480, FPS 470
Last 100 training episodes: mean/median reward -59.42/-69.83, min/max -100.0/9.6
Policy entropy: 2.290, Critic Loss: 0.087, Actor Loss 0.251

Updates 5290, training timesteps 423280, FPS 470
Last 100 training episodes: mean/median reward -55.83/-66.34, min/max -100.0/9.6
Policy entropy: 2.290, Critic Loss: 0.115, Actor Loss -0.288

Updates 5300, training timesteps 424080, FPS 470
Last 100 training episodes: mean/median reward -55.03/-64.68, min/max -100.0/5.7
Policy entropy: 2.291, Critic Loss: 0.036, Actor Loss 0.113

Updates 5310, training timesteps 424880, FPS 470
Last 100 training episodes: mean/median reward -58.67/-69.83, min/max -100.0/4.3
Policy entropy: 2.291, Critic Loss: 0.134, Actor Loss -0.088

Updates 5320, training timesteps 425680, FPS 470
Last 100 training episodes: mean/median reward -58.79/-66.34, min/max -100.0/6.6
Policy entropy: 2.291, Critic Loss: 0.102, Actor Loss -0.333

Updates 5330, training timesteps 426480, FPS 470
Last 100 training episodes: mean/median reward -46.45/-54.04, min/max -100.0/7.0
Policy entropy: 2.292, Critic Loss: 0.107, Actor Loss 0.020

Updates 5340, training timesteps 427280, FPS 470
Last 100 training episodes: mean/median reward -51.90/-56.88, min/max -100.0/7.0
Policy entropy: 2.287, Critic Loss: 0.138, Actor Loss 0.065

Updates 5350, training timesteps 428080, FPS 470
Last 100 training episodes: mean/median reward -51.89/-61.45, min/max -100.0/5.4
Policy entropy: 2.289, Critic Loss: 0.162, Actor Loss -0.314

Updates 5360, training timesteps 428880, FPS 470
Last 100 training episodes: mean/median reward -59.54/-71.67, min/max -100.0/0.0
Policy entropy: 2.291, Critic Loss: 0.121, Actor Loss 0.113

Updates 5370, training timesteps 429680, FPS 470
Last 100 training episodes: mean/median reward -58.34/-73.51, min/max -100.0/0.0
Policy entropy: 2.291, Critic Loss: 0.070, Actor Loss 0.201

Updates 5380, training timesteps 430480, FPS 470
Last 100 training episodes: mean/median reward -52.58/-59.87, min/max -100.0/7.0
Policy entropy: 2.291, Critic Loss: 0.111, Actor Loss -0.051

Updates 5390, training timesteps 431280, FPS 470
Last 100 training episodes: mean/median reward -53.15/-66.34, min/max -100.0/11.2
Policy entropy: 2.286, Critic Loss: 0.269, Actor Loss -0.540

Updates 5400, training timesteps 432080, FPS 470
Last 100 training episodes: mean/median reward -58.25/-71.67, min/max -100.0/5.7
Policy entropy: 2.288, Critic Loss: 0.108, Actor Loss 0.068

Updates 5410, training timesteps 432880, FPS 470
Last 100 training episodes: mean/median reward -53.50/-66.34, min/max -100.0/12.6
Policy entropy: 2.289, Critic Loss: 0.312, Actor Loss -0.478

Updates 5420, training timesteps 433680, FPS 470
Last 100 training episodes: mean/median reward -53.50/-63.02, min/max -100.0/5.4
Policy entropy: 2.289, Critic Loss: 0.096, Actor Loss -0.048

Updates 5430, training timesteps 434480, FPS 470
Last 100 training episodes: mean/median reward -48.45/-56.88, min/max -100.0/13.8
Policy entropy: 2.293, Critic Loss: 0.125, Actor Loss 0.026

Updates 5440, training timesteps 435280, FPS 470
Last 100 training episodes: mean/median reward -52.91/-58.38, min/max -100.0/4.4
Policy entropy: 2.291, Critic Loss: 0.065, Actor Loss 0.170

Updates 5450, training timesteps 436080, FPS 470
Last 100 training episodes: mean/median reward -59.44/-69.83, min/max -100.0/4.4
Policy entropy: 2.290, Critic Loss: 0.080, Actor Loss -0.054

Updates 5460, training timesteps 436880, FPS 470
Last 100 training episodes: mean/median reward -58.54/-73.51, min/max -100.0/5.7
Policy entropy: 2.283, Critic Loss: 0.093, Actor Loss 0.153

Updates 5470, training timesteps 437680, FPS 470
Last 100 training episodes: mean/median reward -55.82/-64.68, min/max -100.0/5.1
Policy entropy: 2.290, Critic Loss: 0.082, Actor Loss -0.015

Updates 5480, training timesteps 438480, FPS 470
Last 100 training episodes: mean/median reward -51.04/-52.69, min/max -100.0/5.7
Policy entropy: 2.291, Critic Loss: 0.071, Actor Loss 0.209

Updates 5490, training timesteps 439280, FPS 470
Last 100 training episodes: mean/median reward -53.37/-63.02, min/max -100.0/6.0
Policy entropy: 2.290, Critic Loss: 0.079, Actor Loss 0.208

Updates 5500, training timesteps 440080, FPS 470
Last 100 training episodes: mean/median reward -54.67/-61.45, min/max -100.0/2.9
Policy entropy: 2.291, Critic Loss: 0.192, Actor Loss -0.357

Updates 5510, training timesteps 440880, FPS 470
Last 100 training episodes: mean/median reward -57.43/-73.51, min/max -100.0/6.6
Policy entropy: 2.290, Critic Loss: 0.136, Actor Loss -0.379

Updates 5520, training timesteps 441680, FPS 470
Last 100 training episodes: mean/median reward -55.43/-64.68, min/max -100.0/0.0
Policy entropy: 2.292, Critic Loss: 0.206, Actor Loss -0.504

Updates 5530, training timesteps 442480, FPS 470
Last 100 training episodes: mean/median reward -55.93/-63.02, min/max -100.0/6.3
Policy entropy: 2.290, Critic Loss: 0.110, Actor Loss 0.106

Updates 5540, training timesteps 443280, FPS 470
Last 100 training episodes: mean/median reward -58.95/-71.67, min/max -100.0/6.3
Policy entropy: 2.290, Critic Loss: 0.145, Actor Loss 0.181

Updates 5550, training timesteps 444080, FPS 470
Last 100 training episodes: mean/median reward -56.97/-66.34, min/max -100.0/4.2
Policy entropy: 2.289, Critic Loss: 0.053, Actor Loss -0.095

Updates 5560, training timesteps 444880, FPS 470
Last 100 training episodes: mean/median reward -59.12/-66.34, min/max -100.0/4.2
Policy entropy: 2.291, Critic Loss: 0.107, Actor Loss 0.007

Updates 5570, training timesteps 445680, FPS 470
Last 100 training episodes: mean/median reward -52.79/-62.02, min/max -100.0/2.3
Policy entropy: 2.294, Critic Loss: 0.110, Actor Loss 0.069

Updates 5580, training timesteps 446480, FPS 470
Last 100 training episodes: mean/median reward -57.90/-63.02, min/max -100.0/3.2
Policy entropy: 2.294, Critic Loss: 0.090, Actor Loss 0.088

Updates 5590, training timesteps 447280, FPS 470
Last 100 training episodes: mean/median reward -58.09/-73.51, min/max -100.0/0.0
Policy entropy: 2.294, Critic Loss: 0.122, Actor Loss -0.178

Updates 5600, training timesteps 448080, FPS 470
Last 100 training episodes: mean/median reward -57.25/-69.53, min/max -100.0/6.0
Policy entropy: 2.293, Critic Loss: 0.093, Actor Loss 0.046

Updates 5610, training timesteps 448880, FPS 470
Last 100 training episodes: mean/median reward -54.21/-63.02, min/max -100.0/5.4
Policy entropy: 2.294, Critic Loss: 0.164, Actor Loss 0.155

Updates 5620, training timesteps 449680, FPS 470
Last 100 training episodes: mean/median reward -63.30/-73.51, min/max -100.0/4.9
Policy entropy: 2.292, Critic Loss: 0.089, Actor Loss 0.275

Updates 5630, training timesteps 450480, FPS 470
Last 100 training episodes: mean/median reward -50.30/-59.87, min/max -100.0/12.8
Policy entropy: 2.292, Critic Loss: 0.140, Actor Loss -0.408

Updates 5640, training timesteps 451280, FPS 470
Last 100 training episodes: mean/median reward -56.26/-68.09, min/max -100.0/2.3
Policy entropy: 2.292, Critic Loss: 0.170, Actor Loss -0.160

Updates 5650, training timesteps 452080, FPS 470
Last 100 training episodes: mean/median reward -54.38/-68.09, min/max -100.0/0.0
Policy entropy: 2.293, Critic Loss: 0.108, Actor Loss -0.192

Updates 5660, training timesteps 452880, FPS 470
Last 100 training episodes: mean/median reward -59.71/-68.09, min/max -100.0/0.0
Policy entropy: 2.292, Critic Loss: 0.065, Actor Loss 0.330

Updates 5670, training timesteps 453680, FPS 470
Last 100 training episodes: mean/median reward -57.44/-69.83, min/max -100.0/6.3
Policy entropy: 2.293, Critic Loss: 0.095, Actor Loss -0.255

Updates 5680, training timesteps 454480, FPS 470
Last 100 training episodes: mean/median reward -58.60/-69.83, min/max -100.0/4.4
Policy entropy: 2.292, Critic Loss: 0.117, Actor Loss 0.121

Updates 5690, training timesteps 455280, FPS 470
Last 100 training episodes: mean/median reward -57.86/-66.34, min/max -100.0/5.7
Policy entropy: 2.292, Critic Loss: 0.281, Actor Loss -0.372

Updates 5700, training timesteps 456080, FPS 470
Last 100 training episodes: mean/median reward -59.35/-69.83, min/max -100.0/10.0
Policy entropy: 2.290, Critic Loss: 0.100, Actor Loss 0.088

Updates 5710, training timesteps 456880, FPS 470
Last 100 training episodes: mean/median reward -58.56/-70.94, min/max -100.0/0.0
Policy entropy: 2.292, Critic Loss: 0.151, Actor Loss -0.637

Updates 5720, training timesteps 457680, FPS 470
Last 100 training episodes: mean/median reward -62.01/-77.38, min/max -100.0/3.4
Policy entropy: 2.292, Critic Loss: 0.105, Actor Loss -0.275

Updates 5730, training timesteps 458480, FPS 470
Last 100 training episodes: mean/median reward -53.72/-59.87, min/max -100.0/4.2
Policy entropy: 2.293, Critic Loss: 0.085, Actor Loss 0.510

Updates 5740, training timesteps 459280, FPS 470
Last 100 training episodes: mean/median reward -56.42/-68.09, min/max -100.0/5.1
Policy entropy: 2.293, Critic Loss: 0.074, Actor Loss -0.074

Updates 5750, training timesteps 460080, FPS 470
Last 100 training episodes: mean/median reward -52.66/-59.87, min/max -100.0/13.9
Policy entropy: 2.296, Critic Loss: 0.135, Actor Loss -0.292

Updates 5760, training timesteps 460880, FPS 470
Last 100 training episodes: mean/median reward -57.34/-69.83, min/max -100.0/10.4
Policy entropy: 2.292, Critic Loss: 0.092, Actor Loss 0.362

Updates 5770, training timesteps 461680, FPS 470
Last 100 training episodes: mean/median reward -58.44/-71.67, min/max -100.0/6.3
Policy entropy: 2.293, Critic Loss: 0.090, Actor Loss -0.038

Updates 5780, training timesteps 462480, FPS 470
Last 100 training episodes: mean/median reward -55.70/-64.68, min/max -100.0/4.4
Policy entropy: 2.291, Critic Loss: 0.097, Actor Loss 0.025

Updates 5790, training timesteps 463280, FPS 470
Last 100 training episodes: mean/median reward -43.23/-40.77, min/max -100.0/8.6
Policy entropy: 2.288, Critic Loss: 0.063, Actor Loss 0.212

Updates 5800, training timesteps 464080, FPS 470
Last 100 training episodes: mean/median reward -47.50/-48.77, min/max -100.0/6.0
Policy entropy: 2.289, Critic Loss: 0.210, Actor Loss -0.189

Updates 5810, training timesteps 464880, FPS 470
Last 100 training episodes: mean/median reward -57.82/-66.34, min/max -100.0/6.3
Policy entropy: 2.291, Critic Loss: 0.264, Actor Loss -0.777

Updates 5820, training timesteps 465680, FPS 470
Last 100 training episodes: mean/median reward -60.38/-71.67, min/max -100.0/3.8
Policy entropy: 2.290, Critic Loss: 0.052, Actor Loss 0.424

Updates 5830, training timesteps 466480, FPS 470
Last 100 training episodes: mean/median reward -54.76/-66.34, min/max -100.0/5.4
Policy entropy: 2.290, Critic Loss: 0.184, Actor Loss -0.369

Updates 5840, training timesteps 467280, FPS 470
Last 100 training episodes: mean/median reward -61.37/-69.83, min/max -100.0/4.6
Policy entropy: 2.291, Critic Loss: 0.142, Actor Loss -0.268

Updates 5850, training timesteps 468080, FPS 470
Last 100 training episodes: mean/median reward -56.31/-69.83, min/max -100.0/6.3
Policy entropy: 2.289, Critic Loss: 0.187, Actor Loss -0.060

Updates 5860, training timesteps 468880, FPS 470
Last 100 training episodes: mean/median reward -55.89/-63.02, min/max -100.0/14.4
Policy entropy: 2.289, Critic Loss: 0.182, Actor Loss 0.397

Updates 5870, training timesteps 469680, FPS 470
Last 100 training episodes: mean/median reward -58.66/-73.51, min/max -100.0/4.0
Policy entropy: 2.288, Critic Loss: 0.142, Actor Loss -0.056

Updates 5880, training timesteps 470480, FPS 470
Last 100 training episodes: mean/median reward -58.29/-69.83, min/max -100.0/4.6
Policy entropy: 2.292, Critic Loss: 0.174, Actor Loss -0.259

Updates 5890, training timesteps 471280, FPS 470
Last 100 training episodes: mean/median reward -43.01/-48.77, min/max -100.0/6.0
Policy entropy: 2.290, Critic Loss: 0.274, Actor Loss -0.633

Updates 5900, training timesteps 472080, FPS 470
Last 100 training episodes: mean/median reward -53.04/-66.34, min/max -100.0/6.6
Policy entropy: 2.289, Critic Loss: 0.122, Actor Loss -0.067

Updates 5910, training timesteps 472880, FPS 470
Last 100 training episodes: mean/median reward -54.62/-63.02, min/max -100.0/4.9
Policy entropy: 2.291, Critic Loss: 0.142, Actor Loss -0.431

Updates 5920, training timesteps 473680, FPS 470
Last 100 training episodes: mean/median reward -55.78/-63.02, min/max -100.0/4.2
Policy entropy: 2.291, Critic Loss: 0.116, Actor Loss 0.121

Updates 5930, training timesteps 474480, FPS 470
Last 100 training episodes: mean/median reward -58.10/-63.02, min/max -100.0/6.3
Policy entropy: 2.292, Critic Loss: 0.062, Actor Loss 0.173

Updates 5940, training timesteps 475280, FPS 470
Last 100 training episodes: mean/median reward -52.56/-66.34, min/max -100.0/12.6
Policy entropy: 2.291, Critic Loss: 0.151, Actor Loss 0.054

Updates 5950, training timesteps 476080, FPS 470
Last 100 training episodes: mean/median reward -59.45/-66.34, min/max -100.0/5.4
Policy entropy: 2.292, Critic Loss: 0.165, Actor Loss -0.261

Updates 5960, training timesteps 476880, FPS 470
Last 100 training episodes: mean/median reward -57.81/-63.99, min/max -100.0/3.2
Policy entropy: 2.291, Critic Loss: 0.042, Actor Loss 0.375

Updates 5970, training timesteps 477680, FPS 470
Last 100 training episodes: mean/median reward -54.97/-66.34, min/max -100.0/1.8
Policy entropy: 2.290, Critic Loss: 0.096, Actor Loss 0.304

Updates 5980, training timesteps 478480, FPS 470
Last 100 training episodes: mean/median reward -57.86/-69.83, min/max -100.0/6.1
Policy entropy: 2.293, Critic Loss: 0.127, Actor Loss -0.401

Updates 5990, training timesteps 479280, FPS 470
Last 100 training episodes: mean/median reward -56.16/-68.09, min/max -100.0/4.2
Policy entropy: 2.290, Critic Loss: 0.099, Actor Loss 0.287

Updates 6000, training timesteps 480080, FPS 470
Last 100 training episodes: mean/median reward -55.77/-68.09, min/max -100.0/3.4
Policy entropy: 2.288, Critic Loss: 0.102, Actor Loss -0.173

Updates 6010, training timesteps 480880, FPS 470
Last 100 training episodes: mean/median reward -55.11/-66.34, min/max -100.0/6.6
Policy entropy: 2.290, Critic Loss: 0.156, Actor Loss -0.228

Updates 6020, training timesteps 481680, FPS 470
Last 100 training episodes: mean/median reward -56.14/-69.83, min/max -100.0/4.9
Policy entropy: 2.290, Critic Loss: 0.126, Actor Loss -0.088

Updates 6030, training timesteps 482480, FPS 470
Last 100 training episodes: mean/median reward -56.58/-73.51, min/max -100.0/4.9
Policy entropy: 2.290, Critic Loss: 0.082, Actor Loss 0.203

Updates 6040, training timesteps 483280, FPS 470
Last 100 training episodes: mean/median reward -54.19/-63.02, min/max -100.0/0.0
Policy entropy: 2.294, Critic Loss: 0.141, Actor Loss -0.304

Updates 6050, training timesteps 484080, FPS 470
Last 100 training episodes: mean/median reward -56.94/-68.09, min/max -100.0/7.0
Policy entropy: 2.293, Critic Loss: 0.102, Actor Loss -0.021

Updates 6060, training timesteps 484880, FPS 470
Last 100 training episodes: mean/median reward -61.25/-69.83, min/max -100.0/4.6
Policy entropy: 2.291, Critic Loss: 0.176, Actor Loss 0.098

Updates 6070, training timesteps 485680, FPS 470
Last 100 training episodes: mean/median reward -52.83/-69.83, min/max -100.0/10.9
Policy entropy: 2.292, Critic Loss: 0.155, Actor Loss -0.198

Updates 6080, training timesteps 486480, FPS 470
Last 100 training episodes: mean/median reward -57.75/-69.83, min/max -100.0/4.9
Policy entropy: 2.293, Critic Loss: 0.126, Actor Loss -0.348

Updates 6090, training timesteps 487280, FPS 470
Last 100 training episodes: mean/median reward -50.69/-64.68, min/max -100.0/6.0
Policy entropy: 2.290, Critic Loss: 0.170, Actor Loss 0.890

Updates 6100, training timesteps 488080, FPS 470
Last 100 training episodes: mean/median reward -54.93/-66.34, min/max -100.0/6.0
Policy entropy: 2.292, Critic Loss: 0.070, Actor Loss 0.134

Updates 6110, training timesteps 488880, FPS 470
Last 100 training episodes: mean/median reward -56.43/-66.34, min/max -100.0/5.1
Policy entropy: 2.292, Critic Loss: 0.086, Actor Loss 0.141

Updates 6120, training timesteps 489680, FPS 470
Last 100 training episodes: mean/median reward -54.22/-68.09, min/max -100.0/4.9
Policy entropy: 2.292, Critic Loss: 0.100, Actor Loss -0.004

Updates 6130, training timesteps 490480, FPS 470
Last 100 training episodes: mean/median reward -55.05/-69.83, min/max -100.0/8.6
Policy entropy: 2.289, Critic Loss: 0.232, Actor Loss 0.045

Updates 6140, training timesteps 491280, FPS 470
Last 100 training episodes: mean/median reward -53.99/-69.83, min/max -100.0/6.0
Policy entropy: 2.291, Critic Loss: 0.117, Actor Loss 0.274

Updates 6150, training timesteps 492080, FPS 470
Last 100 training episodes: mean/median reward -60.81/-73.51, min/max -100.0/11.1
Policy entropy: 2.289, Critic Loss: 0.070, Actor Loss 0.012

Updates 6160, training timesteps 492880, FPS 470
Last 100 training episodes: mean/median reward -52.22/-61.45, min/max -100.0/5.4
Policy entropy: 2.290, Critic Loss: 0.102, Actor Loss 0.137

Updates 6170, training timesteps 493680, FPS 470
Last 100 training episodes: mean/median reward -48.77/-56.88, min/max -100.0/2.8
Policy entropy: 2.292, Critic Loss: 0.119, Actor Loss 0.117

Updates 6180, training timesteps 494480, FPS 470
Last 100 training episodes: mean/median reward -53.10/-63.02, min/max -100.0/2.8
Policy entropy: 2.292, Critic Loss: 0.143, Actor Loss 0.139

Updates 6190, training timesteps 495280, FPS 470
Last 100 training episodes: mean/median reward -58.31/-75.44, min/max -100.0/4.4
Policy entropy: 2.292, Critic Loss: 0.120, Actor Loss -0.095

Updates 6200, training timesteps 496080, FPS 470
Last 100 training episodes: mean/median reward -53.54/-63.02, min/max -100.0/3.4
Policy entropy: 2.291, Critic Loss: 0.099, Actor Loss 0.150

Updates 6210, training timesteps 496880, FPS 470
Last 100 training episodes: mean/median reward -60.72/-69.83, min/max -100.0/7.6
Policy entropy: 2.292, Critic Loss: 0.368, Actor Loss -0.506

Updates 6220, training timesteps 497680, FPS 470
Last 100 training episodes: mean/median reward -55.68/-66.34, min/max -100.0/7.6
Policy entropy: 2.294, Critic Loss: 0.172, Actor Loss -0.375

Updates 6230, training timesteps 498480, FPS 470
Last 100 training episodes: mean/median reward -54.68/-71.67, min/max -100.0/3.6
Policy entropy: 2.291, Critic Loss: 0.222, Actor Loss -0.602

Updates 6240, training timesteps 499280, FPS 470
Last 100 training episodes: mean/median reward -61.76/-73.51, min/max -100.0/0.0
Policy entropy: 2.292, Critic Loss: 0.062, Actor Loss 0.115

Updates 6250, training timesteps 500080, FPS 470
Last 100 training episodes: mean/median reward -54.75/-69.83, min/max -100.0/3.6
Policy entropy: 2.289, Critic Loss: 0.236, Actor Loss -0.015

Updates 6260, training timesteps 500880, FPS 470
Last 100 training episodes: mean/median reward -56.48/-68.09, min/max -100.0/11.8
Policy entropy: 2.290, Critic Loss: 0.094, Actor Loss 0.114

Updates 6270, training timesteps 501680, FPS 470
Last 100 training episodes: mean/median reward -54.30/-63.02, min/max -100.0/11.8
Policy entropy: 2.291, Critic Loss: 0.101, Actor Loss -0.175

Updates 6280, training timesteps 502480, FPS 470
Last 100 training episodes: mean/median reward -58.06/-71.67, min/max -100.0/3.8
Policy entropy: 2.292, Critic Loss: 0.227, Actor Loss -0.415

Updates 6290, training timesteps 503280, FPS 471
Last 100 training episodes: mean/median reward -57.38/-66.34, min/max -100.0/5.5
Policy entropy: 2.290, Critic Loss: 0.075, Actor Loss 0.343

Updates 6300, training timesteps 504080, FPS 470
Last 100 training episodes: mean/median reward -56.04/-63.02, min/max -100.0/5.1
Policy entropy: 2.291, Critic Loss: 0.103, Actor Loss 0.066

Updates 6310, training timesteps 504880, FPS 470
Last 100 training episodes: mean/median reward -57.31/-66.34, min/max -100.0/6.3
Policy entropy: 2.291, Critic Loss: 0.073, Actor Loss 0.159

Updates 6320, training timesteps 505680, FPS 470
Last 100 training episodes: mean/median reward -57.41/-69.83, min/max -104.3/5.4
Policy entropy: 2.291, Critic Loss: 0.075, Actor Loss 0.151

Updates 6330, training timesteps 506480, FPS 471
Last 100 training episodes: mean/median reward -51.74/-61.45, min/max -100.0/3.1
Policy entropy: 2.291, Critic Loss: 0.157, Actor Loss -0.347

Updates 6340, training timesteps 507280, FPS 471
Last 100 training episodes: mean/median reward -55.45/-63.02, min/max -100.0/4.2
Policy entropy: 2.293, Critic Loss: 0.071, Actor Loss 0.104

Updates 6350, training timesteps 508080, FPS 471
Last 100 training episodes: mean/median reward -50.45/-59.87, min/max -100.0/6.8
Policy entropy: 2.291, Critic Loss: 0.212, Actor Loss 0.524

Updates 6360, training timesteps 508880, FPS 470
Last 100 training episodes: mean/median reward -58.69/-69.83, min/max -100.0/6.3
Policy entropy: 2.291, Critic Loss: 0.078, Actor Loss -0.017

Updates 6370, training timesteps 509680, FPS 470
Last 100 training episodes: mean/median reward -50.23/-63.02, min/max -100.0/9.3
Policy entropy: 2.290, Critic Loss: 0.201, Actor Loss 0.129

Updates 6380, training timesteps 510480, FPS 470
Last 100 training episodes: mean/median reward -55.58/-63.02, min/max -100.0/6.7
Policy entropy: 2.291, Critic Loss: 0.212, Actor Loss -0.530

Updates 6390, training timesteps 511280, FPS 470
Last 100 training episodes: mean/median reward -54.59/-66.34, min/max -100.0/7.4
Policy entropy: 2.290, Critic Loss: 0.062, Actor Loss 0.319

Updates 6400, training timesteps 512080, FPS 470
Last 100 training episodes: mean/median reward -48.83/-59.87, min/max -100.0/6.3
Policy entropy: 2.288, Critic Loss: 0.239, Actor Loss -0.568

Updates 6410, training timesteps 512880, FPS 470
Last 100 training episodes: mean/median reward -54.48/-59.87, min/max -100.0/10.5
Policy entropy: 2.293, Critic Loss: 0.212, Actor Loss -0.164

Updates 6420, training timesteps 513680, FPS 470
Last 100 training episodes: mean/median reward -59.22/-73.51, min/max -100.0/7.2
Policy entropy: 2.293, Critic Loss: 0.326, Actor Loss 0.368

Updates 6430, training timesteps 514480, FPS 470
Last 100 training episodes: mean/median reward -55.20/-63.52, min/max -100.0/4.9
Policy entropy: 2.292, Critic Loss: 0.120, Actor Loss -0.117

Updates 6440, training timesteps 515280, FPS 470
Last 100 training episodes: mean/median reward -63.85/-71.67, min/max -100.0/3.1
Policy entropy: 2.294, Critic Loss: 0.156, Actor Loss -0.503

Updates 6450, training timesteps 516080, FPS 470
Last 100 training episodes: mean/median reward -58.82/-73.51, min/max -100.0/1.4
Policy entropy: 2.291, Critic Loss: 0.117, Actor Loss -0.301

Updates 6460, training timesteps 516880, FPS 470
Last 100 training episodes: mean/median reward -58.61/-69.83, min/max -100.0/1.6
Policy entropy: 2.293, Critic Loss: 0.094, Actor Loss 0.173

Updates 6470, training timesteps 517680, FPS 470
Last 100 training episodes: mean/median reward -61.82/-77.38, min/max -100.0/4.2
Policy entropy: 2.292, Critic Loss: 0.125, Actor Loss -0.423

Updates 6480, training timesteps 518480, FPS 470
Last 100 training episodes: mean/median reward -58.41/-68.09, min/max -100.0/5.7
Policy entropy: 2.290, Critic Loss: 0.176, Actor Loss -0.206

Updates 6490, training timesteps 519280, FPS 470
Last 100 training episodes: mean/median reward -53.53/-56.88, min/max -100.0/0.0
Policy entropy: 2.291, Critic Loss: 0.171, Actor Loss -0.116

Updates 6500, training timesteps 520080, FPS 470
Last 100 training episodes: mean/median reward -53.61/-64.68, min/max -100.0/6.6
Policy entropy: 2.292, Critic Loss: 0.152, Actor Loss -0.250

Updates 6510, training timesteps 520880, FPS 470
Last 100 training episodes: mean/median reward -56.07/-63.02, min/max -100.0/0.0
Policy entropy: 2.290, Critic Loss: 0.107, Actor Loss -0.244

Updates 6520, training timesteps 521680, FPS 470
Last 100 training episodes: mean/median reward -53.15/-60.12, min/max -100.0/5.1
Policy entropy: 2.292, Critic Loss: 0.138, Actor Loss -0.208

Updates 6530, training timesteps 522480, FPS 470
Last 100 training episodes: mean/median reward -55.43/-63.02, min/max -100.0/4.2
Policy entropy: 2.293, Critic Loss: 0.158, Actor Loss -0.054

Updates 6540, training timesteps 523280, FPS 470
Last 100 training episodes: mean/median reward -64.43/-71.67, min/max -100.0/4.6
Policy entropy: 2.292, Critic Loss: 0.103, Actor Loss -0.235

Updates 6550, training timesteps 524080, FPS 471
Last 100 training episodes: mean/median reward -53.28/-63.02, min/max -100.0/7.1
Policy entropy: 2.291, Critic Loss: 0.077, Actor Loss 0.103

Updates 6560, training timesteps 524880, FPS 471
Last 100 training episodes: mean/median reward -56.37/-66.34, min/max -100.0/6.1
Policy entropy: 2.293, Critic Loss: 0.164, Actor Loss 0.210

Updates 6570, training timesteps 525680, FPS 471
Last 100 training episodes: mean/median reward -54.26/-64.53, min/max -100.0/5.1
Policy entropy: 2.292, Critic Loss: 0.189, Actor Loss -0.365

Updates 6580, training timesteps 526480, FPS 471
Last 100 training episodes: mean/median reward -54.59/-66.34, min/max -100.0/6.0
Policy entropy: 2.290, Critic Loss: 0.153, Actor Loss -0.093

Updates 6590, training timesteps 527280, FPS 471
Last 100 training episodes: mean/median reward -54.29/-66.34, min/max -100.0/6.8
Policy entropy: 2.290, Critic Loss: 0.182, Actor Loss -0.660

Updates 6600, training timesteps 528080, FPS 471
Last 100 training episodes: mean/median reward -54.68/-71.67, min/max -100.0/6.6
Policy entropy: 2.291, Critic Loss: 0.258, Actor Loss -0.048

Updates 6610, training timesteps 528880, FPS 471
Last 100 training episodes: mean/median reward -58.71/-69.83, min/max -100.0/7.0
Policy entropy: 2.290, Critic Loss: 0.069, Actor Loss 0.333

Updates 6620, training timesteps 529680, FPS 471
Last 100 training episodes: mean/median reward -55.68/-66.34, min/max -100.0/7.0
Policy entropy: 2.292, Critic Loss: 0.089, Actor Loss 0.007

Updates 6630, training timesteps 530480, FPS 471
Last 100 training episodes: mean/median reward -54.66/-66.34, min/max -100.0/0.0
Policy entropy: 2.293, Critic Loss: 0.105, Actor Loss 0.048

Updates 6640, training timesteps 531280, FPS 471
Last 100 training episodes: mean/median reward -54.53/-64.68, min/max -100.0/5.4
Policy entropy: 2.294, Critic Loss: 0.158, Actor Loss 0.132

Updates 6650, training timesteps 532080, FPS 471
Last 100 training episodes: mean/median reward -57.38/-66.84, min/max -100.0/0.0
Policy entropy: 2.292, Critic Loss: 0.137, Actor Loss -0.408

Updates 6660, training timesteps 532880, FPS 471
Last 100 training episodes: mean/median reward -58.44/-69.83, min/max -100.0/3.4
Policy entropy: 2.291, Critic Loss: 0.119, Actor Loss -0.249

Updates 6670, training timesteps 533680, FPS 471
Last 100 training episodes: mean/median reward -54.65/-64.76, min/max -100.0/0.0
Policy entropy: 2.291, Critic Loss: 0.125, Actor Loss 0.439

Updates 6680, training timesteps 534480, FPS 471
Last 100 training episodes: mean/median reward -58.85/-73.51, min/max -100.0/6.0
Policy entropy: 2.291, Critic Loss: 0.188, Actor Loss 0.519

Updates 6690, training timesteps 535280, FPS 471
Last 100 training episodes: mean/median reward -57.70/-69.83, min/max -100.0/4.6
Policy entropy: 2.291, Critic Loss: 0.108, Actor Loss -0.174

Updates 6700, training timesteps 536080, FPS 471
Last 100 training episodes: mean/median reward -49.63/-61.45, min/max -100.0/3.9
Policy entropy: 2.292, Critic Loss: 0.097, Actor Loss 0.183

Updates 6710, training timesteps 536880, FPS 471
Last 100 training episodes: mean/median reward -52.36/-63.02, min/max -100.0/5.7
Policy entropy: 2.292, Critic Loss: 0.145, Actor Loss 0.518

Updates 6720, training timesteps 537680, FPS 471
Last 100 training episodes: mean/median reward -57.07/-66.34, min/max -100.0/5.1
Policy entropy: 2.292, Critic Loss: 0.116, Actor Loss 0.065

Updates 6730, training timesteps 538480, FPS 471
Last 100 training episodes: mean/median reward -58.36/-69.83, min/max -100.0/1.7
Policy entropy: 2.292, Critic Loss: 0.146, Actor Loss -0.128

Updates 6740, training timesteps 539280, FPS 471
Last 100 training episodes: mean/median reward -53.55/-59.87, min/max -100.0/0.0
Policy entropy: 2.293, Critic Loss: 0.069, Actor Loss 0.212

Updates 6750, training timesteps 540080, FPS 471
Last 100 training episodes: mean/median reward -55.21/-63.11, min/max -100.0/7.0
Policy entropy: 2.294, Critic Loss: 0.242, Actor Loss -0.875

Updates 6760, training timesteps 540880, FPS 471
Last 100 training episodes: mean/median reward -53.13/-63.06, min/max -100.0/2.9
Policy entropy: 2.292, Critic Loss: 0.142, Actor Loss -0.171

Updates 6770, training timesteps 541680, FPS 471
Last 100 training episodes: mean/median reward -51.42/-61.45, min/max -100.0/8.9
Policy entropy: 2.287, Critic Loss: 0.129, Actor Loss -0.036

Updates 6780, training timesteps 542480, FPS 471
Last 100 training episodes: mean/median reward -57.06/-69.83, min/max -100.0/2.4
Policy entropy: 2.290, Critic Loss: 0.088, Actor Loss 0.141

Updates 6790, training timesteps 543280, FPS 471
Last 100 training episodes: mean/median reward -51.89/-59.87, min/max -100.0/6.3
Policy entropy: 2.291, Critic Loss: 0.110, Actor Loss 0.269

Updates 6800, training timesteps 544080, FPS 471
Last 100 training episodes: mean/median reward -59.09/-69.83, min/max -100.0/6.3
Policy entropy: 2.290, Critic Loss: 0.210, Actor Loss -0.206

Updates 6810, training timesteps 544880, FPS 471
Last 100 training episodes: mean/median reward -54.69/-64.68, min/max -100.0/3.2
Policy entropy: 2.289, Critic Loss: 0.125, Actor Loss 0.281

Updates 6820, training timesteps 545680, FPS 471
Last 100 training episodes: mean/median reward -55.65/-66.34, min/max -100.0/8.8
Policy entropy: 2.290, Critic Loss: 0.177, Actor Loss -0.285

Updates 6830, training timesteps 546480, FPS 471
Last 100 training episodes: mean/median reward -53.09/-61.45, min/max -100.0/8.8
Policy entropy: 2.290, Critic Loss: 0.123, Actor Loss 0.261

Updates 6840, training timesteps 547280, FPS 471
Last 100 training episodes: mean/median reward -55.69/-66.34, min/max -100.0/9.1
Policy entropy: 2.290, Critic Loss: 0.273, Actor Loss -0.250

Updates 6850, training timesteps 548080, FPS 471
Last 100 training episodes: mean/median reward -57.56/-73.51, min/max -100.0/4.6
Policy entropy: 2.292, Critic Loss: 0.068, Actor Loss 0.273

Updates 6860, training timesteps 548880, FPS 471
Last 100 training episodes: mean/median reward -56.97/-68.09, min/max -100.0/3.1
Policy entropy: 2.292, Critic Loss: 0.163, Actor Loss -0.153

Updates 6870, training timesteps 549680, FPS 471
Last 100 training episodes: mean/median reward -57.44/-66.34, min/max -100.0/13.6
Policy entropy: 2.292, Critic Loss: 0.196, Actor Loss 0.278

Updates 6880, training timesteps 550480, FPS 471
Last 100 training episodes: mean/median reward -53.80/-66.34, min/max -100.0/5.1
Policy entropy: 2.291, Critic Loss: 0.189, Actor Loss -0.167

Updates 6890, training timesteps 551280, FPS 471
Last 100 training episodes: mean/median reward -62.23/-73.51, min/max -100.0/4.4
Policy entropy: 2.291, Critic Loss: 0.062, Actor Loss 0.187

Updates 6900, training timesteps 552080, FPS 471
Last 100 training episodes: mean/median reward -56.72/-69.83, min/max -100.0/7.0
Policy entropy: 2.292, Critic Loss: 0.177, Actor Loss -0.377

Updates 6910, training timesteps 552880, FPS 471
Last 100 training episodes: mean/median reward -53.27/-63.02, min/max -100.0/1.5
Policy entropy: 2.292, Critic Loss: 0.082, Actor Loss 0.128

Updates 6920, training timesteps 553680, FPS 471
Last 100 training episodes: mean/median reward -60.12/-71.67, min/max -100.0/3.1
Policy entropy: 2.290, Critic Loss: 0.183, Actor Loss -0.015

Updates 6930, training timesteps 554480, FPS 471
Last 100 training episodes: mean/median reward -54.71/-66.22, min/max -100.0/0.0
Policy entropy: 2.293, Critic Loss: 0.152, Actor Loss -0.509

Updates 6940, training timesteps 555280, FPS 471
Last 100 training episodes: mean/median reward -63.72/-77.38, min/max -100.0/3.1
Policy entropy: 2.293, Critic Loss: 0.138, Actor Loss -0.049

Updates 6950, training timesteps 556080, FPS 471
Last 100 training episodes: mean/median reward -56.48/-65.64, min/max -100.0/12.4
Policy entropy: 2.292, Critic Loss: 0.143, Actor Loss -0.325

Updates 6960, training timesteps 556880, FPS 471
Last 100 training episodes: mean/median reward -45.82/-56.88, min/max -100.0/6.6
Policy entropy: 2.293, Critic Loss: 0.319, Actor Loss -0.842

Updates 6970, training timesteps 557680, FPS 471
Last 100 training episodes: mean/median reward -48.56/-58.38, min/max -100.0/5.1
Policy entropy: 2.292, Critic Loss: 0.109, Actor Loss -0.145

Updates 6980, training timesteps 558480, FPS 471
Last 100 training episodes: mean/median reward -54.56/-66.34, min/max -100.0/0.0
Policy entropy: 2.292, Critic Loss: 0.110, Actor Loss -0.013

Updates 6990, training timesteps 559280, FPS 471
Last 100 training episodes: mean/median reward -54.72/-59.87, min/max -100.0/2.1
Policy entropy: 2.294, Critic Loss: 0.155, Actor Loss -0.261

Updates 7000, training timesteps 560080, FPS 471
Last 100 training episodes: mean/median reward -58.94/-73.51, min/max -100.0/6.6
Policy entropy: 2.294, Critic Loss: 0.152, Actor Loss -0.043

Updates 7010, training timesteps 560880, FPS 471
Last 100 training episodes: mean/median reward -60.62/-71.67, min/max -100.0/6.8
Policy entropy: 2.291, Critic Loss: 0.085, Actor Loss 0.343

Updates 7020, training timesteps 561680, FPS 471
Last 100 training episodes: mean/median reward -59.13/-66.34, min/max -100.0/5.1
Policy entropy: 2.293, Critic Loss: 0.273, Actor Loss -0.812

Updates 7030, training timesteps 562480, FPS 471
Last 100 training episodes: mean/median reward -54.09/-63.02, min/max -100.0/12.8
Policy entropy: 2.291, Critic Loss: 0.384, Actor Loss -0.528

Updates 7040, training timesteps 563280, FPS 471
Last 100 training episodes: mean/median reward -48.08/-51.74, min/max -100.0/4.2
Policy entropy: 2.293, Critic Loss: 0.068, Actor Loss 0.106

Updates 7050, training timesteps 564080, FPS 471
Last 100 training episodes: mean/median reward -53.87/-63.02, min/max -100.0/4.6
Policy entropy: 2.292, Critic Loss: 0.141, Actor Loss 0.288

Updates 7060, training timesteps 564880, FPS 471
Last 100 training episodes: mean/median reward -55.44/-64.68, min/max -100.0/6.3
Policy entropy: 2.293, Critic Loss: 0.189, Actor Loss -0.389

Updates 7070, training timesteps 565680, FPS 471
Last 100 training episodes: mean/median reward -57.50/-73.51, min/max -100.0/7.8
Policy entropy: 2.293, Critic Loss: 0.201, Actor Loss 0.287

Updates 7080, training timesteps 566480, FPS 471
Last 100 training episodes: mean/median reward -62.61/-73.51, min/max -100.0/8.8
Policy entropy: 2.289, Critic Loss: 0.228, Actor Loss -0.323

Updates 7090, training timesteps 567280, FPS 471
Last 100 training episodes: mean/median reward -59.46/-71.67, min/max -100.0/7.7
Policy entropy: 2.289, Critic Loss: 0.086, Actor Loss -0.084

Updates 7100, training timesteps 568080, FPS 471
Last 100 training episodes: mean/median reward -51.63/-59.87, min/max -100.0/8.1
Policy entropy: 2.291, Critic Loss: 0.145, Actor Loss -0.378

Updates 7110, training timesteps 568880, FPS 471
Last 100 training episodes: mean/median reward -60.00/-75.44, min/max -100.0/7.0
Policy entropy: 2.292, Critic Loss: 0.067, Actor Loss -0.144

Updates 7120, training timesteps 569680, FPS 471
Last 100 training episodes: mean/median reward -58.01/-71.67, min/max -100.0/3.6
Policy entropy: 2.291, Critic Loss: 0.142, Actor Loss 0.402

Updates 7130, training timesteps 570480, FPS 471
Last 100 training episodes: mean/median reward -53.25/-66.34, min/max -100.0/5.7
Policy entropy: 2.293, Critic Loss: 0.128, Actor Loss -0.030

Updates 7140, training timesteps 571280, FPS 471
Last 100 training episodes: mean/median reward -51.87/-54.04, min/max -100.0/7.0
Policy entropy: 2.292, Critic Loss: 0.170, Actor Loss -0.027

Updates 7150, training timesteps 572080, FPS 471
Last 100 training episodes: mean/median reward -56.17/-66.43, min/max -100.0/5.7
Policy entropy: 2.291, Critic Loss: 0.163, Actor Loss -0.048

Updates 7160, training timesteps 572880, FPS 471
Last 100 training episodes: mean/median reward -57.97/-66.34, min/max -100.0/0.0
Policy entropy: 2.292, Critic Loss: 0.063, Actor Loss 0.025

Updates 7170, training timesteps 573680, FPS 471
Last 100 training episodes: mean/median reward -55.90/-66.34, min/max -100.0/7.4
Policy entropy: 2.295, Critic Loss: 0.170, Actor Loss -0.325

Updates 7180, training timesteps 574480, FPS 471
Last 100 training episodes: mean/median reward -59.40/-66.34, min/max -100.0/6.3
Policy entropy: 2.293, Critic Loss: 0.100, Actor Loss -0.014

Updates 7190, training timesteps 575280, FPS 471
Last 100 training episodes: mean/median reward -59.00/-69.83, min/max -100.0/1.9
Policy entropy: 2.293, Critic Loss: 0.123, Actor Loss -0.312

Updates 7200, training timesteps 576080, FPS 471
Last 100 training episodes: mean/median reward -59.43/-73.51, min/max -100.0/7.8
Policy entropy: 2.287, Critic Loss: 0.092, Actor Loss 0.058

Updates 7210, training timesteps 576880, FPS 471
Last 100 training episodes: mean/median reward -54.23/-66.34, min/max -100.0/5.1
Policy entropy: 2.290, Critic Loss: 0.266, Actor Loss -0.732

Updates 7220, training timesteps 577680, FPS 471
Last 100 training episodes: mean/median reward -51.88/-61.69, min/max -100.0/6.0
Policy entropy: 2.292, Critic Loss: 0.109, Actor Loss 0.143

Updates 7230, training timesteps 578480, FPS 471
Last 100 training episodes: mean/median reward -61.57/-73.51, min/max -100.0/5.7
Policy entropy: 2.291, Critic Loss: 0.065, Actor Loss 0.296

Updates 7240, training timesteps 579280, FPS 471
Last 100 training episodes: mean/median reward -60.47/-73.51, min/max -100.0/13.0
Policy entropy: 2.292, Critic Loss: 0.078, Actor Loss 0.281

Updates 7250, training timesteps 580080, FPS 471
Last 100 training episodes: mean/median reward -51.45/-59.87, min/max -100.0/2.5
Policy entropy: 2.290, Critic Loss: 0.102, Actor Loss -0.215

Updates 7260, training timesteps 580880, FPS 471
Last 100 training episodes: mean/median reward -57.80/-71.67, min/max -100.0/5.4
Policy entropy: 2.289, Critic Loss: 0.201, Actor Loss -0.235

Updates 7270, training timesteps 581680, FPS 471
Last 100 training episodes: mean/median reward -56.30/-66.34, min/max -100.0/3.6
Policy entropy: 2.290, Critic Loss: 0.180, Actor Loss -0.224

Updates 7280, training timesteps 582480, FPS 471
Last 100 training episodes: mean/median reward -55.67/-66.34, min/max -100.0/8.2
Policy entropy: 2.288, Critic Loss: 0.123, Actor Loss 0.346

Updates 7290, training timesteps 583280, FPS 471
Last 100 training episodes: mean/median reward -58.22/-68.09, min/max -100.0/4.2
Policy entropy: 2.289, Critic Loss: 0.084, Actor Loss 0.594

Updates 7300, training timesteps 584080, FPS 471
Last 100 training episodes: mean/median reward -55.63/-63.02, min/max -100.0/4.6
Policy entropy: 2.291, Critic Loss: 0.137, Actor Loss -0.033

Updates 7310, training timesteps 584880, FPS 471
Last 100 training episodes: mean/median reward -61.44/-77.38, min/max -100.0/5.7
Policy entropy: 2.291, Critic Loss: 0.154, Actor Loss 0.461

Updates 7320, training timesteps 585680, FPS 471
Last 100 training episodes: mean/median reward -47.96/-55.46, min/max -100.0/6.3
Policy entropy: 2.291, Critic Loss: 0.247, Actor Loss -0.770

Updates 7330, training timesteps 586480, FPS 471
Last 100 training episodes: mean/median reward -58.03/-66.34, min/max -100.0/1.6
Policy entropy: 2.290, Critic Loss: 0.125, Actor Loss 0.098

Updates 7340, training timesteps 587280, FPS 471
Last 100 training episodes: mean/median reward -53.49/-66.34, min/max -100.0/4.9
Policy entropy: 2.290, Critic Loss: 0.093, Actor Loss 0.023

Updates 7350, training timesteps 588080, FPS 471
Last 100 training episodes: mean/median reward -53.88/-63.02, min/max -100.0/6.6
Policy entropy: 2.291, Critic Loss: 0.114, Actor Loss -0.303

Updates 7360, training timesteps 588880, FPS 471
Last 100 training episodes: mean/median reward -57.87/-67.35, min/max -100.0/6.0
Policy entropy: 2.290, Critic Loss: 0.103, Actor Loss 0.037

Updates 7370, training timesteps 589680, FPS 471
Last 100 training episodes: mean/median reward -53.87/-64.68, min/max -100.0/3.1
Policy entropy: 2.290, Critic Loss: 0.144, Actor Loss 0.016

Updates 7380, training timesteps 590480, FPS 471
Last 100 training episodes: mean/median reward -55.35/-63.02, min/max -100.0/11.4
Policy entropy: 2.291, Critic Loss: 0.149, Actor Loss -0.108

Updates 7390, training timesteps 591280, FPS 471
Last 100 training episodes: mean/median reward -55.45/-66.34, min/max -100.0/3.4
Policy entropy: 2.290, Critic Loss: 0.098, Actor Loss 0.101

Updates 7400, training timesteps 592080, FPS 471
Last 100 training episodes: mean/median reward -50.11/-55.46, min/max -100.0/3.9
Policy entropy: 2.292, Critic Loss: 0.128, Actor Loss -0.343

Updates 7410, training timesteps 592880, FPS 471
Last 100 training episodes: mean/median reward -58.46/-66.34, min/max -100.0/8.8
Policy entropy: 2.291, Critic Loss: 0.190, Actor Loss 0.303

Updates 7420, training timesteps 593680, FPS 471
Last 100 training episodes: mean/median reward -54.68/-59.87, min/max -100.0/4.6
Policy entropy: 2.289, Critic Loss: 0.078, Actor Loss 0.462

Updates 7430, training timesteps 594480, FPS 471
Last 100 training episodes: mean/median reward -55.62/-66.34, min/max -100.0/2.5
Policy entropy: 2.292, Critic Loss: 0.141, Actor Loss 0.165

Updates 7440, training timesteps 595280, FPS 471
Last 100 training episodes: mean/median reward -53.81/-64.68, min/max -100.0/4.4
Policy entropy: 2.293, Critic Loss: 0.074, Actor Loss 0.190

Updates 7450, training timesteps 596080, FPS 471
Last 100 training episodes: mean/median reward -58.55/-73.51, min/max -100.0/10.2
Policy entropy: 2.293, Critic Loss: 0.092, Actor Loss -0.139

Updates 7460, training timesteps 596880, FPS 471
Last 100 training episodes: mean/median reward -57.34/-69.83, min/max -100.0/4.9
Policy entropy: 2.292, Critic Loss: 0.100, Actor Loss 0.329

Updates 7470, training timesteps 597680, FPS 471
Last 100 training episodes: mean/median reward -57.24/-69.83, min/max -100.0/6.3
Policy entropy: 2.293, Critic Loss: 0.159, Actor Loss 0.206

Updates 7480, training timesteps 598480, FPS 471
Last 100 training episodes: mean/median reward -56.77/-69.83, min/max -100.0/6.3
Policy entropy: 2.291, Critic Loss: 0.315, Actor Loss -0.646

Updates 7490, training timesteps 599280, FPS 471
Last 100 training episodes: mean/median reward -59.78/-70.50, min/max -100.0/7.0
Policy entropy: 2.291, Critic Loss: 0.130, Actor Loss 0.290

Updates 7500, training timesteps 600080, FPS 471
Last 100 training episodes: mean/median reward -52.82/-63.27, min/max -100.0/6.0
Policy entropy: 2.290, Critic Loss: 0.143, Actor Loss 0.014

Updates 7510, training timesteps 600880, FPS 471
Last 100 training episodes: mean/median reward -54.89/-66.34, min/max -100.0/8.4
Policy entropy: 2.291, Critic Loss: 0.090, Actor Loss -0.117

Updates 7520, training timesteps 601680, FPS 471
Last 100 training episodes: mean/median reward -60.28/-73.51, min/max -100.0/0.0
Policy entropy: 2.290, Critic Loss: 0.067, Actor Loss 0.164

Updates 7530, training timesteps 602480, FPS 471
Last 100 training episodes: mean/median reward -56.17/-69.83, min/max -100.0/4.0
Policy entropy: 2.292, Critic Loss: 0.132, Actor Loss -0.313

Updates 7540, training timesteps 603280, FPS 471
Last 100 training episodes: mean/median reward -58.49/-73.51, min/max -100.0/12.0
Policy entropy: 2.292, Critic Loss: 0.129, Actor Loss -0.230

Updates 7550, training timesteps 604080, FPS 471
Last 100 training episodes: mean/median reward -52.58/-64.93, min/max -100.0/7.8
Policy entropy: 2.295, Critic Loss: 0.103, Actor Loss -0.206

Updates 7560, training timesteps 604880, FPS 471
Last 100 training episodes: mean/median reward -53.87/-66.34, min/max -100.0/5.1
Policy entropy: 2.291, Critic Loss: 0.129, Actor Loss 0.019

Updates 7570, training timesteps 605680, FPS 471
Last 100 training episodes: mean/median reward -57.91/-69.83, min/max -100.0/8.6
Policy entropy: 2.293, Critic Loss: 0.082, Actor Loss -0.057

Updates 7580, training timesteps 606480, FPS 471
Last 100 training episodes: mean/median reward -54.40/-66.34, min/max -100.0/5.1
Policy entropy: 2.292, Critic Loss: 0.057, Actor Loss -0.028

Updates 7590, training timesteps 607280, FPS 471
Last 100 training episodes: mean/median reward -58.25/-73.51, min/max -100.0/6.0
Policy entropy: 2.291, Critic Loss: 0.106, Actor Loss -0.025

Updates 7600, training timesteps 608080, FPS 471
Last 100 training episodes: mean/median reward -50.73/-59.87, min/max -100.0/7.0
Policy entropy: 2.293, Critic Loss: 0.090, Actor Loss 0.211

Updates 7610, training timesteps 608880, FPS 471
Last 100 training episodes: mean/median reward -50.85/-55.46, min/max -100.0/7.0
Policy entropy: 2.289, Critic Loss: 0.083, Actor Loss -0.004

Updates 7620, training timesteps 609680, FPS 471
Last 100 training episodes: mean/median reward -51.98/-58.38, min/max -100.0/6.0
Policy entropy: 2.288, Critic Loss: 0.108, Actor Loss 0.080

Updates 7630, training timesteps 610480, FPS 471
Last 100 training episodes: mean/median reward -54.82/-63.34, min/max -100.0/1.7
Policy entropy: 2.291, Critic Loss: 0.080, Actor Loss 0.018

Updates 7640, training timesteps 611280, FPS 471
Last 100 training episodes: mean/median reward -52.81/-63.02, min/max -100.0/3.4
Policy entropy: 2.291, Critic Loss: 0.154, Actor Loss -0.126

Updates 7650, training timesteps 612080, FPS 471
Last 100 training episodes: mean/median reward -60.65/-73.51, min/max -100.0/12.0
Policy entropy: 2.294, Critic Loss: 0.099, Actor Loss -0.053

Updates 7660, training timesteps 612880, FPS 471
Last 100 training episodes: mean/median reward -55.54/-68.09, min/max -100.0/12.0
Policy entropy: 2.292, Critic Loss: 0.056, Actor Loss 0.492

Updates 7670, training timesteps 613680, FPS 471
Last 100 training episodes: mean/median reward -59.26/-69.83, min/max -100.0/5.1
Policy entropy: 2.292, Critic Loss: 0.105, Actor Loss -0.369

Updates 7680, training timesteps 614480, FPS 471
Last 100 training episodes: mean/median reward -59.77/-69.83, min/max -100.0/5.4
Policy entropy: 2.292, Critic Loss: 0.280, Actor Loss -0.241

Updates 7690, training timesteps 615280, FPS 471
Last 100 training episodes: mean/median reward -50.66/-61.45, min/max -100.0/7.0
Policy entropy: 2.291, Critic Loss: 0.137, Actor Loss 0.228

Updates 7700, training timesteps 616080, FPS 471
Last 100 training episodes: mean/median reward -58.97/-73.51, min/max -100.0/11.5
Policy entropy: 2.291, Critic Loss: 0.087, Actor Loss -0.010

Updates 7710, training timesteps 616880, FPS 471
Last 100 training episodes: mean/median reward -59.29/-69.83, min/max -100.0/5.4
Policy entropy: 2.293, Critic Loss: 0.083, Actor Loss 0.025

Updates 7720, training timesteps 617680, FPS 471
Last 100 training episodes: mean/median reward -57.90/-66.34, min/max -100.0/13.0
Policy entropy: 2.293, Critic Loss: 0.091, Actor Loss 0.041

Updates 7730, training timesteps 618480, FPS 471
Last 100 training episodes: mean/median reward -56.92/-64.68, min/max -100.0/6.0
Policy entropy: 2.294, Critic Loss: 0.117, Actor Loss -0.007

Updates 7740, training timesteps 619280, FPS 471
Last 100 training episodes: mean/median reward -56.28/-66.34, min/max -100.0/4.2
Policy entropy: 2.290, Critic Loss: 0.113, Actor Loss -0.093

Updates 7750, training timesteps 620080, FPS 471
Last 100 training episodes: mean/median reward -56.51/-69.83, min/max -100.0/4.0
Policy entropy: 2.294, Critic Loss: 0.169, Actor Loss -0.444

Updates 7760, training timesteps 620880, FPS 471
Last 100 training episodes: mean/median reward -58.53/-68.09, min/max -100.0/2.9
Policy entropy: 2.294, Critic Loss: 0.119, Actor Loss -0.013

Updates 7770, training timesteps 621680, FPS 471
Last 100 training episodes: mean/median reward -59.34/-75.44, min/max -100.0/4.6
Policy entropy: 2.292, Critic Loss: 0.065, Actor Loss 0.069

Updates 7780, training timesteps 622480, FPS 471
Last 100 training episodes: mean/median reward -56.20/-69.83, min/max -100.0/12.0
Policy entropy: 2.293, Critic Loss: 0.207, Actor Loss -0.517

Updates 7790, training timesteps 623280, FPS 471
Last 100 training episodes: mean/median reward -55.50/-71.67, min/max -100.0/5.7
Policy entropy: 2.290, Critic Loss: 0.087, Actor Loss -0.000

Updates 7800, training timesteps 624080, FPS 471
Last 100 training episodes: mean/median reward -56.20/-66.06, min/max -100.0/12.1
Policy entropy: 2.293, Critic Loss: 0.162, Actor Loss -0.258

Updates 7810, training timesteps 624880, FPS 471
Last 100 training episodes: mean/median reward -53.55/-66.34, min/max -100.0/6.6
Policy entropy: 2.290, Critic Loss: 0.142, Actor Loss -0.148

Updates 7820, training timesteps 625680, FPS 471
Last 100 training episodes: mean/median reward -57.96/-66.34, min/max -100.0/6.3
Policy entropy: 2.288, Critic Loss: 0.125, Actor Loss 0.085

Updates 7830, training timesteps 626480, FPS 471
Last 100 training episodes: mean/median reward -52.29/-59.87, min/max -100.0/6.0
Policy entropy: 2.287, Critic Loss: 0.119, Actor Loss -0.074

Updates 7840, training timesteps 627280, FPS 471
Last 100 training episodes: mean/median reward -51.63/-62.14, min/max -100.0/5.7
Policy entropy: 2.289, Critic Loss: 0.061, Actor Loss 0.223

Updates 7850, training timesteps 628080, FPS 471
Last 100 training episodes: mean/median reward -58.38/-68.09, min/max -100.0/10.6
Policy entropy: 2.289, Critic Loss: 0.147, Actor Loss -0.281

Updates 7860, training timesteps 628880, FPS 471
Last 100 training episodes: mean/median reward -59.41/-73.51, min/max -100.0/10.6
Policy entropy: 2.291, Critic Loss: 0.103, Actor Loss -0.091

Updates 7870, training timesteps 629680, FPS 471
Last 100 training episodes: mean/median reward -44.40/-50.05, min/max -100.0/9.3
Policy entropy: 2.289, Critic Loss: 0.104, Actor Loss 0.093

Updates 7880, training timesteps 630480, FPS 471
Last 100 training episodes: mean/median reward -53.06/-63.44, min/max -100.0/6.3
Policy entropy: 2.292, Critic Loss: 0.166, Actor Loss -0.072

Updates 7890, training timesteps 631280, FPS 471
Last 100 training episodes: mean/median reward -53.30/-63.02, min/max -100.0/10.9
Policy entropy: 2.293, Critic Loss: 0.205, Actor Loss -0.480

Updates 7900, training timesteps 632080, FPS 471
Last 100 training episodes: mean/median reward -57.71/-69.83, min/max -100.0/5.7
Policy entropy: 2.289, Critic Loss: 0.130, Actor Loss -0.087

Updates 7910, training timesteps 632880, FPS 471
Last 100 training episodes: mean/median reward -59.20/-69.83, min/max -100.0/4.6
Policy entropy: 2.289, Critic Loss: 0.093, Actor Loss 0.361

Updates 7920, training timesteps 633680, FPS 471
Last 100 training episodes: mean/median reward -55.26/-66.34, min/max -100.0/6.1
Policy entropy: 2.294, Critic Loss: 0.172, Actor Loss -0.204

Updates 7930, training timesteps 634480, FPS 471
Last 100 training episodes: mean/median reward -55.73/-68.09, min/max -100.0/13.7
Policy entropy: 2.292, Critic Loss: 0.075, Actor Loss 0.088

Updates 7940, training timesteps 635280, FPS 471
Last 100 training episodes: mean/median reward -56.70/-66.34, min/max -100.0/9.3
Policy entropy: 2.292, Critic Loss: 0.166, Actor Loss -0.466

Updates 7950, training timesteps 636080, FPS 471
Last 100 training episodes: mean/median reward -56.09/-68.09, min/max -100.0/4.4
Policy entropy: 2.292, Critic Loss: 0.142, Actor Loss -0.178

Updates 7960, training timesteps 636880, FPS 471
Last 100 training episodes: mean/median reward -56.35/-66.34, min/max -100.0/4.2
Policy entropy: 2.291, Critic Loss: 0.112, Actor Loss -0.095

Updates 7970, training timesteps 637680, FPS 471
Last 100 training episodes: mean/median reward -54.62/-61.45, min/max -100.0/5.1
Policy entropy: 2.293, Critic Loss: 0.109, Actor Loss -0.156

Updates 7980, training timesteps 638480, FPS 471
Last 100 training episodes: mean/median reward -56.24/-59.87, min/max -100.0/4.0
Policy entropy: 2.293, Critic Loss: 0.100, Actor Loss 0.030

Updates 7990, training timesteps 639280, FPS 471
Last 100 training episodes: mean/median reward -53.57/-61.45, min/max -100.0/7.0
Policy entropy: 2.289, Critic Loss: 0.253, Actor Loss -0.636

Updates 8000, training timesteps 640080, FPS 471
Last 100 training episodes: mean/median reward -56.03/-63.02, min/max -100.0/2.3
Policy entropy: 2.292, Critic Loss: 0.096, Actor Loss -0.140

Updates 8010, training timesteps 640880, FPS 471
Last 100 training episodes: mean/median reward -55.38/-64.68, min/max -100.0/8.1
Policy entropy: 2.291, Critic Loss: 0.143, Actor Loss 0.096

Updates 8020, training timesteps 641680, FPS 471
Last 100 training episodes: mean/median reward -51.98/-63.02, min/max -100.0/10.1
Policy entropy: 2.291, Critic Loss: 0.289, Actor Loss -0.638

Updates 8030, training timesteps 642480, FPS 471
Last 100 training episodes: mean/median reward -53.73/-64.68, min/max -100.0/6.1
Policy entropy: 2.292, Critic Loss: 0.150, Actor Loss -0.087

Updates 8040, training timesteps 643280, FPS 471
Last 100 training episodes: mean/median reward -60.39/-69.83, min/max -100.0/4.4
Policy entropy: 2.292, Critic Loss: 0.160, Actor Loss -0.021

Updates 8050, training timesteps 644080, FPS 471
Last 100 training episodes: mean/median reward -54.63/-67.76, min/max -100.0/4.9
Policy entropy: 2.291, Critic Loss: 0.181, Actor Loss -0.129

Updates 8060, training timesteps 644880, FPS 471
Last 100 training episodes: mean/median reward -56.86/-68.09, min/max -100.0/0.0
Policy entropy: 2.293, Critic Loss: 0.176, Actor Loss -0.289

Updates 8070, training timesteps 645680, FPS 471
Last 100 training episodes: mean/median reward -64.02/-75.44, min/max -100.0/8.0
Policy entropy: 2.291, Critic Loss: 0.102, Actor Loss -0.324

Updates 8080, training timesteps 646480, FPS 471
Last 100 training episodes: mean/median reward -56.88/-69.83, min/max -100.0/2.6
Policy entropy: 2.293, Critic Loss: 0.064, Actor Loss 0.158

Updates 8090, training timesteps 647280, FPS 471
Last 100 training episodes: mean/median reward -45.28/-54.04, min/max -100.0/5.4
Policy entropy: 2.292, Critic Loss: 0.096, Actor Loss 0.291

Updates 8100, training timesteps 648080, FPS 471
Last 100 training episodes: mean/median reward -54.17/-64.68, min/max -100.0/6.0
Policy entropy: 2.290, Critic Loss: 0.156, Actor Loss -0.174

Updates 8110, training timesteps 648880, FPS 471
Last 100 training episodes: mean/median reward -58.18/-66.34, min/max -100.0/6.0
Policy entropy: 2.291, Critic Loss: 0.090, Actor Loss 0.264

Updates 8120, training timesteps 649680, FPS 471
Last 100 training episodes: mean/median reward -55.96/-68.09, min/max -100.0/0.0
Policy entropy: 2.293, Critic Loss: 0.077, Actor Loss 0.178

Updates 8130, training timesteps 650480, FPS 471
Last 100 training episodes: mean/median reward -52.55/-63.02, min/max -100.0/2.5
Policy entropy: 2.294, Critic Loss: 0.108, Actor Loss 0.196

Updates 8140, training timesteps 651280, FPS 471
Last 100 training episodes: mean/median reward -50.94/-59.87, min/max -100.0/7.3
Policy entropy: 2.293, Critic Loss: 0.106, Actor Loss 0.110

Updates 8150, training timesteps 652080, FPS 471
Last 100 training episodes: mean/median reward -57.79/-69.83, min/max -100.0/4.9
Policy entropy: 2.292, Critic Loss: 0.151, Actor Loss 0.036

Updates 8160, training timesteps 652880, FPS 471
Last 100 training episodes: mean/median reward -51.21/-56.88, min/max -100.0/2.8
Policy entropy: 2.293, Critic Loss: 0.218, Actor Loss -0.063

Updates 8170, training timesteps 653680, FPS 471
Last 100 training episodes: mean/median reward -54.22/-63.02, min/max -100.0/9.5
Policy entropy: 2.292, Critic Loss: 0.136, Actor Loss 0.387

Updates 8180, training timesteps 654480, FPS 471
Last 100 training episodes: mean/median reward -46.41/-48.77, min/max -100.0/6.0
Policy entropy: 2.291, Critic Loss: 0.112, Actor Loss 0.014

Updates 8190, training timesteps 655280, FPS 471
Last 100 training episodes: mean/median reward -50.15/-58.38, min/max -100.0/6.2
Policy entropy: 2.293, Critic Loss: 0.142, Actor Loss -0.012

Updates 8200, training timesteps 656080, FPS 471
Last 100 training episodes: mean/median reward -51.26/-58.60, min/max -100.0/4.9
Policy entropy: 2.294, Critic Loss: 0.150, Actor Loss 0.183

Updates 8210, training timesteps 656880, FPS 471
Last 100 training episodes: mean/median reward -58.29/-73.51, min/max -100.0/0.0
Policy entropy: 2.293, Critic Loss: 0.192, Actor Loss -0.328

Updates 8220, training timesteps 657680, FPS 471
Last 100 training episodes: mean/median reward -54.54/-63.02, min/max -100.0/8.0
Policy entropy: 2.292, Critic Loss: 0.166, Actor Loss -0.366

Updates 8230, training timesteps 658480, FPS 471
Last 100 training episodes: mean/median reward -52.56/-63.02, min/max -100.0/6.6
Policy entropy: 2.287, Critic Loss: 0.072, Actor Loss 0.159

Updates 8240, training timesteps 659280, FPS 471
Last 100 training episodes: mean/median reward -58.35/-69.83, min/max -100.0/2.0
Policy entropy: 2.291, Critic Loss: 0.128, Actor Loss -0.378

Updates 8250, training timesteps 660080, FPS 471
Last 100 training episodes: mean/median reward -64.39/-77.38, min/max -100.0/7.0
Policy entropy: 2.291, Critic Loss: 0.161, Actor Loss 0.367

Updates 8260, training timesteps 660880, FPS 471
Last 100 training episodes: mean/median reward -55.08/-68.09, min/max -100.0/7.0
Policy entropy: 2.292, Critic Loss: 0.163, Actor Loss -0.316

Updates 8270, training timesteps 661680, FPS 471
Last 100 training episodes: mean/median reward -52.71/-59.87, min/max -100.0/3.2
Policy entropy: 2.292, Critic Loss: 0.145, Actor Loss 0.049

Updates 8280, training timesteps 662480, FPS 471
Last 100 training episodes: mean/median reward -58.30/-66.34, min/max -100.0/2.9
Policy entropy: 2.290, Critic Loss: 0.111, Actor Loss 0.326

Updates 8290, training timesteps 663280, FPS 471
Last 100 training episodes: mean/median reward -62.34/-73.51, min/max -100.0/3.6
Policy entropy: 2.290, Critic Loss: 0.064, Actor Loss -0.018

Updates 8300, training timesteps 664080, FPS 471
Last 100 training episodes: mean/median reward -60.81/-75.44, min/max -100.0/3.1
Policy entropy: 2.291, Critic Loss: 0.096, Actor Loss 0.148

Updates 8310, training timesteps 664880, FPS 471
Last 100 training episodes: mean/median reward -59.71/-66.34, min/max -100.0/2.9
Policy entropy: 2.291, Critic Loss: 0.083, Actor Loss -0.088

Updates 8320, training timesteps 665680, FPS 471
Last 100 training episodes: mean/median reward -50.87/-61.45, min/max -100.0/4.2
Policy entropy: 2.293, Critic Loss: 0.164, Actor Loss -0.382

Updates 8330, training timesteps 666480, FPS 471
Last 100 training episodes: mean/median reward -61.46/-73.51, min/max -100.0/5.7
Policy entropy: 2.292, Critic Loss: 0.102, Actor Loss 0.236

Updates 8340, training timesteps 667280, FPS 471
Last 100 training episodes: mean/median reward -54.28/-66.34, min/max -100.0/5.1
Policy entropy: 2.290, Critic Loss: 0.117, Actor Loss -0.057

Updates 8350, training timesteps 668080, FPS 471
Last 100 training episodes: mean/median reward -51.00/-63.02, min/max -100.0/7.0
Policy entropy: 2.291, Critic Loss: 0.135, Actor Loss -0.429

Updates 8360, training timesteps 668880, FPS 471
Last 100 training episodes: mean/median reward -53.97/-66.34, min/max -100.0/5.1
Policy entropy: 2.289, Critic Loss: 0.083, Actor Loss 0.267

Updates 8370, training timesteps 669680, FPS 471
Last 100 training episodes: mean/median reward -53.68/-66.34, min/max -100.0/4.9
Policy entropy: 2.292, Critic Loss: 0.140, Actor Loss -0.300

Updates 8380, training timesteps 670480, FPS 471
Last 100 training episodes: mean/median reward -56.58/-66.34, min/max -100.0/5.7
Policy entropy: 2.292, Critic Loss: 0.142, Actor Loss -0.062

Updates 8390, training timesteps 671280, FPS 471
Last 100 training episodes: mean/median reward -51.14/-63.02, min/max -100.0/0.0
Policy entropy: 2.293, Critic Loss: 0.090, Actor Loss -0.165

Updates 8400, training timesteps 672080, FPS 471
Last 100 training episodes: mean/median reward -51.31/-59.87, min/max -100.0/6.0
Policy entropy: 2.294, Critic Loss: 0.344, Actor Loss -0.738

Updates 8410, training timesteps 672880, FPS 471
Last 100 training episodes: mean/median reward -66.30/-79.19, min/max -100.0/5.7
Policy entropy: 2.291, Critic Loss: 0.074, Actor Loss 0.246

Updates 8420, training timesteps 673680, FPS 471
Last 100 training episodes: mean/median reward -55.89/-69.83, min/max -100.0/7.9
Policy entropy: 2.292, Critic Loss: 0.108, Actor Loss 0.078

Updates 8430, training timesteps 674480, FPS 471
Last 100 training episodes: mean/median reward -49.72/-56.88, min/max -100.0/6.0
Policy entropy: 2.293, Critic Loss: 0.294, Actor Loss -0.714

Updates 8440, training timesteps 675280, FPS 471
Last 100 training episodes: mean/median reward -50.79/-62.83, min/max -100.0/5.7
Policy entropy: 2.292, Critic Loss: 0.250, Actor Loss -0.531

Updates 8450, training timesteps 676080, FPS 471
Last 100 training episodes: mean/median reward -61.10/-77.38, min/max -100.0/12.4
Policy entropy: 2.290, Critic Loss: 0.074, Actor Loss 0.130

Updates 8460, training timesteps 676880, FPS 471
Last 100 training episodes: mean/median reward -57.82/-68.09, min/max -100.0/4.6
Policy entropy: 2.291, Critic Loss: 0.142, Actor Loss -0.273

Updates 8470, training timesteps 677680, FPS 471
Last 100 training episodes: mean/median reward -55.42/-69.83, min/max -100.0/7.8
Policy entropy: 2.290, Critic Loss: 0.178, Actor Loss -0.324

Updates 8480, training timesteps 678480, FPS 471
Last 100 training episodes: mean/median reward -58.31/-66.34, min/max -100.0/4.4
Policy entropy: 2.292, Critic Loss: 0.115, Actor Loss 0.030

Updates 8490, training timesteps 679280, FPS 471
Last 100 training episodes: mean/median reward -54.27/-64.68, min/max -100.0/9.1
Policy entropy: 2.292, Critic Loss: 0.152, Actor Loss -0.234

Updates 8500, training timesteps 680080, FPS 471
Last 100 training episodes: mean/median reward -61.07/-77.38, min/max -100.0/14.1
Policy entropy: 2.292, Critic Loss: 0.196, Actor Loss 0.001

Updates 8510, training timesteps 680880, FPS 471
Last 100 training episodes: mean/median reward -52.94/-64.68, min/max -100.0/7.7
Policy entropy: 2.289, Critic Loss: 0.086, Actor Loss -0.075

Updates 8520, training timesteps 681680, FPS 471
Last 100 training episodes: mean/median reward -56.56/-69.83, min/max -100.0/11.5
Policy entropy: 2.290, Critic Loss: 0.074, Actor Loss -0.092

Updates 8530, training timesteps 682480, FPS 471
Last 100 training episodes: mean/median reward -59.93/-71.67, min/max -100.0/8.3
Policy entropy: 2.290, Critic Loss: 0.200, Actor Loss -0.347

Updates 8540, training timesteps 683280, FPS 471
Last 100 training episodes: mean/median reward -56.20/-66.34, min/max -100.0/4.0
Policy entropy: 2.290, Critic Loss: 0.107, Actor Loss 0.031

Updates 8550, training timesteps 684080, FPS 471
Last 100 training episodes: mean/median reward -56.02/-68.09, min/max -100.0/7.1
Policy entropy: 2.291, Critic Loss: 0.131, Actor Loss -0.149

Updates 8560, training timesteps 684880, FPS 471
Last 100 training episodes: mean/median reward -64.91/-77.38, min/max -100.0/5.7
Policy entropy: 2.290, Critic Loss: 0.094, Actor Loss 0.269

Updates 8570, training timesteps 685680, FPS 471
Last 100 training episodes: mean/median reward -54.74/-66.34, min/max -100.0/8.3
Policy entropy: 2.290, Critic Loss: 0.148, Actor Loss 0.068

Updates 8580, training timesteps 686480, FPS 471
Last 100 training episodes: mean/median reward -54.95/-64.68, min/max -100.0/6.4
Policy entropy: 2.290, Critic Loss: 0.132, Actor Loss -0.078

Updates 8590, training timesteps 687280, FPS 471
Last 100 training episodes: mean/median reward -56.95/-68.09, min/max -100.0/5.4
Policy entropy: 2.285, Critic Loss: 0.139, Actor Loss -0.289

Updates 8600, training timesteps 688080, FPS 471
Last 100 training episodes: mean/median reward -55.90/-69.83, min/max -100.0/9.1
Policy entropy: 2.289, Critic Loss: 0.139, Actor Loss -0.278

Updates 8610, training timesteps 688880, FPS 471
Last 100 training episodes: mean/median reward -58.62/-69.83, min/max -100.0/4.2
Policy entropy: 2.289, Critic Loss: 0.123, Actor Loss 0.306

Updates 8620, training timesteps 689680, FPS 471
Last 100 training episodes: mean/median reward -56.25/-66.34, min/max -100.0/4.2
Policy entropy: 2.290, Critic Loss: 0.066, Actor Loss 0.066

Updates 8630, training timesteps 690480, FPS 471
Last 100 training episodes: mean/median reward -57.86/-63.11, min/max -100.0/4.2
Policy entropy: 2.291, Critic Loss: 0.203, Actor Loss -0.402

Updates 8640, training timesteps 691280, FPS 471
Last 100 training episodes: mean/median reward -53.01/-56.88, min/max -100.0/4.2
Policy entropy: 2.292, Critic Loss: 0.199, Actor Loss -0.279

Updates 8650, training timesteps 692080, FPS 471
Last 100 training episodes: mean/median reward -55.11/-71.58, min/max -100.0/3.7
Policy entropy: 2.293, Critic Loss: 0.177, Actor Loss 0.034

Updates 8660, training timesteps 692880, FPS 471
Last 100 training episodes: mean/median reward -51.54/-66.34, min/max -100.0/7.8
Policy entropy: 2.294, Critic Loss: 0.106, Actor Loss -0.106

Updates 8670, training timesteps 693680, FPS 471
Last 100 training episodes: mean/median reward -53.75/-63.02, min/max -100.0/3.4
Policy entropy: 2.292, Critic Loss: 0.118, Actor Loss -0.064

Updates 8680, training timesteps 694480, FPS 471
Last 100 training episodes: mean/median reward -60.38/-73.51, min/max -100.0/5.7
Policy entropy: 2.291, Critic Loss: 0.125, Actor Loss 0.464

Updates 8690, training timesteps 695280, FPS 471
Last 100 training episodes: mean/median reward -55.38/-66.34, min/max -100.0/6.6
Policy entropy: 2.291, Critic Loss: 0.112, Actor Loss -0.107

Updates 8700, training timesteps 696080, FPS 471
Last 100 training episodes: mean/median reward -53.13/-63.11, min/max -100.0/5.1
Policy entropy: 2.291, Critic Loss: 0.186, Actor Loss 0.130

Updates 8710, training timesteps 696880, FPS 471
Last 100 training episodes: mean/median reward -51.96/-58.38, min/max -100.0/14.2
Policy entropy: 2.291, Critic Loss: 0.113, Actor Loss -0.198

Updates 8720, training timesteps 697680, FPS 471
Last 100 training episodes: mean/median reward -54.89/-64.68, min/max -100.0/0.0
Policy entropy: 2.292, Critic Loss: 0.103, Actor Loss -0.298

Updates 8730, training timesteps 698480, FPS 471
Last 100 training episodes: mean/median reward -63.75/-75.44, min/max -100.0/3.2
Policy entropy: 2.292, Critic Loss: 0.100, Actor Loss 0.001

Updates 8740, training timesteps 699280, FPS 471
Last 100 training episodes: mean/median reward -54.45/-66.34, min/max -100.0/6.3
Policy entropy: 2.289, Critic Loss: 0.118, Actor Loss -0.236

Updates 8750, training timesteps 700080, FPS 471
Last 100 training episodes: mean/median reward -55.82/-63.02, min/max -100.0/5.7
Policy entropy: 2.289, Critic Loss: 0.323, Actor Loss -0.829

Updates 8760, training timesteps 700880, FPS 471
Last 100 training episodes: mean/median reward -53.68/-65.41, min/max -100.0/12.0
Policy entropy: 2.291, Critic Loss: 0.114, Actor Loss 0.024

Updates 8770, training timesteps 701680, FPS 471
Last 100 training episodes: mean/median reward -54.42/-66.34, min/max -100.0/3.8
Policy entropy: 2.291, Critic Loss: 0.066, Actor Loss 0.147

Updates 8780, training timesteps 702480, FPS 471
Last 100 training episodes: mean/median reward -56.37/-69.83, min/max -100.0/3.2
Policy entropy: 2.291, Critic Loss: 0.128, Actor Loss -0.399

Updates 8790, training timesteps 703280, FPS 471
Last 100 training episodes: mean/median reward -55.50/-66.34, min/max -100.0/4.9
Policy entropy: 2.293, Critic Loss: 0.126, Actor Loss -0.285

Updates 8800, training timesteps 704080, FPS 471
Last 100 training episodes: mean/median reward -57.24/-66.34, min/max -100.0/12.0
Policy entropy: 2.290, Critic Loss: 0.053, Actor Loss 0.202

Updates 8810, training timesteps 704880, FPS 471
Last 100 training episodes: mean/median reward -56.79/-59.87, min/max -100.0/4.4
Policy entropy: 2.292, Critic Loss: 0.183, Actor Loss -0.291

Updates 8820, training timesteps 705680, FPS 471
Last 100 training episodes: mean/median reward -60.03/-69.83, min/max -100.0/0.0
Policy entropy: 2.292, Critic Loss: 0.247, Actor Loss -0.262

Updates 8830, training timesteps 706480, FPS 471
Last 100 training episodes: mean/median reward -53.48/-64.68, min/max -100.0/5.1
Policy entropy: 2.289, Critic Loss: 0.245, Actor Loss -0.289

Updates 8840, training timesteps 707280, FPS 471
Last 100 training episodes: mean/median reward -56.04/-66.34, min/max -100.0/6.3
Policy entropy: 2.291, Critic Loss: 0.205, Actor Loss -0.113

Updates 8850, training timesteps 708080, FPS 471
Last 100 training episodes: mean/median reward -52.82/-61.45, min/max -100.0/5.4
Policy entropy: 2.291, Critic Loss: 0.122, Actor Loss -0.135

Updates 8860, training timesteps 708880, FPS 471
Last 100 training episodes: mean/median reward -54.95/-63.02, min/max -100.0/4.2
Policy entropy: 2.290, Critic Loss: 0.187, Actor Loss 0.010

Updates 8870, training timesteps 709680, FPS 471
Last 100 training episodes: mean/median reward -58.78/-66.34, min/max -100.0/4.4
Policy entropy: 2.291, Critic Loss: 0.114, Actor Loss 0.132

Updates 8880, training timesteps 710480, FPS 471
Last 100 training episodes: mean/median reward -58.99/-69.83, min/max -100.0/12.8
Policy entropy: 2.290, Critic Loss: 0.117, Actor Loss 0.118

Updates 8890, training timesteps 711280, FPS 471
Last 100 training episodes: mean/median reward -54.27/-63.02, min/max -100.0/12.8
Policy entropy: 2.291, Critic Loss: 0.137, Actor Loss 0.081

Updates 8900, training timesteps 712080, FPS 471
Last 100 training episodes: mean/median reward -59.10/-69.83, min/max -100.0/6.1
Policy entropy: 2.291, Critic Loss: 0.117, Actor Loss 0.386

Updates 8910, training timesteps 712880, FPS 471
Last 100 training episodes: mean/median reward -59.37/-69.83, min/max -100.0/6.1
Policy entropy: 2.290, Critic Loss: 0.227, Actor Loss -0.019

Updates 8920, training timesteps 713680, FPS 471
Last 100 training episodes: mean/median reward -55.90/-63.02, min/max -100.0/3.8
Policy entropy: 2.289, Critic Loss: 0.102, Actor Loss -0.047

Updates 8930, training timesteps 714480, FPS 471
Last 100 training episodes: mean/median reward -50.95/-59.87, min/max -100.0/7.4
Policy entropy: 2.284, Critic Loss: 0.148, Actor Loss -0.196

Updates 8940, training timesteps 715280, FPS 471
Last 100 training episodes: mean/median reward -57.23/-66.34, min/max -100.0/10.9
Policy entropy: 2.287, Critic Loss: 0.077, Actor Loss 0.109

Updates 8950, training timesteps 716080, FPS 471
Last 100 training episodes: mean/median reward -55.20/-59.87, min/max -100.0/12.3
Policy entropy: 2.289, Critic Loss: 0.107, Actor Loss -0.333

Updates 8960, training timesteps 716880, FPS 471
Last 100 training episodes: mean/median reward -55.23/-64.68, min/max -100.0/2.0
Policy entropy: 2.290, Critic Loss: 0.164, Actor Loss -0.354

Updates 8970, training timesteps 717680, FPS 471
Last 100 training episodes: mean/median reward -57.96/-69.93, min/max -100.0/8.2
Policy entropy: 2.288, Critic Loss: 0.111, Actor Loss 0.086

Updates 8980, training timesteps 718480, FPS 471
Last 100 training episodes: mean/median reward -52.49/-63.02, min/max -100.0/6.6
Policy entropy: 2.288, Critic Loss: 0.134, Actor Loss -0.191

Updates 8990, training timesteps 719280, FPS 471
Last 100 training episodes: mean/median reward -50.25/-59.87, min/max -100.0/6.6
Policy entropy: 2.290, Critic Loss: 0.163, Actor Loss -0.402

Updates 9000, training timesteps 720080, FPS 471
Last 100 training episodes: mean/median reward -52.87/-66.34, min/max -100.0/7.7
Policy entropy: 2.290, Critic Loss: 0.210, Actor Loss -0.413

Updates 9010, training timesteps 720880, FPS 471
Last 100 training episodes: mean/median reward -58.09/-69.83, min/max -100.0/8.2
Policy entropy: 2.291, Critic Loss: 0.048, Actor Loss 0.253

Updates 9020, training timesteps 721680, FPS 471
Last 100 training episodes: mean/median reward -51.02/-64.68, min/max -100.0/4.2
Policy entropy: 2.291, Critic Loss: 0.203, Actor Loss -0.171

Updates 9030, training timesteps 722480, FPS 471
Last 100 training episodes: mean/median reward -52.47/-69.83, min/max -100.0/1.7
Policy entropy: 2.292, Critic Loss: 0.086, Actor Loss 0.084

Updates 9040, training timesteps 723280, FPS 471
Last 100 training episodes: mean/median reward -50.78/-63.02, min/max -100.0/7.0
Policy entropy: 2.290, Critic Loss: 0.167, Actor Loss 0.054

Updates 9050, training timesteps 724080, FPS 471
Last 100 training episodes: mean/median reward -50.80/-61.45, min/max -100.0/11.1
Policy entropy: 2.290, Critic Loss: 0.175, Actor Loss -0.508

Updates 9060, training timesteps 724880, FPS 471
Last 100 training episodes: mean/median reward -51.21/-64.68, min/max -100.0/11.0
Policy entropy: 2.290, Critic Loss: 0.155, Actor Loss -0.140

Updates 9070, training timesteps 725680, FPS 471
Last 100 training episodes: mean/median reward -53.20/-64.68, min/max -100.0/11.0
Policy entropy: 2.292, Critic Loss: 0.082, Actor Loss 0.146

Updates 9080, training timesteps 726480, FPS 471
Last 100 training episodes: mean/median reward -52.24/-56.88, min/max -100.0/13.3
Policy entropy: 2.292, Critic Loss: 0.146, Actor Loss -0.197

Updates 9090, training timesteps 727280, FPS 471
Last 100 training episodes: mean/median reward -53.11/-63.02, min/max -100.0/4.0
Policy entropy: 2.292, Critic Loss: 0.118, Actor Loss -0.371

Updates 9100, training timesteps 728080, FPS 471
Last 100 training episodes: mean/median reward -47.26/-55.46, min/max -100.0/10.4
Policy entropy: 2.290, Critic Loss: 0.155, Actor Loss -0.002

Updates 9110, training timesteps 728880, FPS 471
Last 100 training episodes: mean/median reward -56.78/-73.51, min/max -100.0/5.7
Policy entropy: 2.294, Critic Loss: 0.126, Actor Loss 0.551

Updates 9120, training timesteps 729680, FPS 471
Last 100 training episodes: mean/median reward -52.58/-59.89, min/max -100.0/5.7
Policy entropy: 2.292, Critic Loss: 0.148, Actor Loss -0.415

Updates 9130, training timesteps 730480, FPS 471
Last 100 training episodes: mean/median reward -62.42/-69.83, min/max -100.0/0.0
Policy entropy: 2.293, Critic Loss: 0.134, Actor Loss -0.244

Updates 9140, training timesteps 731280, FPS 471
Last 100 training episodes: mean/median reward -56.03/-64.68, min/max -100.0/3.1
Policy entropy: 2.291, Critic Loss: 0.105, Actor Loss -0.125

Updates 9150, training timesteps 732080, FPS 471
Last 100 training episodes: mean/median reward -47.88/-50.05, min/max -100.0/4.9
Policy entropy: 2.289, Critic Loss: 0.207, Actor Loss -0.198

Updates 9160, training timesteps 732880, FPS 471
Last 100 training episodes: mean/median reward -54.17/-63.02, min/max -100.0/5.4
Policy entropy: 2.290, Critic Loss: 0.191, Actor Loss -0.110

Updates 9170, training timesteps 733680, FPS 472
Last 100 training episodes: mean/median reward -59.72/-66.34, min/max -100.0/7.4
Policy entropy: 2.289, Critic Loss: 0.077, Actor Loss 0.014

Updates 9180, training timesteps 734480, FPS 472
Last 100 training episodes: mean/median reward -52.58/-64.68, min/max -100.0/10.1
Policy entropy: 2.290, Critic Loss: 0.099, Actor Loss -0.118

Updates 9190, training timesteps 735280, FPS 472
Last 100 training episodes: mean/median reward -50.66/-64.69, min/max -100.0/6.0
Policy entropy: 2.290, Critic Loss: 0.170, Actor Loss -0.191

Updates 9200, training timesteps 736080, FPS 472
Last 100 training episodes: mean/median reward -51.49/-64.68, min/max -100.0/7.9
Policy entropy: 2.290, Critic Loss: 0.106, Actor Loss -0.192

Updates 9210, training timesteps 736880, FPS 472
Last 100 training episodes: mean/median reward -55.94/-69.83, min/max -100.0/1.1
Policy entropy: 2.291, Critic Loss: 0.081, Actor Loss 0.017

Updates 9220, training timesteps 737680, FPS 472
Last 100 training episodes: mean/median reward -50.50/-55.46, min/max -100.0/5.1
Policy entropy: 2.290, Critic Loss: 0.115, Actor Loss -0.025

Updates 9230, training timesteps 738480, FPS 472
Last 100 training episodes: mean/median reward -52.51/-66.34, min/max -100.0/3.4
Policy entropy: 2.290, Critic Loss: 0.094, Actor Loss -0.078

Updates 9240, training timesteps 739280, FPS 472
Last 100 training episodes: mean/median reward -58.07/-69.83, min/max -100.0/5.4
Policy entropy: 2.290, Critic Loss: 0.097, Actor Loss 0.029

Updates 9250, training timesteps 740080, FPS 472
Last 100 training episodes: mean/median reward -54.31/-66.34, min/max -100.0/5.1
Policy entropy: 2.288, Critic Loss: 0.117, Actor Loss -0.106

Updates 9260, training timesteps 740880, FPS 472
Last 100 training episodes: mean/median reward -55.43/-68.09, min/max -100.0/3.1
Policy entropy: 2.290, Critic Loss: 0.142, Actor Loss -0.191

Updates 9270, training timesteps 741680, FPS 472
Last 100 training episodes: mean/median reward -58.87/-66.34, min/max -100.0/8.6
Policy entropy: 2.290, Critic Loss: 0.137, Actor Loss 0.095

Updates 9280, training timesteps 742480, FPS 472
Last 100 training episodes: mean/median reward -54.57/-63.02, min/max -100.0/7.0
Policy entropy: 2.290, Critic Loss: 0.133, Actor Loss -0.074

Updates 9290, training timesteps 743280, FPS 472
Last 100 training episodes: mean/median reward -52.74/-59.87, min/max -100.0/5.1
Policy entropy: 2.291, Critic Loss: 0.119, Actor Loss -0.060

Updates 9300, training timesteps 744080, FPS 472
Last 100 training episodes: mean/median reward -55.79/-68.09, min/max -100.0/6.0
Policy entropy: 2.288, Critic Loss: 0.230, Actor Loss -0.266

Updates 9310, training timesteps 744880, FPS 472
Last 100 training episodes: mean/median reward -53.18/-63.02, min/max -100.0/6.0
Policy entropy: 2.290, Critic Loss: 0.109, Actor Loss 0.009

Updates 9320, training timesteps 745680, FPS 472
Last 100 training episodes: mean/median reward -47.17/-56.88, min/max -100.0/12.8
Policy entropy: 2.290, Critic Loss: 0.092, Actor Loss 0.040

Updates 9330, training timesteps 746480, FPS 472
Last 100 training episodes: mean/median reward -51.79/-63.02, min/max -100.0/7.4
Policy entropy: 2.290, Critic Loss: 0.100, Actor Loss 0.015

Updates 9340, training timesteps 747280, FPS 472
Last 100 training episodes: mean/median reward -55.51/-66.34, min/max -100.0/5.1
Policy entropy: 2.290, Critic Loss: 0.082, Actor Loss 0.128

Updates 9350, training timesteps 748080, FPS 472
Last 100 training episodes: mean/median reward -59.14/-69.83, min/max -100.0/6.0
Policy entropy: 2.290, Critic Loss: 0.117, Actor Loss -0.255

Updates 9360, training timesteps 748880, FPS 472
Last 100 training episodes: mean/median reward -56.23/-68.09, min/max -100.0/2.8
Policy entropy: 2.289, Critic Loss: 0.079, Actor Loss 0.178

Updates 9370, training timesteps 749680, FPS 472
Last 100 training episodes: mean/median reward -54.73/-68.09, min/max -100.0/7.0
Policy entropy: 2.290, Critic Loss: 0.090, Actor Loss 0.369

Updates 9380, training timesteps 750480, FPS 472
Last 100 training episodes: mean/median reward -53.73/-63.02, min/max -100.0/6.3
Policy entropy: 2.290, Critic Loss: 0.062, Actor Loss 0.275

Updates 9390, training timesteps 751280, FPS 472
Last 100 training episodes: mean/median reward -57.16/-66.34, min/max -100.0/3.4
Policy entropy: 2.289, Critic Loss: 0.071, Actor Loss 0.149

Updates 9400, training timesteps 752080, FPS 472
Last 100 training episodes: mean/median reward -50.77/-58.38, min/max -100.0/7.0
Policy entropy: 2.290, Critic Loss: 0.063, Actor Loss 0.210

Updates 9410, training timesteps 752880, FPS 472
Last 100 training episodes: mean/median reward -55.76/-69.83, min/max -100.0/4.0
Policy entropy: 2.290, Critic Loss: 0.117, Actor Loss 0.381

Updates 9420, training timesteps 753680, FPS 472
Last 100 training episodes: mean/median reward -58.78/-66.34, min/max -100.0/4.0
Policy entropy: 2.292, Critic Loss: 0.069, Actor Loss 0.066

Updates 9430, training timesteps 754480, FPS 472
Last 100 training episodes: mean/median reward -56.10/-69.83, min/max -100.0/7.4
Policy entropy: 2.291, Critic Loss: 0.102, Actor Loss 0.068

Updates 9440, training timesteps 755280, FPS 472
Last 100 training episodes: mean/median reward -56.67/-73.51, min/max -100.0/7.4
Policy entropy: 2.291, Critic Loss: 0.145, Actor Loss -0.247

Updates 9450, training timesteps 756080, FPS 472
Last 100 training episodes: mean/median reward -59.07/-75.44, min/max -100.0/11.2
Policy entropy: 2.288, Critic Loss: 0.143, Actor Loss -0.152

Updates 9460, training timesteps 756880, FPS 472
Last 100 training episodes: mean/median reward -60.89/-66.34, min/max -100.0/5.1
Policy entropy: 2.290, Critic Loss: 0.126, Actor Loss -0.044

Updates 9470, training timesteps 757680, FPS 472
Last 100 training episodes: mean/median reward -59.36/-69.83, min/max -100.0/2.8
Policy entropy: 2.291, Critic Loss: 0.135, Actor Loss -0.093

Updates 9480, training timesteps 758480, FPS 472
Last 100 training episodes: mean/median reward -48.95/-56.88, min/max -100.0/7.0
Policy entropy: 2.292, Critic Loss: 0.132, Actor Loss -0.285

Updates 9490, training timesteps 759280, FPS 472
Last 100 training episodes: mean/median reward -54.86/-63.02, min/max -100.0/3.8
Policy entropy: 2.289, Critic Loss: 0.210, Actor Loss -0.178

Updates 9500, training timesteps 760080, FPS 472
Last 100 training episodes: mean/median reward -59.88/-71.67, min/max -100.0/7.7
Policy entropy: 2.289, Critic Loss: 0.390, Actor Loss -1.040

Updates 9510, training timesteps 760880, FPS 472
Last 100 training episodes: mean/median reward -54.18/-66.34, min/max -100.0/4.2
Policy entropy: 2.287, Critic Loss: 0.173, Actor Loss -0.073

Updates 9520, training timesteps 761680, FPS 472
Last 100 training episodes: mean/median reward -52.06/-56.88, min/max -100.0/6.6
Policy entropy: 2.290, Critic Loss: 0.088, Actor Loss -0.074

Updates 9530, training timesteps 762480, FPS 472
Last 100 training episodes: mean/median reward -54.33/-64.68, min/max -100.0/3.4
Policy entropy: 2.292, Critic Loss: 0.176, Actor Loss 0.538

Updates 9540, training timesteps 763280, FPS 472
Last 100 training episodes: mean/median reward -55.90/-63.02, min/max -100.0/3.4
Policy entropy: 2.293, Critic Loss: 0.086, Actor Loss -0.221

Updates 9550, training timesteps 764080, FPS 472
Last 100 training episodes: mean/median reward -56.04/-69.83, min/max -100.0/6.0
Policy entropy: 2.294, Critic Loss: 0.163, Actor Loss -0.558

Updates 9560, training timesteps 764880, FPS 472
Last 100 training episodes: mean/median reward -53.59/-66.34, min/max -100.0/7.0
Policy entropy: 2.291, Critic Loss: 0.140, Actor Loss -0.170

Updates 9570, training timesteps 765680, FPS 472
Last 100 training episodes: mean/median reward -52.84/-63.02, min/max -100.0/7.0
Policy entropy: 2.292, Critic Loss: 0.131, Actor Loss -0.178

Updates 9580, training timesteps 766480, FPS 472
Last 100 training episodes: mean/median reward -54.96/-63.02, min/max -100.0/2.4
Policy entropy: 2.288, Critic Loss: 0.088, Actor Loss 0.261

Updates 9590, training timesteps 767280, FPS 472
Last 100 training episodes: mean/median reward -52.41/-63.02, min/max -100.0/6.3
Policy entropy: 2.292, Critic Loss: 0.067, Actor Loss 0.015

Updates 9600, training timesteps 768080, FPS 472
Last 100 training episodes: mean/median reward -53.59/-61.45, min/max -100.0/0.0
Policy entropy: 2.293, Critic Loss: 0.096, Actor Loss -0.195

Updates 9610, training timesteps 768880, FPS 472
Last 100 training episodes: mean/median reward -51.66/-59.87, min/max -100.0/6.6
Policy entropy: 2.295, Critic Loss: 0.163, Actor Loss -0.495

Updates 9620, training timesteps 769680, FPS 472
Last 100 training episodes: mean/median reward -63.32/-77.38, min/max -100.0/6.0
Policy entropy: 2.291, Critic Loss: 0.180, Actor Loss -0.370

Updates 9630, training timesteps 770480, FPS 472
Last 100 training episodes: mean/median reward -56.38/-66.34, min/max -100.0/4.6
Policy entropy: 2.290, Critic Loss: 0.176, Actor Loss -0.080

Updates 9640, training timesteps 771280, FPS 472
Last 100 training episodes: mean/median reward -53.86/-66.25, min/max -100.0/5.4
Policy entropy: 2.291, Critic Loss: 0.434, Actor Loss -1.311

Updates 9650, training timesteps 772080, FPS 472
Last 100 training episodes: mean/median reward -55.44/-63.02, min/max -100.0/5.1
Policy entropy: 2.291, Critic Loss: 0.122, Actor Loss -0.173

Updates 9660, training timesteps 772880, FPS 472
Last 100 training episodes: mean/median reward -52.39/-66.34, min/max -100.0/6.1
Policy entropy: 2.289, Critic Loss: 0.104, Actor Loss -0.006

Updates 9670, training timesteps 773680, FPS 472
Last 100 training episodes: mean/median reward -55.13/-63.02, min/max -100.0/5.1
Policy entropy: 2.292, Critic Loss: 0.236, Actor Loss -0.337

Updates 9680, training timesteps 774480, FPS 472
Last 100 training episodes: mean/median reward -53.65/-66.34, min/max -100.0/7.3
Policy entropy: 2.294, Critic Loss: 0.125, Actor Loss 0.238

Updates 9690, training timesteps 775280, FPS 472
Last 100 training episodes: mean/median reward -52.30/-66.26, min/max -100.0/4.4
Policy entropy: 2.292, Critic Loss: 0.064, Actor Loss 0.219

Updates 9700, training timesteps 776080, FPS 472
Last 100 training episodes: mean/median reward -54.68/-64.68, min/max -100.0/3.4
Policy entropy: 2.291, Critic Loss: 0.201, Actor Loss 0.156

Updates 9710, training timesteps 776880, FPS 472
Last 100 training episodes: mean/median reward -54.52/-59.87, min/max -100.0/14.4
Policy entropy: 2.293, Critic Loss: 0.136, Actor Loss 0.206

Updates 9720, training timesteps 777680, FPS 472
Last 100 training episodes: mean/median reward -56.98/-66.34, min/max -100.0/0.0
Policy entropy: 2.293, Critic Loss: 0.059, Actor Loss 0.090

Updates 9730, training timesteps 778480, FPS 472
Last 100 training episodes: mean/median reward -55.10/-71.67, min/max -100.0/5.7
Policy entropy: 2.293, Critic Loss: 0.102, Actor Loss -0.344

Updates 9740, training timesteps 779280, FPS 472
Last 100 training episodes: mean/median reward -52.14/-66.34, min/max -100.0/7.7
Policy entropy: 2.291, Critic Loss: 0.210, Actor Loss -0.184

Updates 9750, training timesteps 780080, FPS 472
Last 100 training episodes: mean/median reward -52.05/-63.02, min/max -100.0/6.4
Policy entropy: 2.291, Critic Loss: 0.120, Actor Loss 0.070

Updates 9760, training timesteps 780880, FPS 472
Last 100 training episodes: mean/median reward -57.55/-68.79, min/max -100.0/6.0
Policy entropy: 2.291, Critic Loss: 0.105, Actor Loss 0.312

Updates 9770, training timesteps 781680, FPS 472
Last 100 training episodes: mean/median reward -48.93/-55.46, min/max -100.0/4.6
Policy entropy: 2.292, Critic Loss: 0.105, Actor Loss -0.163

Updates 9780, training timesteps 782480, FPS 472
Last 100 training episodes: mean/median reward -54.52/-63.02, min/max -100.0/4.6
Policy entropy: 2.290, Critic Loss: 0.101, Actor Loss 0.004

Updates 9790, training timesteps 783280, FPS 472
Last 100 training episodes: mean/median reward -51.96/-64.40, min/max -100.0/6.3
Policy entropy: 2.291, Critic Loss: 0.073, Actor Loss 0.133

Updates 9800, training timesteps 784080, FPS 472
Last 100 training episodes: mean/median reward -61.81/-75.44, min/max -100.0/5.7
Policy entropy: 2.291, Critic Loss: 0.167, Actor Loss -0.024

Updates 9810, training timesteps 784880, FPS 472
Last 100 training episodes: mean/median reward -63.67/-75.44, min/max -100.0/3.4
Policy entropy: 2.290, Critic Loss: 0.197, Actor Loss 0.185

Updates 9820, training timesteps 785680, FPS 472
Last 100 training episodes: mean/median reward -53.11/-64.68, min/max -100.0/4.6
Policy entropy: 2.289, Critic Loss: 0.193, Actor Loss -0.502

Updates 9830, training timesteps 786480, FPS 472
Last 100 training episodes: mean/median reward -44.81/-51.79, min/max -100.0/4.6
Policy entropy: 2.288, Critic Loss: 0.143, Actor Loss 0.054

Updates 9840, training timesteps 787280, FPS 472
Last 100 training episodes: mean/median reward -51.73/-63.02, min/max -100.0/12.0
Policy entropy: 2.288, Critic Loss: 0.182, Actor Loss 0.066

Updates 9850, training timesteps 788080, FPS 472
Last 100 training episodes: mean/median reward -54.87/-61.45, min/max -100.0/7.9
Policy entropy: 2.288, Critic Loss: 0.159, Actor Loss -0.225

Updates 9860, training timesteps 788880, FPS 472
Last 100 training episodes: mean/median reward -58.38/-69.61, min/max -100.0/5.4
Policy entropy: 2.288, Critic Loss: 0.146, Actor Loss 0.263

Updates 9870, training timesteps 789680, FPS 472
Last 100 training episodes: mean/median reward -53.99/-68.09, min/max -100.0/9.9
Policy entropy: 2.287, Critic Loss: 0.128, Actor Loss 0.254

Updates 9880, training timesteps 790480, FPS 472
Last 100 training episodes: mean/median reward -56.36/-71.67, min/max -100.0/4.2
Policy entropy: 2.288, Critic Loss: 0.159, Actor Loss 0.153

Updates 9890, training timesteps 791280, FPS 472
Last 100 training episodes: mean/median reward -56.38/-69.83, min/max -100.0/3.4
Policy entropy: 2.287, Critic Loss: 0.180, Actor Loss -0.407

Updates 9900, training timesteps 792080, FPS 472
Last 100 training episodes: mean/median reward -56.05/-66.34, min/max -100.0/4.4
Policy entropy: 2.288, Critic Loss: 0.164, Actor Loss -0.055

Updates 9910, training timesteps 792880, FPS 472
Last 100 training episodes: mean/median reward -51.20/-54.04, min/max -100.0/5.1
Policy entropy: 2.291, Critic Loss: 0.156, Actor Loss -0.230

Updates 9920, training timesteps 793680, FPS 472
Last 100 training episodes: mean/median reward -58.05/-68.09, min/max -100.0/6.3
Policy entropy: 2.290, Critic Loss: 0.222, Actor Loss -0.234

Updates 9930, training timesteps 794480, FPS 472
Last 100 training episodes: mean/median reward -54.74/-66.34, min/max -100.0/4.2
Policy entropy: 2.287, Critic Loss: 0.144, Actor Loss -0.144

Updates 9940, training timesteps 795280, FPS 472
Last 100 training episodes: mean/median reward -54.29/-59.87, min/max -100.0/4.9
Policy entropy: 2.291, Critic Loss: 0.104, Actor Loss 0.201

Updates 9950, training timesteps 796080, FPS 472
Last 100 training episodes: mean/median reward -60.79/-71.58, min/max -100.0/0.0
Policy entropy: 2.290, Critic Loss: 0.149, Actor Loss -0.364

Updates 9960, training timesteps 796880, FPS 472
Last 100 training episodes: mean/median reward -59.39/-69.83, min/max -100.0/5.1
Policy entropy: 2.287, Critic Loss: 0.121, Actor Loss 0.628

Updates 9970, training timesteps 797680, FPS 472
Last 100 training episodes: mean/median reward -52.37/-63.02, min/max -100.0/4.6
Policy entropy: 2.290, Critic Loss: 0.112, Actor Loss 0.590

Updates 9980, training timesteps 798480, FPS 472
Last 100 training episodes: mean/median reward -53.51/-66.34, min/max -100.0/2.0
Policy entropy: 2.289, Critic Loss: 0.135, Actor Loss -0.226

Updates 9990, training timesteps 799280, FPS 472
Last 100 training episodes: mean/median reward -57.26/-69.83, min/max -100.0/4.9
Policy entropy: 2.292, Critic Loss: 0.095, Actor Loss -0.084

Updates 10000, training timesteps 800080, FPS 472
Last 100 training episodes: mean/median reward -51.53/-58.38, min/max -100.0/6.9
Policy entropy: 2.291, Critic Loss: 0.067, Actor Loss 0.188

Updates 10010, training timesteps 800880, FPS 472
Last 100 training episodes: mean/median reward -57.34/-69.83, min/max -100.0/6.5
Policy entropy: 2.292, Critic Loss: 0.131, Actor Loss -0.113

Updates 10020, training timesteps 801680, FPS 472
Last 100 training episodes: mean/median reward -58.25/-73.51, min/max -100.0/6.3
Policy entropy: 2.290, Critic Loss: 0.103, Actor Loss -0.208

Updates 10030, training timesteps 802480, FPS 472
Last 100 training episodes: mean/median reward -56.03/-69.63, min/max -100.0/5.4
Policy entropy: 2.292, Critic Loss: 0.163, Actor Loss -0.147

Updates 10040, training timesteps 803280, FPS 472
Last 100 training episodes: mean/median reward -55.64/-59.87, min/max -100.0/6.6
Policy entropy: 2.291, Critic Loss: 0.192, Actor Loss -0.268

Updates 10050, training timesteps 804080, FPS 472
Last 100 training episodes: mean/median reward -49.21/-52.69, min/max -100.0/7.0
Policy entropy: 2.292, Critic Loss: 0.074, Actor Loss 0.199

Updates 10060, training timesteps 804880, FPS 472
Last 100 training episodes: mean/median reward -54.63/-66.34, min/max -100.0/3.6
Policy entropy: 2.290, Critic Loss: 0.076, Actor Loss 0.162

Updates 10070, training timesteps 805680, FPS 472
Last 100 training episodes: mean/median reward -55.76/-66.34, min/max -100.0/5.1
Policy entropy: 2.289, Critic Loss: 0.110, Actor Loss 0.251

Updates 10080, training timesteps 806480, FPS 472
Last 100 training episodes: mean/median reward -55.11/-64.68, min/max -100.0/5.7
Policy entropy: 2.288, Critic Loss: 0.111, Actor Loss -0.111

Updates 10090, training timesteps 807280, FPS 472
Last 100 training episodes: mean/median reward -52.14/-61.45, min/max -100.0/6.0
Policy entropy: 2.289, Critic Loss: 0.314, Actor Loss -0.703

Updates 10100, training timesteps 808080, FPS 472
Last 100 training episodes: mean/median reward -48.24/-59.87, min/max -100.0/7.9
Policy entropy: 2.291, Critic Loss: 0.093, Actor Loss 0.372

Updates 10110, training timesteps 808880, FPS 472
Last 100 training episodes: mean/median reward -54.46/-65.29, min/max -100.0/3.8
Policy entropy: 2.290, Critic Loss: 0.132, Actor Loss -0.162

Updates 10120, training timesteps 809680, FPS 472
Last 100 training episodes: mean/median reward -55.91/-65.29, min/max -100.0/6.3
Policy entropy: 2.290, Critic Loss: 0.081, Actor Loss 0.386

Updates 10130, training timesteps 810480, FPS 472
Last 100 training episodes: mean/median reward -54.02/-63.02, min/max -100.0/4.9
Policy entropy: 2.290, Critic Loss: 0.166, Actor Loss 0.533

Updates 10140, training timesteps 811280, FPS 472
Last 100 training episodes: mean/median reward -60.31/-73.51, min/max -100.0/2.4
Policy entropy: 2.289, Critic Loss: 0.107, Actor Loss -0.039

Updates 10150, training timesteps 812080, FPS 472
Last 100 training episodes: mean/median reward -55.95/-68.09, min/max -100.0/8.2
Policy entropy: 2.290, Critic Loss: 0.059, Actor Loss 0.070

Updates 10160, training timesteps 812880, FPS 472
Last 100 training episodes: mean/median reward -56.60/-69.83, min/max -100.0/10.0
Policy entropy: 2.291, Critic Loss: 0.204, Actor Loss 0.136

Updates 10170, training timesteps 813680, FPS 472
Last 100 training episodes: mean/median reward -57.83/-73.51, min/max -100.0/3.1
Policy entropy: 2.290, Critic Loss: 0.139, Actor Loss -0.371

Updates 10180, training timesteps 814480, FPS 472
Last 100 training episodes: mean/median reward -51.91/-56.64, min/max -100.0/3.1
Policy entropy: 2.292, Critic Loss: 0.118, Actor Loss 0.202

Updates 10190, training timesteps 815280, FPS 472
Last 100 training episodes: mean/median reward -51.54/-63.02, min/max -100.0/6.3
Policy entropy: 2.291, Critic Loss: 0.158, Actor Loss -0.163

Updates 10200, training timesteps 816080, FPS 472
Last 100 training episodes: mean/median reward -61.06/-69.83, min/max -100.0/4.4
Policy entropy: 2.291, Critic Loss: 0.097, Actor Loss 0.110

Updates 10210, training timesteps 816880, FPS 472
Last 100 training episodes: mean/median reward -54.32/-64.48, min/max -100.0/7.4
Policy entropy: 2.290, Critic Loss: 0.101, Actor Loss 0.067

Updates 10220, training timesteps 817680, FPS 472
Last 100 training episodes: mean/median reward -55.31/-63.02, min/max -100.0/5.4
Policy entropy: 2.290, Critic Loss: 0.107, Actor Loss -0.127

Updates 10230, training timesteps 818480, FPS 472
Last 100 training episodes: mean/median reward -51.74/-59.87, min/max -100.0/5.4
Policy entropy: 2.291, Critic Loss: 0.351, Actor Loss -0.733

Updates 10240, training timesteps 819280, FPS 472
Last 100 training episodes: mean/median reward -53.44/-64.68, min/max -100.0/0.0
Policy entropy: 2.290, Critic Loss: 0.054, Actor Loss 0.171

Updates 10250, training timesteps 820080, FPS 472
Last 100 training episodes: mean/median reward -50.33/-56.88, min/max -100.0/4.0
Policy entropy: 2.292, Critic Loss: 0.069, Actor Loss 0.075

Updates 10260, training timesteps 820880, FPS 472
Last 100 training episodes: mean/median reward -56.74/-71.67, min/max -100.0/7.4
Policy entropy: 2.290, Critic Loss: 0.157, Actor Loss -0.255

Updates 10270, training timesteps 821680, FPS 472
Last 100 training episodes: mean/median reward -51.07/-61.27, min/max -100.0/0.8
Policy entropy: 2.292, Critic Loss: 0.156, Actor Loss -0.133

Updates 10280, training timesteps 822480, FPS 472
Last 100 training episodes: mean/median reward -55.23/-63.02, min/max -100.0/4.9
Policy entropy: 2.287, Critic Loss: 0.125, Actor Loss -0.154

Updates 10290, training timesteps 823280, FPS 472
Last 100 training episodes: mean/median reward -52.64/-61.45, min/max -100.0/3.6
Policy entropy: 2.290, Critic Loss: 0.097, Actor Loss -0.099

Updates 10300, training timesteps 824080, FPS 472
Last 100 training episodes: mean/median reward -55.33/-64.68, min/max -100.0/1.1
Policy entropy: 2.292, Critic Loss: 0.206, Actor Loss -0.579

Updates 10310, training timesteps 824880, FPS 472
Last 100 training episodes: mean/median reward -62.27/-75.44, min/max -100.0/5.1
Policy entropy: 2.290, Critic Loss: 0.089, Actor Loss -0.007

Updates 10320, training timesteps 825680, FPS 472
Last 100 training episodes: mean/median reward -54.58/-63.02, min/max -100.0/13.2
Policy entropy: 2.287, Critic Loss: 0.183, Actor Loss -0.331

Updates 10330, training timesteps 826480, FPS 472
Last 100 training episodes: mean/median reward -54.10/-63.02, min/max -100.0/12.6
Policy entropy: 2.290, Critic Loss: 0.164, Actor Loss -0.017

Updates 10340, training timesteps 827280, FPS 472
Last 100 training episodes: mean/median reward -56.77/-66.34, min/max -100.0/3.8
Policy entropy: 2.289, Critic Loss: 0.101, Actor Loss -0.094

Updates 10350, training timesteps 828080, FPS 472
Last 100 training episodes: mean/median reward -55.47/-73.51, min/max -100.0/6.0
Policy entropy: 2.291, Critic Loss: 0.193, Actor Loss -0.294

Updates 10360, training timesteps 828880, FPS 472
Last 100 training episodes: mean/median reward -51.28/-58.38, min/max -100.0/6.0
Policy entropy: 2.289, Critic Loss: 0.211, Actor Loss 0.110

Updates 10370, training timesteps 829680, FPS 472
Last 100 training episodes: mean/median reward -53.97/-65.41, min/max -100.0/9.4
Policy entropy: 2.289, Critic Loss: 0.178, Actor Loss -0.244

Updates 10380, training timesteps 830480, FPS 472
Last 100 training episodes: mean/median reward -57.06/-69.83, min/max -100.0/4.9
Policy entropy: 2.289, Critic Loss: 0.124, Actor Loss -0.039

Updates 10390, training timesteps 831280, FPS 472
Last 100 training episodes: mean/median reward -59.11/-66.34, min/max -100.0/4.4
Policy entropy: 2.289, Critic Loss: 0.108, Actor Loss 0.130

Updates 10400, training timesteps 832080, FPS 472
Last 100 training episodes: mean/median reward -54.70/-63.02, min/max -100.6/7.0
Policy entropy: 2.290, Critic Loss: 0.065, Actor Loss 0.232

Updates 10410, training timesteps 832880, FPS 472
Last 100 training episodes: mean/median reward -50.08/-66.34, min/max -100.0/7.0
Policy entropy: 2.287, Critic Loss: 0.154, Actor Loss -0.411

Updates 10420, training timesteps 833680, FPS 472
Last 100 training episodes: mean/median reward -51.08/-66.34, min/max -100.0/4.4
Policy entropy: 2.288, Critic Loss: 0.064, Actor Loss 0.223

Updates 10430, training timesteps 834480, FPS 472
Last 100 training episodes: mean/median reward -50.17/-61.45, min/max -100.0/9.6
Policy entropy: 2.288, Critic Loss: 0.116, Actor Loss -0.095

Updates 10440, training timesteps 835280, FPS 472
Last 100 training episodes: mean/median reward -54.13/-69.83, min/max -100.0/3.8
Policy entropy: 2.291, Critic Loss: 0.063, Actor Loss 0.150

Updates 10450, training timesteps 836080, FPS 472
Last 100 training episodes: mean/median reward -53.74/-64.68, min/max -100.0/3.4
Policy entropy: 2.289, Critic Loss: 0.133, Actor Loss -0.223

Updates 10460, training timesteps 836880, FPS 472
Last 100 training episodes: mean/median reward -53.85/-59.87, min/max -100.0/3.4
Policy entropy: 2.291, Critic Loss: 0.135, Actor Loss -0.428

Updates 10470, training timesteps 837680, FPS 472
Last 100 training episodes: mean/median reward -56.47/-63.02, min/max -100.0/5.4
Policy entropy: 2.289, Critic Loss: 0.152, Actor Loss 0.252

Updates 10480, training timesteps 838480, FPS 472
Last 100 training episodes: mean/median reward -51.37/-66.34, min/max -100.0/3.6
Policy entropy: 2.290, Critic Loss: 0.249, Actor Loss -0.230

Updates 10490, training timesteps 839280, FPS 472
Last 100 training episodes: mean/median reward -52.64/-66.16, min/max -100.0/11.1
Policy entropy: 2.288, Critic Loss: 0.246, Actor Loss -0.446

Updates 10500, training timesteps 840080, FPS 472
Last 100 training episodes: mean/median reward -56.46/-73.51, min/max -100.0/7.8
Policy entropy: 2.286, Critic Loss: 0.132, Actor Loss -0.205

Updates 10510, training timesteps 840880, FPS 472
Last 100 training episodes: mean/median reward -55.23/-66.34, min/max -100.0/4.1
Policy entropy: 2.288, Critic Loss: 0.116, Actor Loss 0.454

Updates 10520, training timesteps 841680, FPS 472
Last 100 training episodes: mean/median reward -58.36/-63.02, min/max -100.0/3.8
Policy entropy: 2.293, Critic Loss: 0.111, Actor Loss -0.172

Updates 10530, training timesteps 842480, FPS 472
Last 100 training episodes: mean/median reward -57.12/-67.39, min/max -100.0/5.1
Policy entropy: 2.290, Critic Loss: 0.117, Actor Loss 0.246

Updates 10540, training timesteps 843280, FPS 472
Last 100 training episodes: mean/median reward -61.10/-69.83, min/max -100.0/5.1
Policy entropy: 2.291, Critic Loss: 0.113, Actor Loss -0.171

Updates 10550, training timesteps 844080, FPS 472
Last 100 training episodes: mean/median reward -57.18/-64.93, min/max -100.0/13.1
Policy entropy: 2.288, Critic Loss: 0.239, Actor Loss -0.171

Updates 10560, training timesteps 844880, FPS 472
Last 100 training episodes: mean/median reward -52.34/-58.38, min/max -100.0/5.1
Policy entropy: 2.289, Critic Loss: 0.131, Actor Loss 0.140

Updates 10570, training timesteps 845680, FPS 472
Last 100 training episodes: mean/median reward -52.94/-61.45, min/max -100.0/6.0
Policy entropy: 2.289, Critic Loss: 0.196, Actor Loss -0.263

Updates 10580, training timesteps 846480, FPS 472
Last 100 training episodes: mean/median reward -59.59/-68.09, min/max -100.0/6.3
Policy entropy: 2.288, Critic Loss: 0.138, Actor Loss -0.233

Updates 10590, training timesteps 847280, FPS 472
Last 100 training episodes: mean/median reward -58.08/-69.83, min/max -100.0/5.1
Policy entropy: 2.291, Critic Loss: 0.130, Actor Loss -0.320

Updates 10600, training timesteps 848080, FPS 472
Last 100 training episodes: mean/median reward -54.97/-61.45, min/max -100.0/5.3
Policy entropy: 2.289, Critic Loss: 0.111, Actor Loss 0.065

Updates 10610, training timesteps 848880, FPS 472
Last 100 training episodes: mean/median reward -55.97/-69.83, min/max -100.0/5.7
Policy entropy: 2.291, Critic Loss: 0.167, Actor Loss -0.188

Updates 10620, training timesteps 849680, FPS 472
Last 100 training episodes: mean/median reward -50.82/-55.46, min/max -100.0/4.4
Policy entropy: 2.290, Critic Loss: 0.115, Actor Loss -0.076

Updates 10630, training timesteps 850480, FPS 472
Last 100 training episodes: mean/median reward -49.74/-59.87, min/max -100.0/8.1
Policy entropy: 2.291, Critic Loss: 0.124, Actor Loss 0.105

Updates 10640, training timesteps 851280, FPS 472
Last 100 training episodes: mean/median reward -52.27/-63.02, min/max -100.0/5.1
Policy entropy: 2.291, Critic Loss: 0.120, Actor Loss 0.113

Updates 10650, training timesteps 852080, FPS 472
Last 100 training episodes: mean/median reward -57.31/-69.83, min/max -100.0/15.3
Policy entropy: 2.290, Critic Loss: 0.089, Actor Loss -0.025

Updates 10660, training timesteps 852880, FPS 472
Last 100 training episodes: mean/median reward -54.76/-69.83, min/max -105.1/2.0
Policy entropy: 2.291, Critic Loss: 0.130, Actor Loss -0.295

Updates 10670, training timesteps 853680, FPS 472
Last 100 training episodes: mean/median reward -59.05/-71.67, min/max -100.0/7.4
Policy entropy: 2.290, Critic Loss: 0.087, Actor Loss -0.118

Updates 10680, training timesteps 854480, FPS 472
Last 100 training episodes: mean/median reward -59.84/-69.83, min/max -100.0/6.3
Policy entropy: 2.293, Critic Loss: 0.078, Actor Loss 0.192

Updates 10690, training timesteps 855280, FPS 472
Last 100 training episodes: mean/median reward -64.03/-73.51, min/max -100.0/6.3
Policy entropy: 2.292, Critic Loss: 0.080, Actor Loss 0.153

Updates 10700, training timesteps 856080, FPS 472
Last 100 training episodes: mean/median reward -52.82/-64.68, min/max -100.0/6.3
Policy entropy: 2.290, Critic Loss: 0.125, Actor Loss 0.101

Updates 10710, training timesteps 856880, FPS 472
Last 100 training episodes: mean/median reward -50.22/-56.88, min/max -100.0/0.0
Policy entropy: 2.291, Critic Loss: 0.058, Actor Loss 0.156

Updates 10720, training timesteps 857680, FPS 472
Last 100 training episodes: mean/median reward -47.34/-55.46, min/max -100.0/4.9
Policy entropy: 2.291, Critic Loss: 0.097, Actor Loss 0.162

Updates 10730, training timesteps 858480, FPS 472
Last 100 training episodes: mean/median reward -47.93/-58.38, min/max -100.0/5.1
Policy entropy: 2.292, Critic Loss: 0.112, Actor Loss -0.165

Updates 10740, training timesteps 859280, FPS 472
Last 100 training episodes: mean/median reward -57.36/-66.34, min/max -100.0/4.6
Policy entropy: 2.289, Critic Loss: 0.211, Actor Loss -0.140

Updates 10750, training timesteps 860080, FPS 472
Last 100 training episodes: mean/median reward -56.99/-75.44, min/max -100.0/12.1
Policy entropy: 2.290, Critic Loss: 0.097, Actor Loss 0.250

Updates 10760, training timesteps 860880, FPS 472
Last 100 training episodes: mean/median reward -60.92/-75.44, min/max -100.0/12.1
Policy entropy: 2.291, Critic Loss: 0.074, Actor Loss 0.094

Updates 10770, training timesteps 861680, FPS 472
Last 100 training episodes: mean/median reward -58.44/-69.83, min/max -100.0/9.8
Policy entropy: 2.289, Critic Loss: 0.135, Actor Loss -0.261

Updates 10780, training timesteps 862480, FPS 472
Last 100 training episodes: mean/median reward -55.22/-63.02, min/max -100.0/9.4
Policy entropy: 2.287, Critic Loss: 0.366, Actor Loss -0.599

Updates 10790, training timesteps 863280, FPS 472
Last 100 training episodes: mean/median reward -55.67/-69.83, min/max -100.0/5.3
Policy entropy: 2.288, Critic Loss: 0.175, Actor Loss -0.014

Updates 10800, training timesteps 864080, FPS 472
Last 100 training episodes: mean/median reward -53.50/-61.44, min/max -100.0/5.3
Policy entropy: 2.291, Critic Loss: 0.220, Actor Loss -0.693

Updates 10810, training timesteps 864880, FPS 472
Last 100 training episodes: mean/median reward -55.49/-63.02, min/max -100.0/5.4
Policy entropy: 2.290, Critic Loss: 0.390, Actor Loss -0.380

Updates 10820, training timesteps 865680, FPS 472
Last 100 training episodes: mean/median reward -58.68/-66.34, min/max -100.0/2.8
Policy entropy: 2.291, Critic Loss: 0.069, Actor Loss -0.013

Updates 10830, training timesteps 866480, FPS 472
Last 100 training episodes: mean/median reward -53.21/-64.03, min/max -100.0/5.1
Policy entropy: 2.292, Critic Loss: 0.205, Actor Loss -0.468

Updates 10840, training timesteps 867280, FPS 472
Last 100 training episodes: mean/median reward -60.17/-69.83, min/max -100.0/3.6
Policy entropy: 2.291, Critic Loss: 0.137, Actor Loss -0.282

Updates 10850, training timesteps 868080, FPS 472
Last 100 training episodes: mean/median reward -59.51/-71.67, min/max -100.0/6.3
Policy entropy: 2.289, Critic Loss: 0.092, Actor Loss -0.075

Updates 10860, training timesteps 868880, FPS 472
Last 100 training episodes: mean/median reward -49.99/-52.69, min/max -100.0/5.4
Policy entropy: 2.290, Critic Loss: 0.100, Actor Loss -0.165

Updates 10870, training timesteps 869680, FPS 472
Last 100 training episodes: mean/median reward -59.27/-71.67, min/max -100.0/1.3
Policy entropy: 2.291, Critic Loss: 0.189, Actor Loss -0.221

Updates 10880, training timesteps 870480, FPS 472
Last 100 training episodes: mean/median reward -51.61/-63.02, min/max -100.0/4.0
Policy entropy: 2.292, Critic Loss: 0.109, Actor Loss -0.149

Updates 10890, training timesteps 871280, FPS 472
Last 100 training episodes: mean/median reward -52.46/-60.98, min/max -100.0/12.8
Policy entropy: 2.291, Critic Loss: 0.197, Actor Loss 0.198

Updates 10900, training timesteps 872080, FPS 472
Last 100 training episodes: mean/median reward -52.93/-64.68, min/max -100.0/3.6
Policy entropy: 2.292, Critic Loss: 0.205, Actor Loss -0.432

Updates 10910, training timesteps 872880, FPS 472
Last 100 training episodes: mean/median reward -56.93/-67.85, min/max -100.0/9.1
Policy entropy: 2.292, Critic Loss: 0.086, Actor Loss -0.261

Updates 10920, training timesteps 873680, FPS 472
Last 100 training episodes: mean/median reward -59.46/-73.51, min/max -100.0/12.5
Policy entropy: 2.292, Critic Loss: 0.197, Actor Loss -0.351

Updates 10930, training timesteps 874480, FPS 472
Last 100 training episodes: mean/median reward -54.56/-66.34, min/max -100.0/5.7
Policy entropy: 2.291, Critic Loss: 0.074, Actor Loss 0.068

Updates 10940, training timesteps 875280, FPS 472
Last 100 training episodes: mean/median reward -55.49/-69.83, min/max -100.0/6.3
Policy entropy: 2.289, Critic Loss: 0.089, Actor Loss 0.291

Updates 10950, training timesteps 876080, FPS 472
Last 100 training episodes: mean/median reward -56.54/-64.68, min/max -100.0/7.7
Policy entropy: 2.290, Critic Loss: 0.066, Actor Loss 0.022

Updates 10960, training timesteps 876880, FPS 472
Last 100 training episodes: mean/median reward -62.47/-73.51, min/max -100.0/4.6
Policy entropy: 2.290, Critic Loss: 0.141, Actor Loss 0.360

Updates 10970, training timesteps 877680, FPS 472
Last 100 training episodes: mean/median reward -51.67/-59.87, min/max -100.0/11.3
Policy entropy: 2.290, Critic Loss: 0.247, Actor Loss -0.569

Updates 10980, training timesteps 878480, FPS 472
Last 100 training episodes: mean/median reward -58.30/-69.83, min/max -100.0/4.4
Policy entropy: 2.290, Critic Loss: 0.197, Actor Loss 0.122

Updates 10990, training timesteps 879280, FPS 472
Last 100 training episodes: mean/median reward -49.48/-61.45, min/max -100.0/7.7
Policy entropy: 2.290, Critic Loss: 0.147, Actor Loss -0.287

Updates 11000, training timesteps 880080, FPS 472
Last 100 training episodes: mean/median reward -57.68/-73.51, min/max -100.0/0.0
Policy entropy: 2.291, Critic Loss: 0.052, Actor Loss 0.245

Updates 11010, training timesteps 880880, FPS 472
Last 100 training episodes: mean/median reward -51.39/-63.02, min/max -100.0/6.6
Policy entropy: 2.291, Critic Loss: 0.113, Actor Loss 0.343

Updates 11020, training timesteps 881680, FPS 472
Last 100 training episodes: mean/median reward -49.24/-55.46, min/max -100.0/10.2
Policy entropy: 2.289, Critic Loss: 0.114, Actor Loss -0.179

Updates 11030, training timesteps 882480, FPS 472
Last 100 training episodes: mean/median reward -54.22/-66.34, min/max -100.0/9.6
Policy entropy: 2.291, Critic Loss: 0.212, Actor Loss -0.382

Updates 11040, training timesteps 883280, FPS 472
Last 100 training episodes: mean/median reward -55.17/-69.83, min/max -100.0/8.8
Policy entropy: 2.290, Critic Loss: 0.109, Actor Loss -0.151

Updates 11050, training timesteps 884080, FPS 472
Last 100 training episodes: mean/median reward -56.51/-69.83, min/max -100.0/7.3
Policy entropy: 2.287, Critic Loss: 0.120, Actor Loss 0.304

Updates 11060, training timesteps 884880, FPS 472
Last 100 training episodes: mean/median reward -56.50/-64.68, min/max -100.0/6.6
Policy entropy: 2.288, Critic Loss: 0.113, Actor Loss 0.447

Updates 11070, training timesteps 885680, FPS 472
Last 100 training episodes: mean/median reward -56.54/-63.02, min/max -100.0/6.6
Policy entropy: 2.291, Critic Loss: 0.188, Actor Loss -0.006

Updates 11080, training timesteps 886480, FPS 472
Last 100 training episodes: mean/median reward -54.61/-65.18, min/max -100.0/3.1
Policy entropy: 2.288, Critic Loss: 0.083, Actor Loss 0.346

Updates 11090, training timesteps 887280, FPS 472
Last 100 training episodes: mean/median reward -55.83/-62.75, min/max -100.0/7.4
Policy entropy: 2.291, Critic Loss: 0.288, Actor Loss -0.346

Updates 11100, training timesteps 888080, FPS 472
Last 100 training episodes: mean/median reward -55.25/-58.38, min/max -100.0/6.3
Policy entropy: 2.292, Critic Loss: 0.197, Actor Loss -0.293

Updates 11110, training timesteps 888880, FPS 472
Last 100 training episodes: mean/median reward -57.35/-69.09, min/max -100.0/0.0
Policy entropy: 2.291, Critic Loss: 0.108, Actor Loss 0.072

Updates 11120, training timesteps 889680, FPS 472
Last 100 training episodes: mean/median reward -56.69/-69.83, min/max -100.0/4.9
Policy entropy: 2.291, Critic Loss: 0.253, Actor Loss 0.303

Updates 11130, training timesteps 890480, FPS 472
Last 100 training episodes: mean/median reward -51.84/-59.87, min/max -100.0/4.9
Policy entropy: 2.292, Critic Loss: 0.109, Actor Loss 0.103

Updates 11140, training timesteps 891280, FPS 472
Last 100 training episodes: mean/median reward -53.31/-63.98, min/max -100.0/5.1
Policy entropy: 2.291, Critic Loss: 0.312, Actor Loss -0.727

Updates 11150, training timesteps 892080, FPS 472
Last 100 training episodes: mean/median reward -59.42/-68.09, min/max -100.0/10.1
Policy entropy: 2.292, Critic Loss: 0.072, Actor Loss 0.046

Updates 11160, training timesteps 892880, FPS 472
Last 100 training episodes: mean/median reward -53.09/-61.45, min/max -100.0/7.7
Policy entropy: 2.290, Critic Loss: 0.178, Actor Loss -0.451

Updates 11170, training timesteps 893680, FPS 472
Last 100 training episodes: mean/median reward -50.56/-56.88, min/max -100.0/6.3
Policy entropy: 2.289, Critic Loss: 0.116, Actor Loss -0.386

Updates 11180, training timesteps 894480, FPS 472
Last 100 training episodes: mean/median reward -56.85/-68.09, min/max -100.0/4.9
Policy entropy: 2.289, Critic Loss: 0.112, Actor Loss -0.223

Updates 11190, training timesteps 895280, FPS 472
Last 100 training episodes: mean/median reward -56.88/-66.34, min/max -100.0/10.6
Policy entropy: 2.289, Critic Loss: 0.152, Actor Loss -0.144

Updates 11200, training timesteps 896080, FPS 472
Last 100 training episodes: mean/median reward -53.60/-66.34, min/max -100.0/7.0
Policy entropy: 2.290, Critic Loss: 0.176, Actor Loss -0.209

Updates 11210, training timesteps 896880, FPS 472
Last 100 training episodes: mean/median reward -58.72/-69.83, min/max -100.0/11.7
Policy entropy: 2.286, Critic Loss: 0.160, Actor Loss -0.027

Updates 11220, training timesteps 897680, FPS 473
Last 100 training episodes: mean/median reward -55.74/-66.34, min/max -100.0/1.8
Policy entropy: 2.289, Critic Loss: 0.097, Actor Loss -0.064

Updates 11230, training timesteps 898480, FPS 473
Last 100 training episodes: mean/median reward -51.13/-61.45, min/max -100.0/5.7
Policy entropy: 2.289, Critic Loss: 0.077, Actor Loss 0.349

Updates 11240, training timesteps 899280, FPS 473
Last 100 training episodes: mean/median reward -53.82/-61.45, min/max -100.0/0.0
Policy entropy: 2.291, Critic Loss: 0.087, Actor Loss 0.127

Updates 11250, training timesteps 900080, FPS 472
Last 100 training episodes: mean/median reward -62.96/-77.38, min/max -100.0/6.0
Policy entropy: 2.289, Critic Loss: 0.129, Actor Loss -0.158

Updates 11260, training timesteps 900880, FPS 473
Last 100 training episodes: mean/median reward -56.44/-66.34, min/max -100.0/5.1
Policy entropy: 2.289, Critic Loss: 0.084, Actor Loss 0.204

Updates 11270, training timesteps 901680, FPS 473
Last 100 training episodes: mean/median reward -49.49/-56.88, min/max -100.0/5.7
Policy entropy: 2.289, Critic Loss: 0.060, Actor Loss 0.133

Updates 11280, training timesteps 902480, FPS 472
Last 100 training episodes: mean/median reward -57.46/-68.09, min/max -100.0/4.0
Policy entropy: 2.291, Critic Loss: 0.096, Actor Loss 0.103

Updates 11290, training timesteps 903280, FPS 472
Last 100 training episodes: mean/median reward -60.74/-73.51, min/max -100.0/0.0
Policy entropy: 2.291, Critic Loss: 0.209, Actor Loss -0.106

Updates 11300, training timesteps 904080, FPS 472
Last 100 training episodes: mean/median reward -54.80/-66.34, min/max -100.0/8.1
Policy entropy: 2.292, Critic Loss: 0.129, Actor Loss 0.039

Updates 11310, training timesteps 904880, FPS 472
Last 100 training episodes: mean/median reward -55.45/-69.83, min/max -100.0/7.1
Policy entropy: 2.292, Critic Loss: 0.113, Actor Loss 0.188

Updates 11320, training timesteps 905680, FPS 472
Last 100 training episodes: mean/median reward -53.95/-61.45, min/max -100.0/0.0
Policy entropy: 2.291, Critic Loss: 0.141, Actor Loss -0.235

Updates 11330, training timesteps 906480, FPS 472
Last 100 training episodes: mean/median reward -51.68/-63.02, min/max -100.0/13.1
Policy entropy: 2.291, Critic Loss: 0.098, Actor Loss 0.232

Updates 11340, training timesteps 907280, FPS 472
Last 100 training episodes: mean/median reward -53.41/-64.68, min/max -100.0/1.8
Policy entropy: 2.293, Critic Loss: 0.205, Actor Loss 0.073

Updates 11350, training timesteps 908080, FPS 472
Last 100 training episodes: mean/median reward -53.96/-69.83, min/max -100.0/3.6
Policy entropy: 2.293, Critic Loss: 0.123, Actor Loss 0.073

Updates 11360, training timesteps 908880, FPS 472
Last 100 training episodes: mean/median reward -48.77/-56.95, min/max -100.0/3.1
Policy entropy: 2.291, Critic Loss: 0.203, Actor Loss -0.068

Updates 11370, training timesteps 909680, FPS 472
Last 100 training episodes: mean/median reward -56.90/-73.51, min/max -100.0/0.0
Policy entropy: 2.290, Critic Loss: 0.095, Actor Loss -0.119

Updates 11380, training timesteps 910480, FPS 472
Last 100 training episodes: mean/median reward -58.03/-71.58, min/max -100.0/0.0
Policy entropy: 2.292, Critic Loss: 0.112, Actor Loss -0.071

Updates 11390, training timesteps 911280, FPS 472
Last 100 training episodes: mean/median reward -49.64/-58.38, min/max -100.0/3.8
Policy entropy: 2.291, Critic Loss: 0.083, Actor Loss 0.088

Updates 11400, training timesteps 912080, FPS 472
Last 100 training episodes: mean/median reward -55.21/-69.83, min/max -100.0/3.4
Policy entropy: 2.289, Critic Loss: 0.314, Actor Loss -0.686

Updates 11410, training timesteps 912880, FPS 472
Last 100 training episodes: mean/median reward -59.33/-73.19, min/max -100.0/6.3
Policy entropy: 2.288, Critic Loss: 0.097, Actor Loss -0.001

Updates 11420, training timesteps 913680, FPS 472
Last 100 training episodes: mean/median reward -59.01/-69.83, min/max -100.0/8.1
Policy entropy: 2.288, Critic Loss: 0.183, Actor Loss -0.269

Updates 11430, training timesteps 914480, FPS 472
Last 100 training episodes: mean/median reward -52.14/-61.45, min/max -100.0/6.6
Policy entropy: 2.288, Critic Loss: 0.160, Actor Loss -0.050

Updates 11440, training timesteps 915280, FPS 472
Last 100 training episodes: mean/median reward -49.04/-55.46, min/max -100.0/5.4
Policy entropy: 2.290, Critic Loss: 0.196, Actor Loss -0.596

Updates 11450, training timesteps 916080, FPS 472
Last 100 training episodes: mean/median reward -57.52/-73.51, min/max -100.0/11.1
Policy entropy: 2.287, Critic Loss: 0.095, Actor Loss 0.116

Updates 11460, training timesteps 916880, FPS 472
Last 100 training episodes: mean/median reward -54.16/-64.68, min/max -100.0/6.6
Policy entropy: 2.291, Critic Loss: 0.099, Actor Loss 0.096

Updates 11470, training timesteps 917680, FPS 472
Last 100 training episodes: mean/median reward -51.33/-59.87, min/max -100.0/6.6
Policy entropy: 2.288, Critic Loss: 0.172, Actor Loss -0.325

Updates 11480, training timesteps 918480, FPS 472
Last 100 training episodes: mean/median reward -56.46/-63.02, min/max -100.0/7.8
Policy entropy: 2.288, Critic Loss: 0.104, Actor Loss -0.154

Updates 11490, training timesteps 919280, FPS 472
Last 100 training episodes: mean/median reward -58.70/-69.83, min/max -100.0/6.6
Policy entropy: 2.289, Critic Loss: 0.124, Actor Loss -0.402

Updates 11500, training timesteps 920080, FPS 472
Last 100 training episodes: mean/median reward -55.33/-63.02, min/max -100.0/9.4
Policy entropy: 2.290, Critic Loss: 0.175, Actor Loss -0.513

Updates 11510, training timesteps 920880, FPS 473
Last 100 training episodes: mean/median reward -57.56/-66.34, min/max -100.0/4.0
Policy entropy: 2.292, Critic Loss: 0.148, Actor Loss -0.351

Updates 11520, training timesteps 921680, FPS 473
Last 100 training episodes: mean/median reward -60.22/-73.51, min/max -100.0/0.0
Policy entropy: 2.292, Critic Loss: 0.074, Actor Loss -0.006

Updates 11530, training timesteps 922480, FPS 472
Last 100 training episodes: mean/median reward -56.58/-63.02, min/max -100.0/4.6
Policy entropy: 2.289, Critic Loss: 0.084, Actor Loss 0.208

Updates 11540, training timesteps 923280, FPS 472
Last 100 training episodes: mean/median reward -53.72/-56.88, min/max -100.0/2.4
Policy entropy: 2.290, Critic Loss: 0.161, Actor Loss 0.030

Updates 11550, training timesteps 924080, FPS 472
Last 100 training episodes: mean/median reward -57.15/-66.06, min/max -100.0/4.9
Policy entropy: 2.290, Critic Loss: 0.129, Actor Loss -0.352

Updates 11560, training timesteps 924880, FPS 472
Last 100 training episodes: mean/median reward -61.05/-73.51, min/max -100.0/6.6
Policy entropy: 2.290, Critic Loss: 0.095, Actor Loss -0.148

Updates 11570, training timesteps 925680, FPS 472
Last 100 training episodes: mean/median reward -60.55/-75.35, min/max -100.0/4.0
Policy entropy: 2.292, Critic Loss: 0.111, Actor Loss -0.397

Updates 11580, training timesteps 926480, FPS 472
Last 100 training episodes: mean/median reward -54.34/-61.45, min/max -100.0/12.5
Policy entropy: 2.291, Critic Loss: 0.059, Actor Loss 0.463

Updates 11590, training timesteps 927280, FPS 472
Last 100 training episodes: mean/median reward -56.34/-66.34, min/max -100.0/2.5
Policy entropy: 2.291, Critic Loss: 0.104, Actor Loss 0.175

Updates 11600, training timesteps 928080, FPS 472
Last 100 training episodes: mean/median reward -53.10/-63.86, min/max -100.0/10.6
Policy entropy: 2.293, Critic Loss: 0.198, Actor Loss -0.320

Updates 11610, training timesteps 928880, FPS 472
Last 100 training episodes: mean/median reward -53.00/-66.34, min/max -100.0/7.3
Policy entropy: 2.290, Critic Loss: 0.150, Actor Loss -0.213

Updates 11620, training timesteps 929680, FPS 472
Last 100 training episodes: mean/median reward -60.02/-66.34, min/max -100.0/3.8
Policy entropy: 2.290, Critic Loss: 0.120, Actor Loss 0.307

Updates 11630, training timesteps 930480, FPS 472
Last 100 training episodes: mean/median reward -56.35/-66.34, min/max -100.0/0.0
Policy entropy: 2.292, Critic Loss: 0.104, Actor Loss -0.134

Updates 11640, training timesteps 931280, FPS 472
Last 100 training episodes: mean/median reward -56.36/-66.34, min/max -100.0/3.1
Policy entropy: 2.292, Critic Loss: 0.158, Actor Loss -0.211

Updates 11650, training timesteps 932080, FPS 473
Last 100 training episodes: mean/median reward -55.64/-66.34, min/max -100.0/0.0
Policy entropy: 2.291, Critic Loss: 0.076, Actor Loss -0.209

Updates 11660, training timesteps 932880, FPS 472
Last 100 training episodes: mean/median reward -59.71/-73.51, min/max -100.0/9.8
Policy entropy: 2.291, Critic Loss: 0.062, Actor Loss 0.160

Updates 11670, training timesteps 933680, FPS 472
Last 100 training episodes: mean/median reward -58.60/-73.51, min/max -100.0/5.4
Policy entropy: 2.289, Critic Loss: 0.130, Actor Loss 0.169

Updates 11680, training timesteps 934480, FPS 473
Last 100 training episodes: mean/median reward -48.58/-56.88, min/max -100.0/8.1
Policy entropy: 2.291, Critic Loss: 0.187, Actor Loss -0.203

Updates 11690, training timesteps 935280, FPS 473
Last 100 training episodes: mean/median reward -51.48/-56.88, min/max -100.0/4.5
Policy entropy: 2.290, Critic Loss: 0.152, Actor Loss 0.078

Updates 11700, training timesteps 936080, FPS 473
Last 100 training episodes: mean/median reward -54.82/-63.02, min/max -100.0/6.3
Policy entropy: 2.291, Critic Loss: 0.156, Actor Loss -0.044

Updates 11710, training timesteps 936880, FPS 473
Last 100 training episodes: mean/median reward -54.79/-63.02, min/max -100.0/6.3
Policy entropy: 2.294, Critic Loss: 0.331, Actor Loss -0.624

Updates 11720, training timesteps 937680, FPS 473
Last 100 training episodes: mean/median reward -59.26/-75.44, min/max -100.0/3.2
Policy entropy: 2.291, Critic Loss: 0.125, Actor Loss -0.222

Updates 11730, training timesteps 938480, FPS 473
Last 100 training episodes: mean/median reward -48.13/-58.38, min/max -100.0/8.8
Policy entropy: 2.291, Critic Loss: 0.094, Actor Loss 0.081

Updates 11740, training timesteps 939280, FPS 473
Last 100 training episodes: mean/median reward -51.15/-66.34, min/max -100.0/4.4
Policy entropy: 2.290, Critic Loss: 0.170, Actor Loss -0.200

Updates 11750, training timesteps 940080, FPS 473
Last 100 training episodes: mean/median reward -50.90/-56.88, min/max -100.0/5.4
Policy entropy: 2.291, Critic Loss: 0.124, Actor Loss 0.120

Updates 11760, training timesteps 940880, FPS 473
Last 100 training episodes: mean/median reward -50.29/-66.34, min/max -100.0/9.3
Policy entropy: 2.291, Critic Loss: 0.179, Actor Loss -0.160

Updates 11770, training timesteps 941680, FPS 473
Last 100 training episodes: mean/median reward -54.84/-66.34, min/max -100.0/0.8
Policy entropy: 2.291, Critic Loss: 0.101, Actor Loss -0.129

Updates 11780, training timesteps 942480, FPS 473
Last 100 training episodes: mean/median reward -58.29/-66.34, min/max -100.0/4.4
Policy entropy: 2.290, Critic Loss: 0.087, Actor Loss -0.019

Updates 11790, training timesteps 943280, FPS 473
Last 100 training episodes: mean/median reward -56.64/-63.02, min/max -100.0/5.1
Policy entropy: 2.289, Critic Loss: 0.108, Actor Loss -0.027

Updates 11800, training timesteps 944080, FPS 473
Last 100 training episodes: mean/median reward -54.06/-56.88, min/max -100.0/6.0
Policy entropy: 2.290, Critic Loss: 0.106, Actor Loss 0.037

Updates 11810, training timesteps 944880, FPS 473
Last 100 training episodes: mean/median reward -58.73/-66.34, min/max -100.0/2.3
Policy entropy: 2.288, Critic Loss: 0.089, Actor Loss -0.016

Updates 11820, training timesteps 945680, FPS 473
Last 100 training episodes: mean/median reward -57.66/-68.09, min/max -100.0/6.1
Policy entropy: 2.290, Critic Loss: 0.182, Actor Loss -0.395

Updates 11830, training timesteps 946480, FPS 473
Last 100 training episodes: mean/median reward -57.93/-69.83, min/max -100.0/4.9
Policy entropy: 2.290, Critic Loss: 0.092, Actor Loss -0.001

Updates 11840, training timesteps 947280, FPS 473
Last 100 training episodes: mean/median reward -54.20/-66.34, min/max -100.0/2.9
Policy entropy: 2.288, Critic Loss: 0.155, Actor Loss 0.016

Updates 11850, training timesteps 948080, FPS 473
Last 100 training episodes: mean/median reward -50.26/-61.45, min/max -100.0/4.4
Policy entropy: 2.290, Critic Loss: 0.174, Actor Loss -0.392

Updates 11860, training timesteps 948880, FPS 473
Last 100 training episodes: mean/median reward -57.15/-64.68, min/max -100.0/0.0
Policy entropy: 2.291, Critic Loss: 0.106, Actor Loss 0.012

Updates 11870, training timesteps 949680, FPS 473
Last 100 training episodes: mean/median reward -55.28/-63.02, min/max -100.0/2.3
Policy entropy: 2.291, Critic Loss: 0.119, Actor Loss 0.052

Updates 11880, training timesteps 950480, FPS 473
Last 100 training episodes: mean/median reward -51.78/-63.02, min/max -100.0/6.0
Policy entropy: 2.288, Critic Loss: 0.214, Actor Loss -0.054

Updates 11890, training timesteps 951280, FPS 473
Last 100 training episodes: mean/median reward -52.27/-61.45, min/max -100.0/6.0
Policy entropy: 2.291, Critic Loss: 0.173, Actor Loss -0.254

Updates 11900, training timesteps 952080, FPS 473
Last 100 training episodes: mean/median reward -58.21/-73.51, min/max -100.0/7.7
Policy entropy: 2.290, Critic Loss: 0.150, Actor Loss -0.346

Updates 11910, training timesteps 952880, FPS 473
Last 100 training episodes: mean/median reward -55.01/-66.34, min/max -100.0/14.0
Policy entropy: 2.290, Critic Loss: 0.245, Actor Loss -0.547

Updates 11920, training timesteps 953680, FPS 473
Last 100 training episodes: mean/median reward -57.12/-69.83, min/max -100.0/5.4
Policy entropy: 2.287, Critic Loss: 0.086, Actor Loss -0.200

Updates 11930, training timesteps 954480, FPS 473
Last 100 training episodes: mean/median reward -53.13/-68.09, min/max -100.0/4.6
Policy entropy: 2.290, Critic Loss: 0.342, Actor Loss -0.904

Updates 11940, training timesteps 955280, FPS 473
Last 100 training episodes: mean/median reward -56.75/-66.34, min/max -100.0/3.8
Policy entropy: 2.288, Critic Loss: 0.143, Actor Loss -0.090

Updates 11950, training timesteps 956080, FPS 473
Last 100 training episodes: mean/median reward -55.13/-69.83, min/max -100.0/6.6
Policy entropy: 2.288, Critic Loss: 0.120, Actor Loss -0.298

Updates 11960, training timesteps 956880, FPS 473
Last 100 training episodes: mean/median reward -56.33/-69.83, min/max -100.0/5.1
Policy entropy: 2.290, Critic Loss: 0.137, Actor Loss 0.093

Updates 11970, training timesteps 957680, FPS 473
Last 100 training episodes: mean/median reward -60.68/-73.51, min/max -100.0/4.9
Policy entropy: 2.289, Critic Loss: 0.112, Actor Loss 0.067

Updates 11980, training timesteps 958480, FPS 473
Last 100 training episodes: mean/median reward -54.60/-63.02, min/max -100.0/3.2
Policy entropy: 2.288, Critic Loss: 0.120, Actor Loss 0.096

Updates 11990, training timesteps 959280, FPS 473
Last 100 training episodes: mean/median reward -51.84/-58.38, min/max -100.0/4.6
Policy entropy: 2.290, Critic Loss: 0.180, Actor Loss -0.478

Updates 12000, training timesteps 960080, FPS 473
Last 100 training episodes: mean/median reward -56.79/-68.09, min/max -100.0/7.7
Policy entropy: 2.290, Critic Loss: 0.132, Actor Loss -0.115

Updates 12010, training timesteps 960880, FPS 473
Last 100 training episodes: mean/median reward -52.42/-59.87, min/max -100.0/7.7
Policy entropy: 2.287, Critic Loss: 0.103, Actor Loss -0.081

Updates 12020, training timesteps 961680, FPS 473
Last 100 training episodes: mean/median reward -58.98/-71.67, min/max -100.0/15.1
Policy entropy: 2.290, Critic Loss: 0.093, Actor Loss 0.190

Updates 12030, training timesteps 962480, FPS 473
Last 100 training episodes: mean/median reward -56.50/-63.02, min/max -100.0/0.0
Policy entropy: 2.291, Critic Loss: 0.145, Actor Loss -0.252

Updates 12040, training timesteps 963280, FPS 473
Last 100 training episodes: mean/median reward -60.21/-66.34, min/max -100.0/4.0
Policy entropy: 2.291, Critic Loss: 0.140, Actor Loss -0.040

Updates 12050, training timesteps 964080, FPS 473
Last 100 training episodes: mean/median reward -56.70/-69.83, min/max -100.0/5.7
Policy entropy: 2.290, Critic Loss: 0.116, Actor Loss 0.447

Updates 12060, training timesteps 964880, FPS 473
Last 100 training episodes: mean/median reward -52.18/-64.68, min/max -100.0/8.4
Policy entropy: 2.290, Critic Loss: 0.162, Actor Loss -0.506

Updates 12070, training timesteps 965680, FPS 473
Last 100 training episodes: mean/median reward -60.19/-73.51, min/max -100.0/3.1
Policy entropy: 2.287, Critic Loss: 0.103, Actor Loss 0.235

Updates 12080, training timesteps 966480, FPS 473
Last 100 training episodes: mean/median reward -54.56/-66.34, min/max -100.0/6.5
Policy entropy: 2.290, Critic Loss: 0.171, Actor Loss -0.184

Updates 12090, training timesteps 967280, FPS 473
Last 100 training episodes: mean/median reward -54.59/-69.83, min/max -100.0/6.6
Policy entropy: 2.289, Critic Loss: 0.125, Actor Loss -0.004

Updates 12100, training timesteps 968080, FPS 473
Last 100 training episodes: mean/median reward -50.05/-54.04, min/max -100.0/5.7
Policy entropy: 2.289, Critic Loss: 0.225, Actor Loss -0.379

Updates 12110, training timesteps 968880, FPS 473
Last 100 training episodes: mean/median reward -55.88/-66.34, min/max -100.0/5.1
Policy entropy: 2.289, Critic Loss: 0.134, Actor Loss -0.282

Updates 12120, training timesteps 969680, FPS 473
Last 100 training episodes: mean/median reward -52.36/-59.87, min/max -100.0/7.0
Policy entropy: 2.288, Critic Loss: 0.150, Actor Loss 0.000

Updates 12130, training timesteps 970480, FPS 473
Last 100 training episodes: mean/median reward -55.10/-73.51, min/max -100.0/4.0
Policy entropy: 2.288, Critic Loss: 0.106, Actor Loss 0.165

Updates 12140, training timesteps 971280, FPS 473
Last 100 training episodes: mean/median reward -49.10/-56.88, min/max -100.0/5.4
Policy entropy: 2.289, Critic Loss: 0.186, Actor Loss -0.227

Updates 12150, training timesteps 972080, FPS 473
Last 100 training episodes: mean/median reward -51.47/-61.45, min/max -100.0/11.5
Policy entropy: 2.288, Critic Loss: 0.264, Actor Loss -0.268

Updates 12160, training timesteps 972880, FPS 473
Last 100 training episodes: mean/median reward -56.46/-69.83, min/max -100.0/5.4
Policy entropy: 2.290, Critic Loss: 0.234, Actor Loss -0.395

Updates 12170, training timesteps 973680, FPS 473
Last 100 training episodes: mean/median reward -60.33/-73.51, min/max -100.0/4.9
Policy entropy: 2.289, Critic Loss: 0.084, Actor Loss -0.207

Updates 12180, training timesteps 974480, FPS 473
Last 100 training episodes: mean/median reward -52.07/-55.46, min/max -100.0/2.0
Policy entropy: 2.289, Critic Loss: 0.131, Actor Loss -0.114

Updates 12190, training timesteps 975280, FPS 473
Last 100 training episodes: mean/median reward -53.87/-60.45, min/max -100.0/6.0
Policy entropy: 2.290, Critic Loss: 0.107, Actor Loss -0.045

Updates 12200, training timesteps 976080, FPS 473
Last 100 training episodes: mean/median reward -52.84/-63.02, min/max -100.0/0.0
Policy entropy: 2.290, Critic Loss: 0.138, Actor Loss 0.054

Updates 12210, training timesteps 976880, FPS 473
Last 100 training episodes: mean/median reward -53.00/-63.02, min/max -100.0/6.0
Policy entropy: 2.286, Critic Loss: 0.225, Actor Loss -0.304

Updates 12220, training timesteps 977680, FPS 473
Last 100 training episodes: mean/median reward -56.29/-64.68, min/max -100.0/6.0
Policy entropy: 2.287, Critic Loss: 0.134, Actor Loss 0.164

Updates 12230, training timesteps 978480, FPS 473
Last 100 training episodes: mean/median reward -55.66/-63.02, min/max -100.0/6.6
Policy entropy: 2.288, Critic Loss: 0.145, Actor Loss -0.283

Updates 12240, training timesteps 979280, FPS 473
Last 100 training episodes: mean/median reward -50.23/-59.87, min/max -100.0/4.9
Policy entropy: 2.289, Critic Loss: 0.113, Actor Loss -0.235

Updates 12250, training timesteps 980080, FPS 473
Last 100 training episodes: mean/median reward -62.00/-69.83, min/max -100.0/0.0
Policy entropy: 2.290, Critic Loss: 0.302, Actor Loss -1.013

Updates 12260, training timesteps 980880, FPS 473
Last 100 training episodes: mean/median reward -58.84/-69.83, min/max -100.0/0.0
Policy entropy: 2.292, Critic Loss: 0.260, Actor Loss -0.547

Updates 12270, training timesteps 981680, FPS 473
Last 100 training episodes: mean/median reward -55.88/-68.09, min/max -100.0/3.4
Policy entropy: 2.291, Critic Loss: 0.087, Actor Loss -0.041

Updates 12280, training timesteps 982480, FPS 473
Last 100 training episodes: mean/median reward -47.13/-48.83, min/max -100.0/8.2
Policy entropy: 2.290, Critic Loss: 0.217, Actor Loss -0.350

Updates 12290, training timesteps 983280, FPS 473
Last 100 training episodes: mean/median reward -59.73/-71.67, min/max -100.0/0.8
Policy entropy: 2.291, Critic Loss: 0.141, Actor Loss -0.002

Updates 12300, training timesteps 984080, FPS 473
Last 100 training episodes: mean/median reward -58.72/-69.83, min/max -100.0/2.0
Policy entropy: 2.288, Critic Loss: 0.068, Actor Loss 0.035

Updates 12310, training timesteps 984880, FPS 473
Last 100 training episodes: mean/median reward -57.80/-66.34, min/max -100.0/5.7
Policy entropy: 2.291, Critic Loss: 0.151, Actor Loss -0.344

Updates 12320, training timesteps 985680, FPS 473
Last 100 training episodes: mean/median reward -51.39/-63.02, min/max -100.0/4.4
Policy entropy: 2.291, Critic Loss: 0.244, Actor Loss -0.752

Updates 12330, training timesteps 986480, FPS 473
Last 100 training episodes: mean/median reward -58.19/-69.83, min/max -100.0/2.5
Policy entropy: 2.290, Critic Loss: 0.084, Actor Loss -0.098

Updates 12340, training timesteps 987280, FPS 473
Last 100 training episodes: mean/median reward -55.03/-66.34, min/max -100.0/6.8
Policy entropy: 2.288, Critic Loss: 0.191, Actor Loss -0.097

Updates 12350, training timesteps 988080, FPS 473
Last 100 training episodes: mean/median reward -46.26/-56.88, min/max -100.0/5.1
Policy entropy: 2.285, Critic Loss: 0.166, Actor Loss -0.091

Updates 12360, training timesteps 988880, FPS 473
Last 100 training episodes: mean/median reward -54.14/-58.38, min/max -100.0/6.3
Policy entropy: 2.286, Critic Loss: 0.273, Actor Loss -0.771

Updates 12370, training timesteps 989680, FPS 473
Last 100 training episodes: mean/median reward -51.87/-54.04, min/max -100.0/6.3
Policy entropy: 2.289, Critic Loss: 0.089, Actor Loss -0.058

Updates 12380, training timesteps 990480, FPS 473
Last 100 training episodes: mean/median reward -46.39/-50.05, min/max -100.0/8.2
Policy entropy: 2.292, Critic Loss: 0.111, Actor Loss -0.234

Updates 12390, training timesteps 991280, FPS 473
Last 100 training episodes: mean/median reward -55.40/-69.83, min/max -100.0/2.1
Policy entropy: 2.290, Critic Loss: 0.133, Actor Loss -0.305

Updates 12400, training timesteps 992080, FPS 473
Last 100 training episodes: mean/median reward -51.72/-59.87, min/max -100.0/4.2
Policy entropy: 2.289, Critic Loss: 0.084, Actor Loss 0.395

Updates 12410, training timesteps 992880, FPS 473
Last 100 training episodes: mean/median reward -57.84/-69.83, min/max -100.0/4.0
Policy entropy: 2.289, Critic Loss: 0.150, Actor Loss -0.202

Updates 12420, training timesteps 993680, FPS 473
Last 100 training episodes: mean/median reward -55.74/-63.02, min/max -100.0/6.4
Policy entropy: 2.288, Critic Loss: 0.118, Actor Loss -0.345

Updates 12430, training timesteps 994480, FPS 473
Last 100 training episodes: mean/median reward -55.11/-66.34, min/max -100.0/6.3
Policy entropy: 2.290, Critic Loss: 0.157, Actor Loss 0.235

Updates 12440, training timesteps 995280, FPS 473
Last 100 training episodes: mean/median reward -52.21/-56.88, min/max -100.0/6.6
Policy entropy: 2.291, Critic Loss: 0.128, Actor Loss -0.070

Updates 12450, training timesteps 996080, FPS 473
Last 100 training episodes: mean/median reward -59.58/-69.83, min/max -100.0/3.2
Policy entropy: 2.289, Critic Loss: 0.176, Actor Loss -0.230

Updates 12460, training timesteps 996880, FPS 473
Last 100 training episodes: mean/median reward -55.25/-69.83, min/max -100.0/3.6
Policy entropy: 2.290, Critic Loss: 0.152, Actor Loss -0.208

Updates 12470, training timesteps 997680, FPS 473
Last 100 training episodes: mean/median reward -52.01/-63.02, min/max -100.0/7.9
Policy entropy: 2.290, Critic Loss: 0.097, Actor Loss -0.263

Updates 12480, training timesteps 998480, FPS 473
Last 100 training episodes: mean/median reward -50.32/-63.02, min/max -100.0/7.0
Policy entropy: 2.287, Critic Loss: 0.317, Actor Loss -0.387

Updates 12490, training timesteps 999280, FPS 473
Last 100 training episodes: mean/median reward -54.64/-63.02, min/max -100.0/7.8
Policy entropy: 2.288, Critic Loss: 0.171, Actor Loss -0.261

Updates 12500, training timesteps 1000080, FPS 473
Last 100 training episodes: mean/median reward -56.21/-64.68, min/max -100.0/4.2
Policy entropy: 2.290, Critic Loss: 0.085, Actor Loss 0.065

Updates 12510, training timesteps 1000880, FPS 473
Last 100 training episodes: mean/median reward -61.36/-71.67, min/max -100.0/6.0
Policy entropy: 2.289, Critic Loss: 0.119, Actor Loss -0.012

Updates 12520, training timesteps 1001680, FPS 473
Last 100 training episodes: mean/median reward -57.33/-66.34, min/max -100.0/4.9
Policy entropy: 2.290, Critic Loss: 0.269, Actor Loss -0.411

Updates 12530, training timesteps 1002480, FPS 473
Last 100 training episodes: mean/median reward -51.48/-57.20, min/max -100.0/3.8
Policy entropy: 2.289, Critic Loss: 0.148, Actor Loss 0.121

Updates 12540, training timesteps 1003280, FPS 473
Last 100 training episodes: mean/median reward -48.12/-59.87, min/max -100.0/8.1
Policy entropy: 2.287, Critic Loss: 0.141, Actor Loss -0.016

Updates 12550, training timesteps 1004080, FPS 473
Last 100 training episodes: mean/median reward -53.00/-61.45, min/max -100.0/9.5
Policy entropy: 2.290, Critic Loss: 0.126, Actor Loss -0.008

Updates 12560, training timesteps 1004880, FPS 473
Last 100 training episodes: mean/median reward -54.97/-66.34, min/max -100.0/4.6
Policy entropy: 2.290, Critic Loss: 0.105, Actor Loss 0.029

Updates 12570, training timesteps 1005680, FPS 473
Last 100 training episodes: mean/median reward -60.20/-68.09, min/max -100.0/7.7
Policy entropy: 2.289, Critic Loss: 0.101, Actor Loss -0.036

Updates 12580, training timesteps 1006480, FPS 473
Last 100 training episodes: mean/median reward -54.91/-66.34, min/max -100.0/7.7
Policy entropy: 2.287, Critic Loss: 0.063, Actor Loss 0.110

Updates 12590, training timesteps 1007280, FPS 473
Last 100 training episodes: mean/median reward -49.03/-52.69, min/max -100.0/4.2
Policy entropy: 2.289, Critic Loss: 0.256, Actor Loss -0.205

Updates 12600, training timesteps 1008080, FPS 473
Last 100 training episodes: mean/median reward -50.44/-58.38, min/max -100.0/6.3
Policy entropy: 2.289, Critic Loss: 0.170, Actor Loss -0.496

Updates 12610, training timesteps 1008880, FPS 473
Last 100 training episodes: mean/median reward -49.27/-61.45, min/max -100.0/4.9
Policy entropy: 2.288, Critic Loss: 0.213, Actor Loss -0.098

Updates 12620, training timesteps 1009680, FPS 473
Last 100 training episodes: mean/median reward -47.68/-54.04, min/max -100.0/2.8
Policy entropy: 2.291, Critic Loss: 0.108, Actor Loss -0.149

Updates 12630, training timesteps 1010480, FPS 473
Last 100 training episodes: mean/median reward -59.30/-65.31, min/max -100.0/6.6
Policy entropy: 2.292, Critic Loss: 0.105, Actor Loss 0.192

Updates 12640, training timesteps 1011280, FPS 473
Last 100 training episodes: mean/median reward -55.62/-64.68, min/max -100.0/6.3
Policy entropy: 2.291, Critic Loss: 0.172, Actor Loss -0.166

Updates 12650, training timesteps 1012080, FPS 473
Last 100 training episodes: mean/median reward -53.61/-66.34, min/max -100.0/3.6
Policy entropy: 2.290, Critic Loss: 0.156, Actor Loss 0.430

Updates 12660, training timesteps 1012880, FPS 473
Last 100 training episodes: mean/median reward -52.98/-64.68, min/max -100.0/12.3
Policy entropy: 2.292, Critic Loss: 0.200, Actor Loss -0.456

Updates 12670, training timesteps 1013680, FPS 473
Last 100 training episodes: mean/median reward -61.17/-67.39, min/max -100.0/2.3
Policy entropy: 2.291, Critic Loss: 0.119, Actor Loss 0.122

Updates 12680, training timesteps 1014480, FPS 473
Last 100 training episodes: mean/median reward -55.99/-63.02, min/max -100.0/4.9
Policy entropy: 2.290, Critic Loss: 0.122, Actor Loss -0.107

Updates 12690, training timesteps 1015280, FPS 473
Last 100 training episodes: mean/median reward -52.14/-59.87, min/max -100.0/10.5
Policy entropy: 2.289, Critic Loss: 0.158, Actor Loss -0.233

Updates 12700, training timesteps 1016080, FPS 473
Last 100 training episodes: mean/median reward -62.13/-73.51, min/max -100.0/6.3
Policy entropy: 2.284, Critic Loss: 0.073, Actor Loss 0.242

Updates 12710, training timesteps 1016880, FPS 473
Last 100 training episodes: mean/median reward -50.14/-58.38, min/max -100.0/7.4
Policy entropy: 2.286, Critic Loss: 0.247, Actor Loss -0.204

Updates 12720, training timesteps 1017680, FPS 473
Last 100 training episodes: mean/median reward -52.65/-59.87, min/max -100.0/8.1
Policy entropy: 2.290, Critic Loss: 0.176, Actor Loss -0.229

Updates 12730, training timesteps 1018480, FPS 473
Last 100 training episodes: mean/median reward -55.03/-63.02, min/max -100.0/5.1
Policy entropy: 2.286, Critic Loss: 0.102, Actor Loss -0.092

Updates 12740, training timesteps 1019280, FPS 473
Last 100 training episodes: mean/median reward -52.52/-63.02, min/max -100.0/5.1
Policy entropy: 2.285, Critic Loss: 0.160, Actor Loss -0.509

Updates 12750, training timesteps 1020080, FPS 473
Last 100 training episodes: mean/median reward -54.74/-63.02, min/max -100.0/5.7
Policy entropy: 2.288, Critic Loss: 0.161, Actor Loss -0.010

Updates 12760, training timesteps 1020880, FPS 473
Last 100 training episodes: mean/median reward -47.56/-51.11, min/max -100.0/0.0
Policy entropy: 2.287, Critic Loss: 0.049, Actor Loss 0.288

Updates 12770, training timesteps 1021680, FPS 473
Last 100 training episodes: mean/median reward -48.39/-55.46, min/max -100.0/5.4
Policy entropy: 2.288, Critic Loss: 0.214, Actor Loss -0.429

Updates 12780, training timesteps 1022480, FPS 473
Last 100 training episodes: mean/median reward -58.84/-68.09, min/max -100.0/5.4
Policy entropy: 2.290, Critic Loss: 0.063, Actor Loss -0.026

Updates 12790, training timesteps 1023280, FPS 473
Last 100 training episodes: mean/median reward -52.33/-63.11, min/max -100.0/5.4
Policy entropy: 2.290, Critic Loss: 0.175, Actor Loss -0.220

Updates 12800, training timesteps 1024080, FPS 473
Last 100 training episodes: mean/median reward -47.12/-53.03, min/max -100.0/4.0
Policy entropy: 2.288, Critic Loss: 0.079, Actor Loss -0.037

Updates 12810, training timesteps 1024880, FPS 473
Last 100 training episodes: mean/median reward -52.52/-60.84, min/max -100.0/6.3
Policy entropy: 2.288, Critic Loss: 0.075, Actor Loss 0.050

Updates 12820, training timesteps 1025680, FPS 473
Last 100 training episodes: mean/median reward -58.12/-66.34, min/max -100.0/4.2
Policy entropy: 2.287, Critic Loss: 0.244, Actor Loss -0.400

Updates 12830, training timesteps 1026480, FPS 473
Last 100 training episodes: mean/median reward -52.21/-64.68, min/max -100.0/4.9
Policy entropy: 2.289, Critic Loss: 0.068, Actor Loss 0.227

Updates 12840, training timesteps 1027280, FPS 473
Last 100 training episodes: mean/median reward -51.65/-59.87, min/max -100.0/6.0
Policy entropy: 2.288, Critic Loss: 0.130, Actor Loss 0.248

Updates 12850, training timesteps 1028080, FPS 473
Last 100 training episodes: mean/median reward -53.04/-66.43, min/max -100.0/4.9
Policy entropy: 2.286, Critic Loss: 0.101, Actor Loss 0.230

Updates 12860, training timesteps 1028880, FPS 473
Last 100 training episodes: mean/median reward -48.87/-63.02, min/max -100.0/6.0
Policy entropy: 2.290, Critic Loss: 0.142, Actor Loss 0.258

Updates 12870, training timesteps 1029680, FPS 473
Last 100 training episodes: mean/median reward -56.90/-68.09, min/max -100.0/4.6
Policy entropy: 2.290, Critic Loss: 0.164, Actor Loss 0.004

Updates 12880, training timesteps 1030480, FPS 473
Last 100 training episodes: mean/median reward -55.57/-69.83, min/max -100.0/5.7
Policy entropy: 2.290, Critic Loss: 0.203, Actor Loss -0.326

Updates 12890, training timesteps 1031280, FPS 473
Last 100 training episodes: mean/median reward -46.41/-48.77, min/max -100.0/7.0
Policy entropy: 2.288, Critic Loss: 0.190, Actor Loss 0.335

Updates 12900, training timesteps 1032080, FPS 473
Last 100 training episodes: mean/median reward -52.45/-59.87, min/max -100.0/3.1
Policy entropy: 2.290, Critic Loss: 0.244, Actor Loss -0.154

Updates 12910, training timesteps 1032880, FPS 473
Last 100 training episodes: mean/median reward -50.70/-63.02, min/max -100.0/4.2
Policy entropy: 2.289, Critic Loss: 0.083, Actor Loss 0.248

Updates 12920, training timesteps 1033680, FPS 473
Last 100 training episodes: mean/median reward -58.24/-69.83, min/max -100.0/6.6
Policy entropy: 2.290, Critic Loss: 0.135, Actor Loss -0.454

Updates 12930, training timesteps 1034480, FPS 473
Last 100 training episodes: mean/median reward -54.97/-63.02, min/max -100.0/3.1
Policy entropy: 2.289, Critic Loss: 0.143, Actor Loss -0.044

Updates 12940, training timesteps 1035280, FPS 473
Last 100 training episodes: mean/median reward -51.66/-59.87, min/max -100.0/0.0
Policy entropy: 2.293, Critic Loss: 0.101, Actor Loss 0.028

Updates 12950, training timesteps 1036080, FPS 473
Last 100 training episodes: mean/median reward -60.63/-71.67, min/max -100.0/10.0
Policy entropy: 2.289, Critic Loss: 0.084, Actor Loss 0.066

Updates 12960, training timesteps 1036880, FPS 473
Last 100 training episodes: mean/median reward -58.73/-68.09, min/max -100.0/6.3
Policy entropy: 2.289, Critic Loss: 0.113, Actor Loss -0.105

Updates 12970, training timesteps 1037680, FPS 473
Last 100 training episodes: mean/median reward -58.94/-71.67, min/max -100.0/6.0
Policy entropy: 2.288, Critic Loss: 0.090, Actor Loss 0.042

Updates 12980, training timesteps 1038480, FPS 473
Last 100 training episodes: mean/median reward -62.44/-73.51, min/max -100.0/3.8
Policy entropy: 2.289, Critic Loss: 0.103, Actor Loss 0.106

Updates 12990, training timesteps 1039280, FPS 473
Last 100 training episodes: mean/median reward -56.52/-66.34, min/max -100.0/0.0
Policy entropy: 2.287, Critic Loss: 0.058, Actor Loss 0.314

Updates 13000, training timesteps 1040080, FPS 473
Last 100 training episodes: mean/median reward -58.09/-69.83, min/max -100.0/4.2
Policy entropy: 2.288, Critic Loss: 0.084, Actor Loss 0.328

Updates 13010, training timesteps 1040880, FPS 473
Last 100 training episodes: mean/median reward -55.05/-63.02, min/max -100.0/4.2
Policy entropy: 2.289, Critic Loss: 0.121, Actor Loss 0.132

Updates 13020, training timesteps 1041680, FPS 473
Last 100 training episodes: mean/median reward -47.97/-56.88, min/max -100.0/6.6
Policy entropy: 2.289, Critic Loss: 0.120, Actor Loss -0.129

Updates 13030, training timesteps 1042480, FPS 473
Last 100 training episodes: mean/median reward -50.75/-57.98, min/max -100.0/4.6
Policy entropy: 2.286, Critic Loss: 0.132, Actor Loss -0.193

Updates 13040, training timesteps 1043280, FPS 473
Last 100 training episodes: mean/median reward -61.12/-71.67, min/max -100.0/0.0
Policy entropy: 2.287, Critic Loss: 0.099, Actor Loss 0.207

Updates 13050, training timesteps 1044080, FPS 473
Last 100 training episodes: mean/median reward -59.37/-68.09, min/max -100.0/6.0
Policy entropy: 2.288, Critic Loss: 0.181, Actor Loss -0.152

Updates 13060, training timesteps 1044880, FPS 473
Last 100 training episodes: mean/median reward -53.72/-58.38, min/max -100.0/6.0
Policy entropy: 2.289, Critic Loss: 0.091, Actor Loss 0.406

Updates 13070, training timesteps 1045680, FPS 473
Last 100 training episodes: mean/median reward -50.39/-56.95, min/max -100.0/4.2
Policy entropy: 2.290, Critic Loss: 0.232, Actor Loss -0.543

Updates 13080, training timesteps 1046480, FPS 473
Last 100 training episodes: mean/median reward -58.55/-64.68, min/max -100.0/0.0
Policy entropy: 2.289, Critic Loss: 0.123, Actor Loss -0.388

Updates 13090, training timesteps 1047280, FPS 473
Last 100 training episodes: mean/median reward -61.68/-66.34, min/max -100.0/3.8
Policy entropy: 2.289, Critic Loss: 0.059, Actor Loss 0.094

Updates 13100, training timesteps 1048080, FPS 473
Last 100 training episodes: mean/median reward -61.26/-73.51, min/max -100.0/4.9
Policy entropy: 2.287, Critic Loss: 0.098, Actor Loss -0.037

Updates 13110, training timesteps 1048880, FPS 473
Last 100 training episodes: mean/median reward -51.12/-59.87, min/max -100.0/8.7
Policy entropy: 2.288, Critic Loss: 0.163, Actor Loss -0.321

Updates 13120, training timesteps 1049680, FPS 473
Last 100 training episodes: mean/median reward -56.57/-69.83, min/max -100.0/4.9
Policy entropy: 2.288, Critic Loss: 0.095, Actor Loss -0.078

Updates 13130, training timesteps 1050480, FPS 473
Last 100 training episodes: mean/median reward -48.31/-56.88, min/max -100.0/4.4
Policy entropy: 2.288, Critic Loss: 0.119, Actor Loss -0.034

Updates 13140, training timesteps 1051280, FPS 473
Last 100 training episodes: mean/median reward -52.35/-63.02, min/max -100.0/4.9
Policy entropy: 2.285, Critic Loss: 0.065, Actor Loss 0.277

Updates 13150, training timesteps 1052080, FPS 473
Last 100 training episodes: mean/median reward -53.87/-59.87, min/max -100.0/7.0
Policy entropy: 2.288, Critic Loss: 0.409, Actor Loss -0.978

Updates 13160, training timesteps 1052880, FPS 473
Last 100 training episodes: mean/median reward -49.14/-55.46, min/max -100.0/5.4
Policy entropy: 2.290, Critic Loss: 0.171, Actor Loss 0.013

Updates 13170, training timesteps 1053680, FPS 473
Last 100 training episodes: mean/median reward -57.67/-69.83, min/max -100.0/3.4
Policy entropy: 2.288, Critic Loss: 0.134, Actor Loss 0.191

Updates 13180, training timesteps 1054480, FPS 473
Last 100 training episodes: mean/median reward -57.38/-66.34, min/max -100.0/6.0
Policy entropy: 2.287, Critic Loss: 0.114, Actor Loss -0.055

Updates 13190, training timesteps 1055280, FPS 473
Last 100 training episodes: mean/median reward -47.23/-51.58, min/max -100.0/5.4
Policy entropy: 2.287, Critic Loss: 0.145, Actor Loss -0.150

Updates 13200, training timesteps 1056080, FPS 473
Last 100 training episodes: mean/median reward -52.05/-61.45, min/max -100.0/6.3
Policy entropy: 2.286, Critic Loss: 0.123, Actor Loss 0.055

Updates 13210, training timesteps 1056880, FPS 473
Last 100 training episodes: mean/median reward -58.12/-68.09, min/max -100.0/5.1
Policy entropy: 2.287, Critic Loss: 0.152, Actor Loss -0.264

Updates 13220, training timesteps 1057680, FPS 473
Last 100 training episodes: mean/median reward -46.40/-55.46, min/max -100.0/14.4
Policy entropy: 2.284, Critic Loss: 0.196, Actor Loss -0.284

Updates 13230, training timesteps 1058480, FPS 473
Last 100 training episodes: mean/median reward -52.39/-59.04, min/max -100.0/6.6
Policy entropy: 2.287, Critic Loss: 0.117, Actor Loss 0.412

Updates 13240, training timesteps 1059280, FPS 473
Last 100 training episodes: mean/median reward -51.85/-59.87, min/max -100.0/10.4
Policy entropy: 2.289, Critic Loss: 0.141, Actor Loss -0.363

Updates 13250, training timesteps 1060080, FPS 473
Last 100 training episodes: mean/median reward -58.57/-69.83, min/max -100.0/10.0
Policy entropy: 2.288, Critic Loss: 0.084, Actor Loss 0.113

Updates 13260, training timesteps 1060880, FPS 473
Last 100 training episodes: mean/median reward -53.32/-59.87, min/max -100.0/10.0
Policy entropy: 2.292, Critic Loss: 0.188, Actor Loss -0.199

Updates 13270, training timesteps 1061680, FPS 473
Last 100 training episodes: mean/median reward -50.46/-58.38, min/max -100.0/6.3
Policy entropy: 2.286, Critic Loss: 0.148, Actor Loss -0.209

Updates 13280, training timesteps 1062480, FPS 473
Last 100 training episodes: mean/median reward -56.01/-69.83, min/max -100.0/6.0
Policy entropy: 2.291, Critic Loss: 0.259, Actor Loss -0.699

Updates 13290, training timesteps 1063280, FPS 473
Last 100 training episodes: mean/median reward -53.21/-66.34, min/max -100.0/6.6
Policy entropy: 2.289, Critic Loss: 0.120, Actor Loss -0.321

Updates 13300, training timesteps 1064080, FPS 473
Last 100 training episodes: mean/median reward -59.36/-66.34, min/max -100.0/2.0
Policy entropy: 2.289, Critic Loss: 0.106, Actor Loss -0.192

Updates 13310, training timesteps 1064880, FPS 473
Last 100 training episodes: mean/median reward -54.71/-69.83, min/max -100.0/4.0
Policy entropy: 2.290, Critic Loss: 0.213, Actor Loss 0.068

Updates 13320, training timesteps 1065680, FPS 473
Last 100 training episodes: mean/median reward -55.22/-66.34, min/max -100.0/7.0
Policy entropy: 2.288, Critic Loss: 0.219, Actor Loss -0.221

Updates 13330, training timesteps 1066480, FPS 473
Last 100 training episodes: mean/median reward -45.41/-46.33, min/max -100.0/4.4
Policy entropy: 2.292, Critic Loss: 0.148, Actor Loss 0.651

Updates 13340, training timesteps 1067280, FPS 473
Last 100 training episodes: mean/median reward -57.35/-69.83, min/max -100.0/10.8
Policy entropy: 2.289, Critic Loss: 0.191, Actor Loss -0.267

Updates 13350, training timesteps 1068080, FPS 473
Last 100 training episodes: mean/median reward -61.00/-69.83, min/max -100.0/9.7
Policy entropy: 2.290, Critic Loss: 0.091, Actor Loss 0.086

Updates 13360, training timesteps 1068880, FPS 473
Last 100 training episodes: mean/median reward -55.81/-66.34, min/max -100.0/7.4
Policy entropy: 2.289, Critic Loss: 0.131, Actor Loss 0.163

Updates 13370, training timesteps 1069680, FPS 473
Last 100 training episodes: mean/median reward -57.29/-66.34, min/max -100.0/6.6
Policy entropy: 2.289, Critic Loss: 0.109, Actor Loss -0.254

Updates 13380, training timesteps 1070480, FPS 473
Last 100 training episodes: mean/median reward -55.12/-66.06, min/max -100.0/4.6
Policy entropy: 2.291, Critic Loss: 0.066, Actor Loss 0.120

Updates 13390, training timesteps 1071280, FPS 473
Last 100 training episodes: mean/median reward -50.44/-59.87, min/max -100.0/3.6
Policy entropy: 2.287, Critic Loss: 0.102, Actor Loss 0.059

Updates 13400, training timesteps 1072080, FPS 473
Last 100 training episodes: mean/median reward -52.45/-65.85, min/max -100.0/5.4
Policy entropy: 2.290, Critic Loss: 0.125, Actor Loss -0.107

Updates 13410, training timesteps 1072880, FPS 473
Last 100 training episodes: mean/median reward -55.04/-66.34, min/max -100.0/5.4
Policy entropy: 2.290, Critic Loss: 0.180, Actor Loss 0.092

Updates 13420, training timesteps 1073680, FPS 473
Last 100 training episodes: mean/median reward -47.97/-58.38, min/max -100.0/5.7
Policy entropy: 2.288, Critic Loss: 0.102, Actor Loss 0.098

Updates 13430, training timesteps 1074480, FPS 473
Last 100 training episodes: mean/median reward -55.80/-66.34, min/max -100.0/5.7
Policy entropy: 2.290, Critic Loss: 0.124, Actor Loss 0.249

Updates 13440, training timesteps 1075280, FPS 473
Last 100 training episodes: mean/median reward -53.61/-63.11, min/max -100.0/5.7
Policy entropy: 2.291, Critic Loss: 0.114, Actor Loss -0.089

Updates 13450, training timesteps 1076080, FPS 473
Last 100 training episodes: mean/median reward -56.96/-68.09, min/max -100.0/3.6
Policy entropy: 2.289, Critic Loss: 0.100, Actor Loss 0.030

Updates 13460, training timesteps 1076880, FPS 473
Last 100 training episodes: mean/median reward -60.60/-73.51, min/max -100.0/4.4
Policy entropy: 2.291, Critic Loss: 0.089, Actor Loss -0.145

Updates 13470, training timesteps 1077680, FPS 473
Last 100 training episodes: mean/median reward -60.71/-66.34, min/max -100.0/6.6
Policy entropy: 2.291, Critic Loss: 0.087, Actor Loss 0.046

Updates 13480, training timesteps 1078480, FPS 473
Last 100 training episodes: mean/median reward -57.35/-66.34, min/max -100.0/0.0
Policy entropy: 2.292, Critic Loss: 0.136, Actor Loss -0.154

Updates 13490, training timesteps 1079280, FPS 473
Last 100 training episodes: mean/median reward -49.19/-59.87, min/max -100.0/4.4
Policy entropy: 2.293, Critic Loss: 0.122, Actor Loss 0.006

Updates 13500, training timesteps 1080080, FPS 473
Last 100 training episodes: mean/median reward -55.33/-63.02, min/max -100.0/5.7
Policy entropy: 2.293, Critic Loss: 0.162, Actor Loss -0.003

Updates 13510, training timesteps 1080880, FPS 473
Last 100 training episodes: mean/median reward -48.40/-56.88, min/max -100.0/7.4
Policy entropy: 2.290, Critic Loss: 0.079, Actor Loss 0.257

Updates 13520, training timesteps 1081680, FPS 473
Last 100 training episodes: mean/median reward -53.79/-63.02, min/max -100.0/5.1
Policy entropy: 2.291, Critic Loss: 0.190, Actor Loss -0.517

Updates 13530, training timesteps 1082480, FPS 473
Last 100 training episodes: mean/median reward -60.25/-68.09, min/max -100.0/4.0
Policy entropy: 2.291, Critic Loss: 0.152, Actor Loss -0.140

Updates 13540, training timesteps 1083280, FPS 473
Last 100 training episodes: mean/median reward -57.07/-69.83, min/max -100.0/6.8
Policy entropy: 2.289, Critic Loss: 0.169, Actor Loss -0.007

Updates 13550, training timesteps 1084080, FPS 473
Last 100 training episodes: mean/median reward -61.39/-69.83, min/max -100.0/4.4
Policy entropy: 2.288, Critic Loss: 0.142, Actor Loss 0.318

Updates 13560, training timesteps 1084880, FPS 473
Last 100 training episodes: mean/median reward -52.02/-59.87, min/max -100.0/6.6
Policy entropy: 2.290, Critic Loss: 0.160, Actor Loss -0.137

Updates 13570, training timesteps 1085680, FPS 473
Last 100 training episodes: mean/median reward -51.80/-63.02, min/max -100.0/5.4
Policy entropy: 2.291, Critic Loss: 0.244, Actor Loss -0.239

Updates 13580, training timesteps 1086480, FPS 473
Last 100 training episodes: mean/median reward -50.39/-56.95, min/max -100.0/6.5
Policy entropy: 2.290, Critic Loss: 0.276, Actor Loss -0.780

Updates 13590, training timesteps 1087280, FPS 473
Last 100 training episodes: mean/median reward -54.21/-64.68, min/max -100.0/6.3
Policy entropy: 2.288, Critic Loss: 0.143, Actor Loss 0.150

Updates 13600, training timesteps 1088080, FPS 473
Last 100 training episodes: mean/median reward -52.54/-63.02, min/max -100.0/2.8
Policy entropy: 2.288, Critic Loss: 0.078, Actor Loss 0.098

Updates 13610, training timesteps 1088880, FPS 473
Last 100 training episodes: mean/median reward -55.97/-66.34, min/max -100.0/9.4
Policy entropy: 2.290, Critic Loss: 0.081, Actor Loss 0.210

Updates 13620, training timesteps 1089680, FPS 473
Last 100 training episodes: mean/median reward -50.04/-61.45, min/max -100.0/2.9
Policy entropy: 2.288, Critic Loss: 0.072, Actor Loss -0.134

Updates 13630, training timesteps 1090480, FPS 473
Last 100 training episodes: mean/median reward -50.83/-60.47, min/max -100.0/4.4
Policy entropy: 2.290, Critic Loss: 0.092, Actor Loss 0.157

Updates 13640, training timesteps 1091280, FPS 473
Last 100 training episodes: mean/median reward -59.47/-73.51, min/max -100.0/8.6
Policy entropy: 2.290, Critic Loss: 0.164, Actor Loss -0.194

Updates 13650, training timesteps 1092080, FPS 473
Last 100 training episodes: mean/median reward -58.99/-68.09, min/max -100.0/6.6
Policy entropy: 2.290, Critic Loss: 0.054, Actor Loss 0.057

Updates 13660, training timesteps 1092880, FPS 473
Last 100 training episodes: mean/median reward -59.45/-69.83, min/max -100.0/4.6
Policy entropy: 2.291, Critic Loss: 0.081, Actor Loss -0.136

Updates 13670, training timesteps 1093680, FPS 473
Last 100 training episodes: mean/median reward -59.43/-73.51, min/max -100.0/8.1
Policy entropy: 2.288, Critic Loss: 0.056, Actor Loss 0.085

Updates 13680, training timesteps 1094480, FPS 473
Last 100 training episodes: mean/median reward -49.75/-61.45, min/max -100.0/4.2
Policy entropy: 2.291, Critic Loss: 0.312, Actor Loss -0.601

Updates 13690, training timesteps 1095280, FPS 473
Last 100 training episodes: mean/median reward -54.71/-66.34, min/max -100.0/0.0
Policy entropy: 2.288, Critic Loss: 0.107, Actor Loss 0.045

Updates 13700, training timesteps 1096080, FPS 473
Last 100 training episodes: mean/median reward -53.32/-56.88, min/max -100.0/3.8
Policy entropy: 2.286, Critic Loss: 0.062, Actor Loss 0.213

Updates 13710, training timesteps 1096880, FPS 473
Last 100 training episodes: mean/median reward -54.94/-66.34, min/max -100.0/4.6
Policy entropy: 2.287, Critic Loss: 0.096, Actor Loss 0.185

Updates 13720, training timesteps 1097680, FPS 473
Last 100 training episodes: mean/median reward -57.10/-64.68, min/max -100.0/4.6
Policy entropy: 2.290, Critic Loss: 0.186, Actor Loss -0.269

Updates 13730, training timesteps 1098480, FPS 473
Last 100 training episodes: mean/median reward -51.44/-59.87, min/max -100.0/6.3
Policy entropy: 2.291, Critic Loss: 0.105, Actor Loss 0.225

Updates 13740, training timesteps 1099280, FPS 473
Last 100 training episodes: mean/median reward -56.93/-68.09, min/max -100.0/3.2
Policy entropy: 2.290, Critic Loss: 0.078, Actor Loss -0.202

Updates 13750, training timesteps 1100080, FPS 473
Last 100 training episodes: mean/median reward -56.58/-66.34, min/max -100.0/11.5
Policy entropy: 2.288, Critic Loss: 0.088, Actor Loss 0.049

Updates 13760, training timesteps 1100880, FPS 473
Last 100 training episodes: mean/median reward -51.36/-59.62, min/max -100.0/9.8
Policy entropy: 2.286, Critic Loss: 0.119, Actor Loss -0.013

Updates 13770, training timesteps 1101680, FPS 473
Last 100 training episodes: mean/median reward -53.72/-59.87, min/max -100.0/5.4
Policy entropy: 2.291, Critic Loss: 0.117, Actor Loss -0.303

Updates 13780, training timesteps 1102480, FPS 473
Last 100 training episodes: mean/median reward -55.43/-69.83, min/max -100.0/4.6
Policy entropy: 2.292, Critic Loss: 0.149, Actor Loss -0.341

Updates 13790, training timesteps 1103280, FPS 473
Last 100 training episodes: mean/median reward -54.97/-66.34, min/max -100.0/12.4
Policy entropy: 2.291, Critic Loss: 0.141, Actor Loss -0.054

Updates 13800, training timesteps 1104080, FPS 473
Last 100 training episodes: mean/median reward -58.26/-61.45, min/max -100.0/2.3
Policy entropy: 2.289, Critic Loss: 0.088, Actor Loss -0.065

Updates 13810, training timesteps 1104880, FPS 473
Last 100 training episodes: mean/median reward -57.64/-73.51, min/max -100.0/4.0
Policy entropy: 2.290, Critic Loss: 0.094, Actor Loss -0.172

Updates 13820, training timesteps 1105680, FPS 473
Last 100 training episodes: mean/median reward -53.48/-64.68, min/max -100.0/4.6
Policy entropy: 2.288, Critic Loss: 0.126, Actor Loss -0.235

Updates 13830, training timesteps 1106480, FPS 473
Last 100 training episodes: mean/median reward -61.44/-64.68, min/max -100.0/0.0
Policy entropy: 2.289, Critic Loss: 0.133, Actor Loss 0.105

Updates 13840, training timesteps 1107280, FPS 473
Last 100 training episodes: mean/median reward -53.21/-59.87, min/max -100.0/4.4
Policy entropy: 2.288, Critic Loss: 0.131, Actor Loss -0.227

Updates 13850, training timesteps 1108080, FPS 473
Last 100 training episodes: mean/median reward -53.73/-67.18, min/max -100.0/0.0
Policy entropy: 2.289, Critic Loss: 0.110, Actor Loss -0.211

Updates 13860, training timesteps 1108880, FPS 473
Last 100 training episodes: mean/median reward -53.48/-67.11, min/max -100.0/7.7
Policy entropy: 2.289, Critic Loss: 0.241, Actor Loss -0.430

Updates 13870, training timesteps 1109680, FPS 473
Last 100 training episodes: mean/median reward -53.71/-61.45, min/max -100.0/4.6
Policy entropy: 2.291, Critic Loss: 0.148, Actor Loss -0.330

Updates 13880, training timesteps 1110480, FPS 473
Last 100 training episodes: mean/median reward -58.22/-66.34, min/max -100.0/4.6
Policy entropy: 2.288, Critic Loss: 0.099, Actor Loss 0.276

Updates 13890, training timesteps 1111280, FPS 473
Last 100 training episodes: mean/median reward -55.88/-68.93, min/max -100.0/6.0
Policy entropy: 2.287, Critic Loss: 0.101, Actor Loss 0.052

Updates 13900, training timesteps 1112080, FPS 473
Last 100 training episodes: mean/median reward -50.07/-61.45, min/max -100.0/9.9
Policy entropy: 2.291, Critic Loss: 0.161, Actor Loss 0.366

Updates 13910, training timesteps 1112880, FPS 473
Last 100 training episodes: mean/median reward -55.30/-64.68, min/max -100.0/5.5
Policy entropy: 2.291, Critic Loss: 0.132, Actor Loss 0.048

Updates 13920, training timesteps 1113680, FPS 473
Last 100 training episodes: mean/median reward -51.76/-63.02, min/max -100.0/0.0
Policy entropy: 2.291, Critic Loss: 0.045, Actor Loss 0.245

Updates 13930, training timesteps 1114480, FPS 473
Last 100 training episodes: mean/median reward -50.76/-59.87, min/max -100.0/4.2
Policy entropy: 2.291, Critic Loss: 0.063, Actor Loss 0.278

Updates 13940, training timesteps 1115280, FPS 473
Last 100 training episodes: mean/median reward -54.15/-61.45, min/max -100.0/3.8
Policy entropy: 2.292, Critic Loss: 0.229, Actor Loss -0.137

Updates 13950, training timesteps 1116080, FPS 473
Last 100 training episodes: mean/median reward -59.13/-73.51, min/max -100.0/6.6
Policy entropy: 2.287, Critic Loss: 0.078, Actor Loss 0.218

Updates 13960, training timesteps 1116880, FPS 473
Last 100 training episodes: mean/median reward -54.16/-64.68, min/max -100.0/6.3
Policy entropy: 2.289, Critic Loss: 0.116, Actor Loss -0.283

Updates 13970, training timesteps 1117680, FPS 473
Last 100 training episodes: mean/median reward -53.06/-66.34, min/max -100.0/7.0
Policy entropy: 2.290, Critic Loss: 0.165, Actor Loss -0.255

Updates 13980, training timesteps 1118480, FPS 473
Last 100 training episodes: mean/median reward -51.78/-59.87, min/max -100.0/3.8
Policy entropy: 2.289, Critic Loss: 0.140, Actor Loss -0.027

Updates 13990, training timesteps 1119280, FPS 473
Last 100 training episodes: mean/median reward -48.42/-53.88, min/max -100.0/5.1
Policy entropy: 2.290, Critic Loss: 0.162, Actor Loss -0.008

Updates 14000, training timesteps 1120080, FPS 473
Last 100 training episodes: mean/median reward -58.84/-69.83, min/max -100.0/2.6
Policy entropy: 2.289, Critic Loss: 0.184, Actor Loss -0.393

Updates 14010, training timesteps 1120880, FPS 473
Last 100 training episodes: mean/median reward -51.87/-66.34, min/max -100.0/7.0
Policy entropy: 2.289, Critic Loss: 0.103, Actor Loss -0.057

Updates 14020, training timesteps 1121680, FPS 473
Last 100 training episodes: mean/median reward -59.23/-69.83, min/max -100.0/5.7
Policy entropy: 2.289, Critic Loss: 0.112, Actor Loss -0.140

Updates 14030, training timesteps 1122480, FPS 473
Last 100 training episodes: mean/median reward -59.53/-69.83, min/max -100.0/1.7
Policy entropy: 2.290, Critic Loss: 0.238, Actor Loss -0.317

Updates 14040, training timesteps 1123280, FPS 473
Last 100 training episodes: mean/median reward -55.58/-68.09, min/max -100.0/5.7
Policy entropy: 2.288, Critic Loss: 0.151, Actor Loss -0.476

Updates 14050, training timesteps 1124080, FPS 473
Last 100 training episodes: mean/median reward -49.95/-54.11, min/max -100.0/6.3
Policy entropy: 2.287, Critic Loss: 0.157, Actor Loss -0.176

Updates 14060, training timesteps 1124880, FPS 473
Last 100 training episodes: mean/median reward -51.68/-59.87, min/max -100.0/2.9
Policy entropy: 2.289, Critic Loss: 0.107, Actor Loss 0.201

Updates 14070, training timesteps 1125680, FPS 473
Last 100 training episodes: mean/median reward -52.37/-61.45, min/max -100.0/4.9
Policy entropy: 2.291, Critic Loss: 0.097, Actor Loss -0.070

Updates 14080, training timesteps 1126480, FPS 473
Last 100 training episodes: mean/median reward -56.54/-66.98, min/max -100.0/4.6
Policy entropy: 2.290, Critic Loss: 0.121, Actor Loss -0.315

Updates 14090, training timesteps 1127280, FPS 473
Last 100 training episodes: mean/median reward -48.85/-56.88, min/max -100.0/5.1
Policy entropy: 2.292, Critic Loss: 0.174, Actor Loss -0.387

Updates 14100, training timesteps 1128080, FPS 473
Last 100 training episodes: mean/median reward -52.83/-59.08, min/max -100.0/3.1
Policy entropy: 2.290, Critic Loss: 0.150, Actor Loss -0.320

Updates 14110, training timesteps 1128880, FPS 473
Last 100 training episodes: mean/median reward -52.43/-59.87, min/max -100.0/4.9
Policy entropy: 2.292, Critic Loss: 0.120, Actor Loss -0.122

Updates 14120, training timesteps 1129680, FPS 473
Last 100 training episodes: mean/median reward -54.60/-58.38, min/max -100.0/5.4
Policy entropy: 2.291, Critic Loss: 0.094, Actor Loss -0.221

Updates 14130, training timesteps 1130480, FPS 473
Last 100 training episodes: mean/median reward -52.09/-63.02, min/max -100.0/6.3
Policy entropy: 2.291, Critic Loss: 0.096, Actor Loss 0.010

Updates 14140, training timesteps 1131280, FPS 473
Last 100 training episodes: mean/median reward -61.39/-64.68, min/max -100.0/0.0
Policy entropy: 2.291, Critic Loss: 0.127, Actor Loss 0.038

Updates 14150, training timesteps 1132080, FPS 473
Last 100 training episodes: mean/median reward -57.70/-66.34, min/max -100.0/4.0
Policy entropy: 2.289, Critic Loss: 0.216, Actor Loss -0.545

Updates 14160, training timesteps 1132880, FPS 473
Last 100 training episodes: mean/median reward -50.00/-54.04, min/max -100.0/4.0
Policy entropy: 2.289, Critic Loss: 0.189, Actor Loss -0.189

Updates 14170, training timesteps 1133680, FPS 473
Last 100 training episodes: mean/median reward -49.55/-55.46, min/max -100.0/6.1
Policy entropy: 2.288, Critic Loss: 0.083, Actor Loss 0.347

Updates 14180, training timesteps 1134480, FPS 473
Last 100 training episodes: mean/median reward -49.80/-51.53, min/max -100.0/6.1
Policy entropy: 2.289, Critic Loss: 0.125, Actor Loss -0.129

Updates 14190, training timesteps 1135280, FPS 473
Last 100 training episodes: mean/median reward -58.20/-71.67, min/max -100.0/5.7
Policy entropy: 2.290, Critic Loss: 0.211, Actor Loss 0.003

Updates 14200, training timesteps 1136080, FPS 473
Last 100 training episodes: mean/median reward -52.68/-62.40, min/max -100.0/6.3
Policy entropy: 2.288, Critic Loss: 0.377, Actor Loss -1.209

Updates 14210, training timesteps 1136880, FPS 473
Last 100 training episodes: mean/median reward -59.29/-66.34, min/max -100.0/11.9
Policy entropy: 2.286, Critic Loss: 0.092, Actor Loss -0.177

Updates 14220, training timesteps 1137680, FPS 473
Last 100 training episodes: mean/median reward -55.81/-68.09, min/max -100.0/5.7
Policy entropy: 2.286, Critic Loss: 0.195, Actor Loss -0.206

Updates 14230, training timesteps 1138480, FPS 473
Last 100 training episodes: mean/median reward -56.99/-66.43, min/max -100.0/2.6
Policy entropy: 2.290, Critic Loss: 0.113, Actor Loss -0.084

Updates 14240, training timesteps 1139280, FPS 473
Last 100 training episodes: mean/median reward -52.78/-63.02, min/max -100.0/5.1
Policy entropy: 2.289, Critic Loss: 0.201, Actor Loss -0.449

Updates 14250, training timesteps 1140080, FPS 473
Last 100 training episodes: mean/median reward -55.77/-67.48, min/max -100.0/6.6
Policy entropy: 2.290, Critic Loss: 0.226, Actor Loss -0.487

Updates 14260, training timesteps 1140880, FPS 473
Last 100 training episodes: mean/median reward -55.85/-68.09, min/max -100.0/12.8
Policy entropy: 2.290, Critic Loss: 0.240, Actor Loss -0.315

Updates 14270, training timesteps 1141680, FPS 473
Last 100 training episodes: mean/median reward -54.58/-66.34, min/max -100.0/9.5
Policy entropy: 2.289, Critic Loss: 0.121, Actor Loss -0.299

Updates 14280, training timesteps 1142480, FPS 473
Last 100 training episodes: mean/median reward -55.76/-69.83, min/max -100.0/0.0
Policy entropy: 2.290, Critic Loss: 0.059, Actor Loss 0.069

Updates 14290, training timesteps 1143280, FPS 473
Last 100 training episodes: mean/median reward -60.52/-77.38, min/max -100.0/4.4
Policy entropy: 2.289, Critic Loss: 0.066, Actor Loss 0.384

Updates 14300, training timesteps 1144080, FPS 473
Last 100 training episodes: mean/median reward -52.01/-60.21, min/max -100.0/0.0
Policy entropy: 2.291, Critic Loss: 0.146, Actor Loss -0.467

Updates 14310, training timesteps 1144880, FPS 473
Last 100 training episodes: mean/median reward -52.95/-64.86, min/max -100.0/5.7
Policy entropy: 2.290, Critic Loss: 0.100, Actor Loss 0.098

Updates 14320, training timesteps 1145680, FPS 473
Last 100 training episodes: mean/median reward -54.34/-64.68, min/max -100.0/5.7
Policy entropy: 2.287, Critic Loss: 0.218, Actor Loss -0.036

Updates 14330, training timesteps 1146480, FPS 473
Last 100 training episodes: mean/median reward -55.66/-63.02, min/max -100.0/10.4
Policy entropy: 2.288, Critic Loss: 0.168, Actor Loss -0.174

Updates 14340, training timesteps 1147280, FPS 473
Last 100 training episodes: mean/median reward -56.98/-66.34, min/max -100.0/3.8
Policy entropy: 2.288, Critic Loss: 0.081, Actor Loss 0.400

Updates 14350, training timesteps 1148080, FPS 473
Last 100 training episodes: mean/median reward -56.22/-66.34, min/max -100.0/5.1
Policy entropy: 2.286, Critic Loss: 0.118, Actor Loss 0.163

Updates 14360, training timesteps 1148880, FPS 473
Last 100 training episodes: mean/median reward -51.89/-61.45, min/max -100.0/5.5
Policy entropy: 2.288, Critic Loss: 0.120, Actor Loss 0.036

Updates 14370, training timesteps 1149680, FPS 473
Last 100 training episodes: mean/median reward -53.01/-63.02, min/max -100.0/5.1
Policy entropy: 2.289, Critic Loss: 0.128, Actor Loss -0.024

Updates 14380, training timesteps 1150480, FPS 473
Last 100 training episodes: mean/median reward -50.81/-59.95, min/max -100.0/17.8
Policy entropy: 2.286, Critic Loss: 0.064, Actor Loss 0.181

Updates 14390, training timesteps 1151280, FPS 473
Last 100 training episodes: mean/median reward -48.32/-48.77, min/max -100.0/10.4
Policy entropy: 2.291, Critic Loss: 0.200, Actor Loss -0.246

Updates 14400, training timesteps 1152080, FPS 473
Last 100 training episodes: mean/median reward -54.94/-63.02, min/max -100.0/11.9
Policy entropy: 2.290, Critic Loss: 0.116, Actor Loss -0.147

Updates 14410, training timesteps 1152880, FPS 473
Last 100 training episodes: mean/median reward -57.54/-68.97, min/max -100.0/11.9
Policy entropy: 2.291, Critic Loss: 0.091, Actor Loss 0.181

Updates 14420, training timesteps 1153680, FPS 473
Last 100 training episodes: mean/median reward -55.63/-59.87, min/max -100.0/3.2
Policy entropy: 2.290, Critic Loss: 0.165, Actor Loss 0.136

Updates 14430, training timesteps 1154480, FPS 473
Last 100 training episodes: mean/median reward -57.12/-63.02, min/max -100.0/7.0
Policy entropy: 2.290, Critic Loss: 0.082, Actor Loss -0.131

Updates 14440, training timesteps 1155280, FPS 473
Last 100 training episodes: mean/median reward -51.46/-63.02, min/max -100.0/13.0
Policy entropy: 2.288, Critic Loss: 0.102, Actor Loss 0.102

Updates 14450, training timesteps 1156080, FPS 473
Last 100 training episodes: mean/median reward -51.97/-59.87, min/max -100.0/13.0
Policy entropy: 2.287, Critic Loss: 0.107, Actor Loss 0.089

Updates 14460, training timesteps 1156880, FPS 473
Last 100 training episodes: mean/median reward -62.50/-73.51, min/max -100.0/5.7
Policy entropy: 2.287, Critic Loss: 0.127, Actor Loss -0.213

Updates 14470, training timesteps 1157680, FPS 473
Last 100 training episodes: mean/median reward -57.46/-66.34, min/max -100.0/8.6
Policy entropy: 2.286, Critic Loss: 0.106, Actor Loss 0.314

Updates 14480, training timesteps 1158480, FPS 473
Last 100 training episodes: mean/median reward -59.67/-73.51, min/max -100.0/5.1
Policy entropy: 2.287, Critic Loss: 0.149, Actor Loss -0.352

Updates 14490, training timesteps 1159280, FPS 473
Last 100 training episodes: mean/median reward -62.27/-73.51, min/max -100.0/0.0
Policy entropy: 2.290, Critic Loss: 0.337, Actor Loss -0.957

Updates 14500, training timesteps 1160080, FPS 473
Last 100 training episodes: mean/median reward -43.45/-50.05, min/max -100.0/3.6
Policy entropy: 2.292, Critic Loss: 0.134, Actor Loss -0.142

Updates 14510, training timesteps 1160880, FPS 473
Last 100 training episodes: mean/median reward -55.73/-66.34, min/max -100.0/5.1
Policy entropy: 2.290, Critic Loss: 0.196, Actor Loss 0.487

Updates 14520, training timesteps 1161680, FPS 473
Last 100 training episodes: mean/median reward -55.54/-64.68, min/max -100.0/3.1
Policy entropy: 2.289, Critic Loss: 0.115, Actor Loss 0.140

Updates 14530, training timesteps 1162480, FPS 473
Last 100 training episodes: mean/median reward -50.10/-52.69, min/max -100.0/4.6
Policy entropy: 2.290, Critic Loss: 0.094, Actor Loss 0.352

Updates 14540, training timesteps 1163280, FPS 473
Last 100 training episodes: mean/median reward -55.40/-61.45, min/max -100.0/11.9
Policy entropy: 2.290, Critic Loss: 0.144, Actor Loss -0.143

Updates 14550, training timesteps 1164080, FPS 473
Last 100 training episodes: mean/median reward -56.12/-66.34, min/max -100.0/8.8
Policy entropy: 2.288, Critic Loss: 0.193, Actor Loss -0.038

Updates 14560, training timesteps 1164880, FPS 473
Last 100 training episodes: mean/median reward -49.30/-54.04, min/max -100.0/8.8
Policy entropy: 2.291, Critic Loss: 0.248, Actor Loss -0.306

Updates 14570, training timesteps 1165680, FPS 473
Last 100 training episodes: mean/median reward -51.89/-59.87, min/max -100.0/1.6
Policy entropy: 2.290, Critic Loss: 0.175, Actor Loss -0.545

Updates 14580, training timesteps 1166480, FPS 473
Last 100 training episodes: mean/median reward -59.47/-73.51, min/max -100.0/12.9
Policy entropy: 2.288, Critic Loss: 0.065, Actor Loss 0.167

Updates 14590, training timesteps 1167280, FPS 473
Last 100 training episodes: mean/median reward -59.53/-69.83, min/max -100.0/6.6
Policy entropy: 2.291, Critic Loss: 0.158, Actor Loss 0.000

Updates 14600, training timesteps 1168080, FPS 473
Last 100 training episodes: mean/median reward -59.01/-66.34, min/max -100.0/6.3
Policy entropy: 2.289, Critic Loss: 0.174, Actor Loss -0.029

Updates 14610, training timesteps 1168880, FPS 473
Last 100 training episodes: mean/median reward -54.37/-69.83, min/max -100.0/6.3
Policy entropy: 2.285, Critic Loss: 0.170, Actor Loss -0.230

Updates 14620, training timesteps 1169680, FPS 473
Last 100 training episodes: mean/median reward -57.32/-69.83, min/max -100.0/3.8
Policy entropy: 2.282, Critic Loss: 0.149, Actor Loss -0.006

Updates 14630, training timesteps 1170480, FPS 473
Last 100 training episodes: mean/median reward -53.11/-56.88, min/max -100.0/3.4
Policy entropy: 2.285, Critic Loss: 0.132, Actor Loss -0.035

Updates 14640, training timesteps 1171280, FPS 473
Last 100 training episodes: mean/median reward -57.81/-69.83, min/max -100.0/4.6
Policy entropy: 2.286, Critic Loss: 0.138, Actor Loss -0.115

Updates 14650, training timesteps 1172080, FPS 473
Last 100 training episodes: mean/median reward -53.81/-63.11, min/max -100.0/3.2
Policy entropy: 2.288, Critic Loss: 0.110, Actor Loss 0.245

Updates 14660, training timesteps 1172880, FPS 473
Last 100 training episodes: mean/median reward -52.93/-61.45, min/max -100.0/5.4
Policy entropy: 2.288, Critic Loss: 0.153, Actor Loss 0.362

Updates 14670, training timesteps 1173680, FPS 473
Last 100 training episodes: mean/median reward -54.92/-64.68, min/max -100.0/7.7
Policy entropy: 2.286, Critic Loss: 0.097, Actor Loss 0.042

Updates 14680, training timesteps 1174480, FPS 473
Last 100 training episodes: mean/median reward -55.88/-66.34, min/max -100.0/5.1
Policy entropy: 2.285, Critic Loss: 0.086, Actor Loss 0.088

Updates 14690, training timesteps 1175280, FPS 473
Last 100 training episodes: mean/median reward -53.00/-59.87, min/max -100.0/6.6
Policy entropy: 2.285, Critic Loss: 0.206, Actor Loss -0.343

Updates 14700, training timesteps 1176080, FPS 473
Last 100 training episodes: mean/median reward -57.36/-61.45, min/max -100.0/3.2
Policy entropy: 2.285, Critic Loss: 0.132, Actor Loss -0.114

Updates 14710, training timesteps 1176880, FPS 473
Last 100 training episodes: mean/median reward -56.96/-66.34, min/max -100.0/7.0
Policy entropy: 2.285, Critic Loss: 0.183, Actor Loss 0.271

Updates 14720, training timesteps 1177680, FPS 473
Last 100 training episodes: mean/median reward -55.61/-66.06, min/max -100.0/7.0
Policy entropy: 2.284, Critic Loss: 0.173, Actor Loss -0.198

Updates 14730, training timesteps 1178480, FPS 473
Last 100 training episodes: mean/median reward -51.65/-54.04, min/max -100.0/10.8
Policy entropy: 2.284, Critic Loss: 0.146, Actor Loss -0.270

Updates 14740, training timesteps 1179280, FPS 473
Last 100 training episodes: mean/median reward -56.54/-63.02, min/max -100.0/3.1
Policy entropy: 2.289, Critic Loss: 0.154, Actor Loss -0.318

Updates 14750, training timesteps 1180080, FPS 473
Last 100 training episodes: mean/median reward -54.66/-63.02, min/max -100.0/8.6
Policy entropy: 2.289, Critic Loss: 0.094, Actor Loss 0.259

Updates 14760, training timesteps 1180880, FPS 473
Last 100 training episodes: mean/median reward -55.30/-63.02, min/max -100.0/6.3
Policy entropy: 2.289, Critic Loss: 0.146, Actor Loss 0.113

Updates 14770, training timesteps 1181680, FPS 473
Last 100 training episodes: mean/median reward -54.17/-63.02, min/max -100.0/6.7
Policy entropy: 2.287, Critic Loss: 0.162, Actor Loss -0.352

Updates 14780, training timesteps 1182480, FPS 473
Last 100 training episodes: mean/median reward -57.32/-66.34, min/max -100.0/14.9
Policy entropy: 2.289, Critic Loss: 0.151, Actor Loss -0.320

Updates 14790, training timesteps 1183280, FPS 473
Last 100 training episodes: mean/median reward -55.29/-69.83, min/max -100.0/4.4
Policy entropy: 2.287, Critic Loss: 0.135, Actor Loss 0.336

Updates 14800, training timesteps 1184080, FPS 473
Last 100 training episodes: mean/median reward -57.66/-69.83, min/max -100.0/6.0
Policy entropy: 2.288, Critic Loss: 0.078, Actor Loss 0.016

Updates 14810, training timesteps 1184880, FPS 473
Last 100 training episodes: mean/median reward -48.64/-56.88, min/max -100.0/6.3
Policy entropy: 2.289, Critic Loss: 0.133, Actor Loss -0.336

Updates 14820, training timesteps 1185680, FPS 473
Last 100 training episodes: mean/median reward -51.17/-63.27, min/max -100.0/6.3
Policy entropy: 2.286, Critic Loss: 0.130, Actor Loss -0.157

Updates 14830, training timesteps 1186480, FPS 473
Last 100 training episodes: mean/median reward -54.64/-66.34, min/max -100.0/8.6
Policy entropy: 2.288, Critic Loss: 0.143, Actor Loss 0.521

Updates 14840, training timesteps 1187280, FPS 473
Last 100 training episodes: mean/median reward -46.92/-55.82, min/max -100.0/8.6
Policy entropy: 2.288, Critic Loss: 0.111, Actor Loss -0.072

Updates 14850, training timesteps 1188080, FPS 473
Last 100 training episodes: mean/median reward -52.71/-59.87, min/max -100.0/3.6
Policy entropy: 2.287, Critic Loss: 0.115, Actor Loss 0.142

Updates 14860, training timesteps 1188880, FPS 473
Last 100 training episodes: mean/median reward -54.11/-63.02, min/max -100.0/7.0
Policy entropy: 2.286, Critic Loss: 0.127, Actor Loss 0.058

Updates 14870, training timesteps 1189680, FPS 473
Last 100 training episodes: mean/median reward -57.72/-68.09, min/max -100.0/7.0
Policy entropy: 2.290, Critic Loss: 0.121, Actor Loss -0.320

Updates 14880, training timesteps 1190480, FPS 473
Last 100 training episodes: mean/median reward -56.58/-66.34, min/max -100.0/5.4
Policy entropy: 2.289, Critic Loss: 0.096, Actor Loss 0.211

Updates 14890, training timesteps 1191280, FPS 473
Last 100 training episodes: mean/median reward -48.59/-58.38, min/max -100.0/8.8
Policy entropy: 2.287, Critic Loss: 0.150, Actor Loss -0.277

Updates 14900, training timesteps 1192080, FPS 473
Last 100 training episodes: mean/median reward -54.57/-66.34, min/max -100.0/5.1
Policy entropy: 2.287, Critic Loss: 0.128, Actor Loss 0.094

Updates 14910, training timesteps 1192880, FPS 473
Last 100 training episodes: mean/median reward -50.39/-54.04, min/max -100.0/5.1
Policy entropy: 2.289, Critic Loss: 0.076, Actor Loss 0.196

Updates 14920, training timesteps 1193680, FPS 473
Last 100 training episodes: mean/median reward -46.56/-53.28, min/max -100.0/2.0
Policy entropy: 2.288, Critic Loss: 0.273, Actor Loss -0.510

Updates 14930, training timesteps 1194480, FPS 473
Last 100 training episodes: mean/median reward -53.23/-63.02, min/max -100.0/2.4
Policy entropy: 2.288, Critic Loss: 0.132, Actor Loss 0.098

Updates 14940, training timesteps 1195280, FPS 473
Last 100 training episodes: mean/median reward -59.60/-69.83, min/max -100.0/3.2
Policy entropy: 2.288, Critic Loss: 0.088, Actor Loss 0.043

Updates 14950, training timesteps 1196080, FPS 473
Last 100 training episodes: mean/median reward -49.90/-56.88, min/max -100.0/6.6
Policy entropy: 2.288, Critic Loss: 0.142, Actor Loss 0.188

Updates 14960, training timesteps 1196880, FPS 473
Last 100 training episodes: mean/median reward -57.07/-66.34, min/max -100.0/6.6
Policy entropy: 2.289, Critic Loss: 0.281, Actor Loss -0.558

Updates 14970, training timesteps 1197680, FPS 473
Last 100 training episodes: mean/median reward -54.65/-68.09, min/max -100.0/4.4
Policy entropy: 2.287, Critic Loss: 0.139, Actor Loss -0.656

Updates 14980, training timesteps 1198480, FPS 473
Last 100 training episodes: mean/median reward -55.03/-66.34, min/max -100.0/4.2
Policy entropy: 2.287, Critic Loss: 0.176, Actor Loss -0.234

Updates 14990, training timesteps 1199280, FPS 473
Last 100 training episodes: mean/median reward -52.82/-59.87, min/max -100.0/14.1
Policy entropy: 2.289, Critic Loss: 0.184, Actor Loss -0.230

Updates 15000, training timesteps 1200080, FPS 473
Last 100 training episodes: mean/median reward -51.79/-60.34, min/max -100.0/14.1
Policy entropy: 2.285, Critic Loss: 0.198, Actor Loss -0.626

Updates 15010, training timesteps 1200880, FPS 473
Last 100 training episodes: mean/median reward -58.74/-66.34, min/max -100.0/4.4
Policy entropy: 2.287, Critic Loss: 0.116, Actor Loss 0.283

Updates 15020, training timesteps 1201680, FPS 473
Last 100 training episodes: mean/median reward -53.51/-64.68, min/max -100.0/3.2
Policy entropy: 2.290, Critic Loss: 0.137, Actor Loss -0.383

Updates 15030, training timesteps 1202480, FPS 473
Last 100 training episodes: mean/median reward -52.74/-63.02, min/max -100.0/6.6
Policy entropy: 2.285, Critic Loss: 0.261, Actor Loss -0.479

Updates 15040, training timesteps 1203280, FPS 473
Last 100 training episodes: mean/median reward -44.39/-46.33, min/max -100.0/5.6
Policy entropy: 2.285, Critic Loss: 0.141, Actor Loss 0.466

Updates 15050, training timesteps 1204080, FPS 473
Last 100 training episodes: mean/median reward -60.09/-70.52, min/max -100.0/0.9
Policy entropy: 2.288, Critic Loss: 0.049, Actor Loss 0.026

Updates 15060, training timesteps 1204880, FPS 473
Last 100 training episodes: mean/median reward -48.21/-57.54, min/max -100.0/4.4
Policy entropy: 2.287, Critic Loss: 0.156, Actor Loss 0.016

Updates 15070, training timesteps 1205680, FPS 473
Last 100 training episodes: mean/median reward -54.69/-66.34, min/max -100.0/7.4
Policy entropy: 2.288, Critic Loss: 0.117, Actor Loss 0.041

Updates 15080, training timesteps 1206480, FPS 473
Last 100 training episodes: mean/median reward -46.60/-45.68, min/max -100.0/6.6
Policy entropy: 2.287, Critic Loss: 0.257, Actor Loss -0.250

Updates 15090, training timesteps 1207280, FPS 473
Last 100 training episodes: mean/median reward -50.82/-58.38, min/max -100.0/7.5
Policy entropy: 2.286, Critic Loss: 0.181, Actor Loss -0.160

Updates 15100, training timesteps 1208080, FPS 473
Last 100 training episodes: mean/median reward -54.84/-63.02, min/max -100.0/6.0
Policy entropy: 2.288, Critic Loss: 0.134, Actor Loss 0.270

Updates 15110, training timesteps 1208880, FPS 473
Last 100 training episodes: mean/median reward -59.85/-69.83, min/max -100.0/3.8
Policy entropy: 2.290, Critic Loss: 0.125, Actor Loss -0.043

Updates 15120, training timesteps 1209680, FPS 473
Last 100 training episodes: mean/median reward -52.85/-66.34, min/max -100.0/9.9
Policy entropy: 2.286, Critic Loss: 0.110, Actor Loss 0.237

Updates 15130, training timesteps 1210480, FPS 473
Last 100 training episodes: mean/median reward -47.38/-54.04, min/max -100.0/13.0
Policy entropy: 2.287, Critic Loss: 0.130, Actor Loss 0.144

Updates 15140, training timesteps 1211280, FPS 473
Last 100 training episodes: mean/median reward -47.25/-59.04, min/max -100.0/5.1
Policy entropy: 2.286, Critic Loss: 0.170, Actor Loss -0.096

Updates 15150, training timesteps 1212080, FPS 473
Last 100 training episodes: mean/median reward -53.43/-59.87, min/max -100.0/10.2
Policy entropy: 2.287, Critic Loss: 0.068, Actor Loss -0.088

Updates 15160, training timesteps 1212880, FPS 473
Last 100 training episodes: mean/median reward -55.21/-69.83, min/max -100.0/5.1
Policy entropy: 2.289, Critic Loss: 0.154, Actor Loss -0.377

Updates 15170, training timesteps 1213680, FPS 473
Last 100 training episodes: mean/median reward -53.99/-58.38, min/max -100.0/6.6
Policy entropy: 2.289, Critic Loss: 0.145, Actor Loss 0.018

Updates 15180, training timesteps 1214480, FPS 473
Last 100 training episodes: mean/median reward -52.65/-56.88, min/max -100.0/5.4
Policy entropy: 2.287, Critic Loss: 0.164, Actor Loss 0.160

Updates 15190, training timesteps 1215280, FPS 473
Last 100 training episodes: mean/median reward -51.64/-61.45, min/max -100.0/7.7
Policy entropy: 2.287, Critic Loss: 0.145, Actor Loss -0.366

Updates 15200, training timesteps 1216080, FPS 473
Last 100 training episodes: mean/median reward -57.04/-68.09, min/max -100.0/11.7
Policy entropy: 2.287, Critic Loss: 0.077, Actor Loss 0.134

Updates 15210, training timesteps 1216880, FPS 473
Last 100 training episodes: mean/median reward -46.47/-52.69, min/max -100.0/5.4
Policy entropy: 2.288, Critic Loss: 0.178, Actor Loss -0.348

Updates 15220, training timesteps 1217680, FPS 473
Last 100 training episodes: mean/median reward -56.15/-66.34, min/max -100.0/6.0
Policy entropy: 2.287, Critic Loss: 0.095, Actor Loss 0.158

Updates 15230, training timesteps 1218480, FPS 473
Last 100 training episodes: mean/median reward -50.33/-59.87, min/max -100.0/7.0
Policy entropy: 2.287, Critic Loss: 0.242, Actor Loss -0.477

Updates 15240, training timesteps 1219280, FPS 473
Last 100 training episodes: mean/median reward -53.17/-58.38, min/max -100.0/5.4
Policy entropy: 2.288, Critic Loss: 0.124, Actor Loss 0.068

Updates 15250, training timesteps 1220080, FPS 473
Last 100 training episodes: mean/median reward -51.85/-59.87, min/max -100.0/7.7
Policy entropy: 2.283, Critic Loss: 0.211, Actor Loss -0.341

Updates 15260, training timesteps 1220880, FPS 473
Last 100 training episodes: mean/median reward -57.55/-69.83, min/max -100.0/4.6
Policy entropy: 2.285, Critic Loss: 0.276, Actor Loss -0.404

Updates 15270, training timesteps 1221680, FPS 473
Last 100 training episodes: mean/median reward -51.98/-66.34, min/max -100.0/9.8
Policy entropy: 2.289, Critic Loss: 0.153, Actor Loss -0.573

Updates 15280, training timesteps 1222480, FPS 473
Last 100 training episodes: mean/median reward -55.18/-66.34, min/max -100.0/7.7
Policy entropy: 2.289, Critic Loss: 0.172, Actor Loss -0.074

Updates 15290, training timesteps 1223280, FPS 473
Last 100 training episodes: mean/median reward -55.13/-64.68, min/max -100.0/7.7
Policy entropy: 2.285, Critic Loss: 0.174, Actor Loss -0.281

Updates 15300, training timesteps 1224080, FPS 473
Last 100 training episodes: mean/median reward -53.83/-66.34, min/max -100.0/7.0
Policy entropy: 2.290, Critic Loss: 0.193, Actor Loss -0.531

Updates 15310, training timesteps 1224880, FPS 473
Last 100 training episodes: mean/median reward -50.09/-58.38, min/max -100.0/5.7
Policy entropy: 2.287, Critic Loss: 0.093, Actor Loss 0.138

Updates 15320, training timesteps 1225680, FPS 473
Last 100 training episodes: mean/median reward -51.21/-56.88, min/max -100.0/6.3
Policy entropy: 2.287, Critic Loss: 0.145, Actor Loss -0.102

Updates 15330, training timesteps 1226480, FPS 473
Last 100 training episodes: mean/median reward -52.04/-63.02, min/max -100.0/7.5
Policy entropy: 2.290, Critic Loss: 0.198, Actor Loss -0.339

Updates 15340, training timesteps 1227280, FPS 473
Last 100 training episodes: mean/median reward -55.18/-69.83, min/max -100.0/2.5
Policy entropy: 2.290, Critic Loss: 0.081, Actor Loss 0.150

Updates 15350, training timesteps 1228080, FPS 473
Last 100 training episodes: mean/median reward -58.56/-70.50, min/max -100.0/5.1
Policy entropy: 2.290, Critic Loss: 0.072, Actor Loss 0.039

Updates 15360, training timesteps 1228880, FPS 473
Last 100 training episodes: mean/median reward -55.39/-68.09, min/max -100.0/7.4
Policy entropy: 2.290, Critic Loss: 0.267, Actor Loss -0.662

Updates 15370, training timesteps 1229680, FPS 473
Last 100 training episodes: mean/median reward -63.04/-77.38, min/max -100.0/3.1
Policy entropy: 2.288, Critic Loss: 0.129, Actor Loss -0.292

Updates 15380, training timesteps 1230480, FPS 473
Last 100 training episodes: mean/median reward -53.86/-66.34, min/max -100.0/4.6
Policy entropy: 2.288, Critic Loss: 0.166, Actor Loss -0.081

Updates 15390, training timesteps 1231280, FPS 473
Last 100 training episodes: mean/median reward -51.54/-62.60, min/max -100.0/8.1
Policy entropy: 2.292, Critic Loss: 0.284, Actor Loss -0.888

Updates 15400, training timesteps 1232080, FPS 473
Last 100 training episodes: mean/median reward -54.54/-63.02, min/max -100.0/1.1
Policy entropy: 2.290, Critic Loss: 0.244, Actor Loss -0.509

Updates 15410, training timesteps 1232880, FPS 473
Last 100 training episodes: mean/median reward -58.03/-69.83, min/max -100.0/4.1
Policy entropy: 2.290, Critic Loss: 0.102, Actor Loss 0.356

Updates 15420, training timesteps 1233680, FPS 473
Last 100 training episodes: mean/median reward -55.14/-63.02, min/max -100.0/5.1
Policy entropy: 2.286, Critic Loss: 0.259, Actor Loss -0.492

Updates 15430, training timesteps 1234480, FPS 473
Last 100 training episodes: mean/median reward -58.77/-66.34, min/max -100.0/3.2
Policy entropy: 2.287, Critic Loss: 0.127, Actor Loss 0.613

Updates 15440, training timesteps 1235280, FPS 474
Last 100 training episodes: mean/median reward -51.64/-63.11, min/max -100.0/4.0
Policy entropy: 2.289, Critic Loss: 0.122, Actor Loss -0.265

Updates 15450, training timesteps 1236080, FPS 474
Last 100 training episodes: mean/median reward -61.98/-71.67, min/max -100.0/7.4
Policy entropy: 2.289, Critic Loss: 0.060, Actor Loss 0.182

Updates 15460, training timesteps 1236880, FPS 474
Last 100 training episodes: mean/median reward -57.06/-69.83, min/max -100.0/13.3
Policy entropy: 2.287, Critic Loss: 0.111, Actor Loss 0.029

Updates 15470, training timesteps 1237680, FPS 474
Last 100 training episodes: mean/median reward -54.07/-63.02, min/max -100.0/6.0
Policy entropy: 2.287, Critic Loss: 0.159, Actor Loss -0.243

Updates 15480, training timesteps 1238480, FPS 474
Last 100 training episodes: mean/median reward -50.12/-64.40, min/max -100.0/6.5
Policy entropy: 2.283, Critic Loss: 0.054, Actor Loss 0.168

Updates 15490, training timesteps 1239280, FPS 474
Last 100 training episodes: mean/median reward -54.54/-63.52, min/max -100.0/5.7
Policy entropy: 2.287, Critic Loss: 0.092, Actor Loss -0.306

Updates 15500, training timesteps 1240080, FPS 474
Last 100 training episodes: mean/median reward -61.13/-68.09, min/max -100.0/5.4
Policy entropy: 2.288, Critic Loss: 0.126, Actor Loss 0.130

Updates 15510, training timesteps 1240880, FPS 474
Last 100 training episodes: mean/median reward -51.11/-59.87, min/max -100.0/4.2
Policy entropy: 2.290, Critic Loss: 0.111, Actor Loss 0.316

Updates 15520, training timesteps 1241680, FPS 474
Last 100 training episodes: mean/median reward -53.08/-59.87, min/max -100.0/4.2
Policy entropy: 2.286, Critic Loss: 0.113, Actor Loss 0.084

Updates 15530, training timesteps 1242480, FPS 474
Last 100 training episodes: mean/median reward -55.51/-64.68, min/max -100.0/8.9
Policy entropy: 2.284, Critic Loss: 0.275, Actor Loss -0.305

Updates 15540, training timesteps 1243280, FPS 474
Last 100 training episodes: mean/median reward -56.69/-69.83, min/max -100.0/4.2
Policy entropy: 2.285, Critic Loss: 0.196, Actor Loss -0.390

Updates 15550, training timesteps 1244080, FPS 474
Last 100 training episodes: mean/median reward -45.27/-58.38, min/max -100.0/8.1
Policy entropy: 2.286, Critic Loss: 0.326, Actor Loss -0.915

Updates 15560, training timesteps 1244880, FPS 474
Last 100 training episodes: mean/median reward -48.68/-57.74, min/max -100.0/5.1
Policy entropy: 2.285, Critic Loss: 0.093, Actor Loss 0.322

Updates 15570, training timesteps 1245680, FPS 474
Last 100 training episodes: mean/median reward -52.18/-59.87, min/max -100.0/4.4
Policy entropy: 2.287, Critic Loss: 0.105, Actor Loss -0.016

Updates 15580, training timesteps 1246480, FPS 474
Last 100 training episodes: mean/median reward -48.04/-57.42, min/max -100.0/5.7
Policy entropy: 2.286, Critic Loss: 0.242, Actor Loss -0.169

Updates 15590, training timesteps 1247280, FPS 474
Last 100 training episodes: mean/median reward -54.77/-66.34, min/max -100.0/11.8
Policy entropy: 2.290, Critic Loss: 0.124, Actor Loss -0.003

Updates 15600, training timesteps 1248080, FPS 474
Last 100 training episodes: mean/median reward -58.68/-73.51, min/max -100.0/4.9
Policy entropy: 2.290, Critic Loss: 0.126, Actor Loss -0.230

Updates 15610, training timesteps 1248880, FPS 474
Last 100 training episodes: mean/median reward -52.94/-63.02, min/max -100.0/4.2
Policy entropy: 2.289, Critic Loss: 0.130, Actor Loss -0.146

Updates 15620, training timesteps 1249680, FPS 474
Last 100 training episodes: mean/median reward -57.12/-63.02, min/max -100.0/2.5
Policy entropy: 2.291, Critic Loss: 0.153, Actor Loss -0.306

Updates 15630, training timesteps 1250480, FPS 474
Last 100 training episodes: mean/median reward -53.86/-63.02, min/max -100.0/5.9
Policy entropy: 2.289, Critic Loss: 0.118, Actor Loss 0.059

Updates 15640, training timesteps 1251280, FPS 474
Last 100 training episodes: mean/median reward -57.89/-66.34, min/max -100.0/3.8
Policy entropy: 2.291, Critic Loss: 0.126, Actor Loss 0.064

Updates 15650, training timesteps 1252080, FPS 474
Last 100 training episodes: mean/median reward -52.37/-58.38, min/max -100.0/6.0
Policy entropy: 2.289, Critic Loss: 0.074, Actor Loss 0.027

Updates 15660, training timesteps 1252880, FPS 474
Last 100 training episodes: mean/median reward -52.45/-59.87, min/max -100.0/7.0
Policy entropy: 2.287, Critic Loss: 0.108, Actor Loss -0.166

Updates 15670, training timesteps 1253680, FPS 474
Last 100 training episodes: mean/median reward -56.38/-73.51, min/max -100.0/5.7
Policy entropy: 2.288, Critic Loss: 0.130, Actor Loss 0.245

Updates 15680, training timesteps 1254480, FPS 474
Last 100 training episodes: mean/median reward -51.56/-59.87, min/max -100.0/7.3
Policy entropy: 2.287, Critic Loss: 0.144, Actor Loss -0.136

Updates 15690, training timesteps 1255280, FPS 474
Last 100 training episodes: mean/median reward -57.83/-66.34, min/max -100.0/0.0
Policy entropy: 2.286, Critic Loss: 0.158, Actor Loss -0.133

Updates 15700, training timesteps 1256080, FPS 474
Last 100 training episodes: mean/median reward -51.34/-58.38, min/max -100.0/4.0
Policy entropy: 2.287, Critic Loss: 0.181, Actor Loss 0.142

Updates 15710, training timesteps 1256880, FPS 474
Last 100 training episodes: mean/median reward -51.72/-68.09, min/max -100.0/6.3
Policy entropy: 2.288, Critic Loss: 0.213, Actor Loss -0.446

Updates 15720, training timesteps 1257680, FPS 474
Last 100 training episodes: mean/median reward -48.32/-54.54, min/max -100.0/7.4
Policy entropy: 2.288, Critic Loss: 0.099, Actor Loss 0.222

Updates 15730, training timesteps 1258480, FPS 474
Last 100 training episodes: mean/median reward -49.18/-48.77, min/max -100.0/0.0
Policy entropy: 2.289, Critic Loss: 0.149, Actor Loss -0.160

Updates 15740, training timesteps 1259280, FPS 474
Last 100 training episodes: mean/median reward -52.83/-66.34, min/max -100.0/4.4
Policy entropy: 2.289, Critic Loss: 0.072, Actor Loss 0.018

Updates 15750, training timesteps 1260080, FPS 474
Last 100 training episodes: mean/median reward -48.71/-58.38, min/max -100.0/13.5
Policy entropy: 2.287, Critic Loss: 0.089, Actor Loss 0.016

Updates 15760, training timesteps 1260880, FPS 474
Last 100 training episodes: mean/median reward -57.51/-68.09, min/max -100.0/1.0
Policy entropy: 2.289, Critic Loss: 0.105, Actor Loss 0.067

Updates 15770, training timesteps 1261680, FPS 474
Last 100 training episodes: mean/median reward -57.80/-66.34, min/max -100.0/4.2
Policy entropy: 2.290, Critic Loss: 0.167, Actor Loss -0.470

Updates 15780, training timesteps 1262480, FPS 474
Last 100 training episodes: mean/median reward -52.86/-59.28, min/max -100.0/5.4
Policy entropy: 2.288, Critic Loss: 0.112, Actor Loss 0.222

Updates 15790, training timesteps 1263280, FPS 474
Last 100 training episodes: mean/median reward -51.10/-54.04, min/max -100.0/4.6
Policy entropy: 2.288, Critic Loss: 0.042, Actor Loss 0.353

Updates 15800, training timesteps 1264080, FPS 474
Last 100 training episodes: mean/median reward -46.96/-52.48, min/max -100.0/5.4
Policy entropy: 2.291, Critic Loss: 0.110, Actor Loss 0.239

Updates 15810, training timesteps 1264880, FPS 474
Last 100 training episodes: mean/median reward -49.86/-61.45, min/max -100.0/5.4
Policy entropy: 2.290, Critic Loss: 0.166, Actor Loss -0.111

Updates 15820, training timesteps 1265680, FPS 474
Last 100 training episodes: mean/median reward -52.01/-64.68, min/max -100.0/5.9
Policy entropy: 2.291, Critic Loss: 0.106, Actor Loss 0.390

Updates 15830, training timesteps 1266480, FPS 474
Last 100 training episodes: mean/median reward -49.45/-52.69, min/max -100.0/7.0
Policy entropy: 2.289, Critic Loss: 0.114, Actor Loss -0.074

Updates 15840, training timesteps 1267280, FPS 474
Last 100 training episodes: mean/median reward -58.00/-70.94, min/max -100.0/7.0
Policy entropy: 2.289, Critic Loss: 0.133, Actor Loss -0.214

Updates 15850, training timesteps 1268080, FPS 474
Last 100 training episodes: mean/median reward -56.25/-69.83, min/max -100.0/5.4
Policy entropy: 2.287, Critic Loss: 0.109, Actor Loss 0.208

Updates 15860, training timesteps 1268880, FPS 474
Last 100 training episodes: mean/median reward -50.78/-61.61, min/max -100.0/5.4
Policy entropy: 2.287, Critic Loss: 0.224, Actor Loss -0.261

Updates 15870, training timesteps 1269680, FPS 474
Last 100 training episodes: mean/median reward -48.47/-59.87, min/max -100.0/8.4
Policy entropy: 2.286, Critic Loss: 0.149, Actor Loss -0.466

Updates 15880, training timesteps 1270480, FPS 474
Last 100 training episodes: mean/median reward -55.61/-66.34, min/max -100.0/11.1
Policy entropy: 2.288, Critic Loss: 0.098, Actor Loss 0.056

Updates 15890, training timesteps 1271280, FPS 474
Last 100 training episodes: mean/median reward -54.46/-66.34, min/max -100.0/6.3
Policy entropy: 2.289, Critic Loss: 0.105, Actor Loss 0.071

Updates 15900, training timesteps 1272080, FPS 474
Last 100 training episodes: mean/median reward -55.33/-61.45, min/max -100.0/5.2
Policy entropy: 2.289, Critic Loss: 0.093, Actor Loss -0.159

Updates 15910, training timesteps 1272880, FPS 474
Last 100 training episodes: mean/median reward -49.54/-59.87, min/max -100.0/15.5
Policy entropy: 2.287, Critic Loss: 0.150, Actor Loss -0.177

Updates 15920, training timesteps 1273680, FPS 474
Last 100 training episodes: mean/median reward -56.76/-66.34, min/max -100.0/2.6
Policy entropy: 2.288, Critic Loss: 0.098, Actor Loss -0.210

Updates 15930, training timesteps 1274480, FPS 474
Last 100 training episodes: mean/median reward -53.38/-68.09, min/max -100.0/6.0
Policy entropy: 2.289, Critic Loss: 0.109, Actor Loss -0.236

Updates 15940, training timesteps 1275280, FPS 474
Last 100 training episodes: mean/median reward -46.37/-50.05, min/max -100.0/12.4
Policy entropy: 2.291, Critic Loss: 0.155, Actor Loss -0.267

Updates 15950, training timesteps 1276080, FPS 474
Last 100 training episodes: mean/median reward -51.62/-59.87, min/max -100.0/4.0
Policy entropy: 2.289, Critic Loss: 0.088, Actor Loss -0.038

Updates 15960, training timesteps 1276880, FPS 474
Last 100 training episodes: mean/median reward -58.65/-71.67, min/max -100.0/6.6
Policy entropy: 2.289, Critic Loss: 0.153, Actor Loss -0.072

Updates 15970, training timesteps 1277680, FPS 474
Last 100 training episodes: mean/median reward -56.18/-61.45, min/max -100.0/2.1
Policy entropy: 2.289, Critic Loss: 0.133, Actor Loss -0.022

Updates 15980, training timesteps 1278480, FPS 474
Last 100 training episodes: mean/median reward -57.88/-69.83, min/max -100.0/6.2
Policy entropy: 2.287, Critic Loss: 0.075, Actor Loss -0.041

Updates 15990, training timesteps 1279280, FPS 474
Last 100 training episodes: mean/median reward -49.18/-55.46, min/max -100.0/6.2
Policy entropy: 2.288, Critic Loss: 0.252, Actor Loss -0.649

Updates 16000, training timesteps 1280080, FPS 474
Last 100 training episodes: mean/median reward -50.64/-59.87, min/max -100.0/4.9
Policy entropy: 2.287, Critic Loss: 0.202, Actor Loss -0.124

Updates 16010, training timesteps 1280880, FPS 474
Last 100 training episodes: mean/median reward -58.41/-69.83, min/max -100.0/4.0
Policy entropy: 2.283, Critic Loss: 0.069, Actor Loss 0.471

Updates 16020, training timesteps 1281680, FPS 474
Last 100 training episodes: mean/median reward -52.87/-66.34, min/max -100.0/6.3
Policy entropy: 2.288, Critic Loss: 0.169, Actor Loss -0.151

Updates 16030, training timesteps 1282480, FPS 474
Last 100 training episodes: mean/median reward -49.78/-56.88, min/max -100.0/2.0
Policy entropy: 2.289, Critic Loss: 0.114, Actor Loss -0.203

Updates 16040, training timesteps 1283280, FPS 474
Last 100 training episodes: mean/median reward -55.24/-66.34, min/max -100.0/4.4
Policy entropy: 2.289, Critic Loss: 0.151, Actor Loss 0.141

Updates 16050, training timesteps 1284080, FPS 474
Last 100 training episodes: mean/median reward -55.60/-66.34, min/max -100.0/5.4
Policy entropy: 2.288, Critic Loss: 0.188, Actor Loss -0.112

Updates 16060, training timesteps 1284880, FPS 474
Last 100 training episodes: mean/median reward -56.39/-69.83, min/max -100.0/6.2
Policy entropy: 2.286, Critic Loss: 0.137, Actor Loss -0.247

Updates 16070, training timesteps 1285680, FPS 474
Last 100 training episodes: mean/median reward -50.71/-61.45, min/max -100.0/0.0
Policy entropy: 2.289, Critic Loss: 0.114, Actor Loss 0.060

Updates 16080, training timesteps 1286480, FPS 474
Last 100 training episodes: mean/median reward -51.01/-54.04, min/max -100.0/4.9
Policy entropy: 2.291, Critic Loss: 0.187, Actor Loss -0.310

Updates 16090, training timesteps 1287280, FPS 474
Last 100 training episodes: mean/median reward -54.63/-61.45, min/max -100.0/4.2
Policy entropy: 2.289, Critic Loss: 0.226, Actor Loss -0.163

Updates 16100, training timesteps 1288080, FPS 474
Last 100 training episodes: mean/median reward -50.42/-61.45, min/max -100.0/6.6
Policy entropy: 2.287, Critic Loss: 0.214, Actor Loss -0.598

Updates 16110, training timesteps 1288880, FPS 474
Last 100 training episodes: mean/median reward -58.15/-69.83, min/max -100.0/2.1
Policy entropy: 2.287, Critic Loss: 0.085, Actor Loss 0.435

Updates 16120, training timesteps 1289680, FPS 474
Last 100 training episodes: mean/median reward -56.43/-64.68, min/max -100.0/4.0
Policy entropy: 2.288, Critic Loss: 0.062, Actor Loss 0.065

Updates 16130, training timesteps 1290480, FPS 474
Last 100 training episodes: mean/median reward -45.87/-49.91, min/max -100.0/4.2
Policy entropy: 2.291, Critic Loss: 0.122, Actor Loss -0.143

Updates 16140, training timesteps 1291280, FPS 474
Last 100 training episodes: mean/median reward -58.63/-69.53, min/max -100.0/4.2
Policy entropy: 2.287, Critic Loss: 0.108, Actor Loss 0.378

Updates 16150, training timesteps 1292080, FPS 474
Last 100 training episodes: mean/median reward -58.08/-73.51, min/max -100.0/8.1
Policy entropy: 2.289, Critic Loss: 0.104, Actor Loss 0.057

Updates 16160, training timesteps 1292880, FPS 474
Last 100 training episodes: mean/median reward -62.44/-77.38, min/max -100.0/5.1
Policy entropy: 2.286, Critic Loss: 0.085, Actor Loss 0.136

Updates 16170, training timesteps 1293680, FPS 474
Last 100 training episodes: mean/median reward -52.70/-59.87, min/max -100.0/7.4
Policy entropy: 2.287, Critic Loss: 0.324, Actor Loss -0.748

Updates 16180, training timesteps 1294480, FPS 474
Last 100 training episodes: mean/median reward -52.49/-59.87, min/max -100.0/7.0
Policy entropy: 2.287, Critic Loss: 0.064, Actor Loss 0.344

Updates 16190, training timesteps 1295280, FPS 474
Last 100 training episodes: mean/median reward -54.52/-63.02, min/max -100.0/5.7
Policy entropy: 2.285, Critic Loss: 0.128, Actor Loss -0.325

Updates 16200, training timesteps 1296080, FPS 474
Last 100 training episodes: mean/median reward -60.01/-69.83, min/max -100.0/3.6
Policy entropy: 2.286, Critic Loss: 0.167, Actor Loss 0.013

Updates 16210, training timesteps 1296880, FPS 474
Last 100 training episodes: mean/median reward -53.95/-66.34, min/max -100.0/5.6
Policy entropy: 2.287, Critic Loss: 0.223, Actor Loss -0.312

Updates 16220, training timesteps 1297680, FPS 474
Last 100 training episodes: mean/median reward -60.87/-75.44, min/max -100.0/4.0
Policy entropy: 2.290, Critic Loss: 0.186, Actor Loss -0.255

Updates 16230, training timesteps 1298480, FPS 474
Last 100 training episodes: mean/median reward -63.21/-77.38, min/max -100.0/5.4
Policy entropy: 2.290, Critic Loss: 0.077, Actor Loss 0.158

Updates 16240, training timesteps 1299280, FPS 474
Last 100 training episodes: mean/median reward -54.64/-62.73, min/max -100.0/5.1
Policy entropy: 2.289, Critic Loss: 0.089, Actor Loss 0.135

Updates 16250, training timesteps 1300080, FPS 474
Last 100 training episodes: mean/median reward -47.87/-59.87, min/max -100.0/5.1
Policy entropy: 2.287, Critic Loss: 0.141, Actor Loss 0.137

Updates 16260, training timesteps 1300880, FPS 474
Last 100 training episodes: mean/median reward -50.51/-63.02, min/max -100.0/5.1
Policy entropy: 2.286, Critic Loss: 0.060, Actor Loss 0.373

Updates 16270, training timesteps 1301680, FPS 474
Last 100 training episodes: mean/median reward -49.44/-59.87, min/max -100.0/12.3
Policy entropy: 2.287, Critic Loss: 0.090, Actor Loss 0.015

Updates 16280, training timesteps 1302480, FPS 474
Last 100 training episodes: mean/median reward -52.89/-63.02, min/max -100.0/3.6
Policy entropy: 2.285, Critic Loss: 0.169, Actor Loss -0.314

Updates 16290, training timesteps 1303280, FPS 474
Last 100 training episodes: mean/median reward -56.74/-65.68, min/max -100.0/4.4
Policy entropy: 2.282, Critic Loss: 0.138, Actor Loss -0.197

Updates 16300, training timesteps 1304080, FPS 474
Last 100 training episodes: mean/median reward -55.11/-69.83, min/max -100.0/5.7
Policy entropy: 2.287, Critic Loss: 0.080, Actor Loss 0.053

Updates 16310, training timesteps 1304880, FPS 474
Last 100 training episodes: mean/median reward -54.28/-63.02, min/max -100.0/6.6
Policy entropy: 2.288, Critic Loss: 0.077, Actor Loss -0.151

Updates 16320, training timesteps 1305680, FPS 474
Last 100 training episodes: mean/median reward -48.17/-51.33, min/max -100.0/5.1
Policy entropy: 2.286, Critic Loss: 0.141, Actor Loss -0.219

Updates 16330, training timesteps 1306480, FPS 474
Last 100 training episodes: mean/median reward -50.71/-63.02, min/max -100.0/13.7
Policy entropy: 2.285, Critic Loss: 0.096, Actor Loss 0.151

Updates 16340, training timesteps 1307280, FPS 474
Last 100 training episodes: mean/median reward -58.16/-71.67, min/max -100.0/3.1
Policy entropy: 2.287, Critic Loss: 0.085, Actor Loss -0.001

Updates 16350, training timesteps 1308080, FPS 474
Last 100 training episodes: mean/median reward -57.86/-71.67, min/max -100.0/4.4
Policy entropy: 2.290, Critic Loss: 0.202, Actor Loss -0.219

Updates 16360, training timesteps 1308880, FPS 474
Last 100 training episodes: mean/median reward -58.98/-73.51, min/max -100.0/3.1
Policy entropy: 2.286, Critic Loss: 0.110, Actor Loss -0.075

Updates 16370, training timesteps 1309680, FPS 474
Last 100 training episodes: mean/median reward -54.52/-69.83, min/max -100.0/8.8
Policy entropy: 2.286, Critic Loss: 0.091, Actor Loss 0.082

Updates 16380, training timesteps 1310480, FPS 474
Last 100 training episodes: mean/median reward -58.34/-69.83, min/max -100.0/4.4
Policy entropy: 2.284, Critic Loss: 0.215, Actor Loss -0.396

Updates 16390, training timesteps 1311280, FPS 474
Last 100 training episodes: mean/median reward -51.42/-56.88, min/max -100.0/7.8
Policy entropy: 2.284, Critic Loss: 0.080, Actor Loss -0.036

Updates 16400, training timesteps 1312080, FPS 474
Last 100 training episodes: mean/median reward -59.67/-75.44, min/max -100.0/2.6
Policy entropy: 2.287, Critic Loss: 0.104, Actor Loss 0.295

Updates 16410, training timesteps 1312880, FPS 474
Last 100 training episodes: mean/median reward -55.61/-61.45, min/max -100.0/0.0
Policy entropy: 2.287, Critic Loss: 0.091, Actor Loss 0.153

Updates 16420, training timesteps 1313680, FPS 474
Last 100 training episodes: mean/median reward -52.54/-56.88, min/max -100.0/1.9
Policy entropy: 2.289, Critic Loss: 0.089, Actor Loss -0.118

Updates 16430, training timesteps 1314480, FPS 474
Last 100 training episodes: mean/median reward -53.45/-56.88, min/max -100.0/5.1
Policy entropy: 2.287, Critic Loss: 0.105, Actor Loss 0.069

Updates 16440, training timesteps 1315280, FPS 474
Last 100 training episodes: mean/median reward -59.97/-71.67, min/max -100.0/8.2
Policy entropy: 2.287, Critic Loss: 0.089, Actor Loss -0.101

Updates 16450, training timesteps 1316080, FPS 474
Last 100 training episodes: mean/median reward -51.72/-59.87, min/max -100.0/9.8
Policy entropy: 2.286, Critic Loss: 0.109, Actor Loss 0.215

Updates 16460, training timesteps 1316880, FPS 474
Last 100 training episodes: mean/median reward -56.13/-68.09, min/max -100.0/6.3
Policy entropy: 2.286, Critic Loss: 0.114, Actor Loss -0.271

Updates 16470, training timesteps 1317680, FPS 474
Last 100 training episodes: mean/median reward -53.69/-66.34, min/max -100.0/7.0
Policy entropy: 2.282, Critic Loss: 0.126, Actor Loss 0.496

Updates 16480, training timesteps 1318480, FPS 474
Last 100 training episodes: mean/median reward -47.89/-55.88, min/max -100.0/6.0
Policy entropy: 2.286, Critic Loss: 0.116, Actor Loss -0.250

Updates 16490, training timesteps 1319280, FPS 474
Last 100 training episodes: mean/median reward -53.75/-63.02, min/max -100.0/2.9
Policy entropy: 2.288, Critic Loss: 0.250, Actor Loss -0.182

Updates 16500, training timesteps 1320080, FPS 474
Last 100 training episodes: mean/median reward -50.97/-58.53, min/max -100.0/8.0
Policy entropy: 2.286, Critic Loss: 0.076, Actor Loss 0.223

Updates 16510, training timesteps 1320880, FPS 474
Last 100 training episodes: mean/median reward -59.97/-66.34, min/max -100.0/6.0
Policy entropy: 2.286, Critic Loss: 0.125, Actor Loss 0.125

Updates 16520, training timesteps 1321680, FPS 474
Last 100 training episodes: mean/median reward -54.96/-59.87, min/max -100.0/5.7
Policy entropy: 2.288, Critic Loss: 0.140, Actor Loss -0.163

Updates 16530, training timesteps 1322480, FPS 474
Last 100 training episodes: mean/median reward -47.11/-55.46, min/max -100.0/7.1
Policy entropy: 2.288, Critic Loss: 0.124, Actor Loss -0.301

Updates 16540, training timesteps 1323280, FPS 474
Last 100 training episodes: mean/median reward -46.77/-52.69, min/max -100.0/6.3
Policy entropy: 2.290, Critic Loss: 0.184, Actor Loss -0.198

Updates 16550, training timesteps 1324080, FPS 474
Last 100 training episodes: mean/median reward -54.46/-60.30, min/max -100.0/11.0
Policy entropy: 2.287, Critic Loss: 0.268, Actor Loss -0.651

Updates 16560, training timesteps 1324880, FPS 474
Last 100 training episodes: mean/median reward -56.74/-66.34, min/max -100.0/11.0
Policy entropy: 2.285, Critic Loss: 0.075, Actor Loss 0.055

Updates 16570, training timesteps 1325680, FPS 474
Last 100 training episodes: mean/median reward -53.94/-63.02, min/max -100.0/6.0
Policy entropy: 2.285, Critic Loss: 0.121, Actor Loss 0.336

Updates 16580, training timesteps 1326480, FPS 474
Last 100 training episodes: mean/median reward -49.40/-56.88, min/max -100.0/8.6
Policy entropy: 2.285, Critic Loss: 0.142, Actor Loss -0.250

Updates 16590, training timesteps 1327280, FPS 474
Last 100 training episodes: mean/median reward -55.75/-61.45, min/max -100.0/5.1
Policy entropy: 2.285, Critic Loss: 0.051, Actor Loss 0.236

Updates 16600, training timesteps 1328080, FPS 474
Last 100 training episodes: mean/median reward -63.66/-81.45, min/max -100.0/4.0
Policy entropy: 2.286, Critic Loss: 0.222, Actor Loss -0.463

Updates 16610, training timesteps 1328880, FPS 474
Last 100 training episodes: mean/median reward -57.29/-69.83, min/max -100.0/7.4
Policy entropy: 2.289, Critic Loss: 0.324, Actor Loss -0.523

Updates 16620, training timesteps 1329680, FPS 474
Last 100 training episodes: mean/median reward -59.75/-69.83, min/max -100.0/4.2
Policy entropy: 2.289, Critic Loss: 0.143, Actor Loss -0.443

Updates 16630, training timesteps 1330480, FPS 474
Last 100 training episodes: mean/median reward -50.48/-64.68, min/max -100.0/6.6
Policy entropy: 2.286, Critic Loss: 0.146, Actor Loss -0.036

Updates 16640, training timesteps 1331280, FPS 474
Last 100 training episodes: mean/median reward -52.80/-63.02, min/max -100.0/8.6
Policy entropy: 2.286, Critic Loss: 0.122, Actor Loss -0.052

Updates 16650, training timesteps 1332080, FPS 474
Last 100 training episodes: mean/median reward -54.35/-69.83, min/max -100.0/4.3
Policy entropy: 2.287, Critic Loss: 0.148, Actor Loss -0.274

Updates 16660, training timesteps 1332880, FPS 474
Last 100 training episodes: mean/median reward -48.11/-59.87, min/max -100.0/5.7
Policy entropy: 2.286, Critic Loss: 0.139, Actor Loss 0.028

Updates 16670, training timesteps 1333680, FPS 474
Last 100 training episodes: mean/median reward -57.15/-63.02, min/max -100.0/5.0
Policy entropy: 2.288, Critic Loss: 0.267, Actor Loss -0.700

Updates 16680, training timesteps 1334480, FPS 474
Last 100 training episodes: mean/median reward -63.91/-77.38, min/max -100.0/3.2
Policy entropy: 2.285, Critic Loss: 0.060, Actor Loss 0.287

Updates 16690, training timesteps 1335280, FPS 474
Last 100 training episodes: mean/median reward -57.57/-63.02, min/max -100.0/3.8
Policy entropy: 2.289, Critic Loss: 0.120, Actor Loss -0.111

Updates 16700, training timesteps 1336080, FPS 474
Last 100 training episodes: mean/median reward -49.62/-58.38, min/max -100.0/6.6
Policy entropy: 2.290, Critic Loss: 0.164, Actor Loss -0.594

Updates 16710, training timesteps 1336880, FPS 474
Last 100 training episodes: mean/median reward -53.93/-68.09, min/max -100.0/4.6
Policy entropy: 2.287, Critic Loss: 0.187, Actor Loss -0.217

Updates 16720, training timesteps 1337680, FPS 474
Last 100 training episodes: mean/median reward -54.37/-64.68, min/max -100.0/3.6
Policy entropy: 2.287, Critic Loss: 0.074, Actor Loss 0.117

Updates 16730, training timesteps 1338480, FPS 474
Last 100 training episodes: mean/median reward -56.58/-69.83, min/max -100.0/6.6
Policy entropy: 2.288, Critic Loss: 0.255, Actor Loss -0.042

Updates 16740, training timesteps 1339280, FPS 474
Last 100 training episodes: mean/median reward -55.33/-63.02, min/max -100.0/4.3
Policy entropy: 2.287, Critic Loss: 0.095, Actor Loss -0.139

Updates 16750, training timesteps 1340080, FPS 474
Last 100 training episodes: mean/median reward -55.86/-66.34, min/max -100.0/4.2
Policy entropy: 2.287, Critic Loss: 0.106, Actor Loss -0.006

Updates 16760, training timesteps 1340880, FPS 474
Last 100 training episodes: mean/median reward -55.64/-64.68, min/max -100.0/4.6
Policy entropy: 2.288, Critic Loss: 0.142, Actor Loss -0.282

Updates 16770, training timesteps 1341680, FPS 474
Last 100 training episodes: mean/median reward -53.07/-58.38, min/max -100.0/4.6
Policy entropy: 2.289, Critic Loss: 0.081, Actor Loss 0.263

Updates 16780, training timesteps 1342480, FPS 474
Last 100 training episodes: mean/median reward -57.98/-69.83, min/max -100.0/5.4
Policy entropy: 2.287, Critic Loss: 0.161, Actor Loss -0.225

Updates 16790, training timesteps 1343280, FPS 474
Last 100 training episodes: mean/median reward -53.52/-66.34, min/max -100.0/5.1
Policy entropy: 2.287, Critic Loss: 0.147, Actor Loss -0.420

Updates 16800, training timesteps 1344080, FPS 474
Last 100 training episodes: mean/median reward -49.23/-59.87, min/max -100.0/5.7
Policy entropy: 2.283, Critic Loss: 0.176, Actor Loss -0.322

Updates 16810, training timesteps 1344880, FPS 474
Last 100 training episodes: mean/median reward -53.29/-63.02, min/max -100.0/6.6
Policy entropy: 2.284, Critic Loss: 0.121, Actor Loss 0.688

Updates 16820, training timesteps 1345680, FPS 474
Last 100 training episodes: mean/median reward -54.69/-61.45, min/max -100.0/8.6
Policy entropy: 2.282, Critic Loss: 0.189, Actor Loss 0.017

Updates 16830, training timesteps 1346480, FPS 474
Last 100 training episodes: mean/median reward -54.15/-59.87, min/max -100.0/5.1
Policy entropy: 2.281, Critic Loss: 0.252, Actor Loss -0.446

Updates 16840, training timesteps 1347280, FPS 474
Last 100 training episodes: mean/median reward -48.14/-52.69, min/max -100.0/5.7
Policy entropy: 2.288, Critic Loss: 0.073, Actor Loss 0.381

Updates 16850, training timesteps 1348080, FPS 474
Last 100 training episodes: mean/median reward -53.00/-63.02, min/max -100.0/7.4
Policy entropy: 2.285, Critic Loss: 0.109, Actor Loss 0.575

Updates 16860, training timesteps 1348880, FPS 474
Last 100 training episodes: mean/median reward -51.59/-63.02, min/max -100.0/5.4
Policy entropy: 2.288, Critic Loss: 0.354, Actor Loss -0.703

Updates 16870, training timesteps 1349680, FPS 474
Last 100 training episodes: mean/median reward -49.01/-56.88, min/max -100.0/4.2
Policy entropy: 2.288, Critic Loss: 0.107, Actor Loss -0.285

Updates 16880, training timesteps 1350480, FPS 474
Last 100 training episodes: mean/median reward -52.67/-63.02, min/max -100.0/5.8
Policy entropy: 2.286, Critic Loss: 0.095, Actor Loss 0.125

Updates 16890, training timesteps 1351280, FPS 474
Last 100 training episodes: mean/median reward -52.62/-64.68, min/max -100.0/4.2
Policy entropy: 2.288, Critic Loss: 0.213, Actor Loss -0.411

Updates 16900, training timesteps 1352080, FPS 474
Last 100 training episodes: mean/median reward -57.19/-69.83, min/max -100.0/9.8
Policy entropy: 2.288, Critic Loss: 0.163, Actor Loss -0.017

Updates 16910, training timesteps 1352880, FPS 474
Last 100 training episodes: mean/median reward -55.87/-66.34, min/max -100.0/5.1
Policy entropy: 2.289, Critic Loss: 0.071, Actor Loss 0.309

Updates 16920, training timesteps 1353680, FPS 474
Last 100 training episodes: mean/median reward -50.86/-63.02, min/max -100.0/5.2
Policy entropy: 2.289, Critic Loss: 0.197, Actor Loss -0.549

Updates 16930, training timesteps 1354480, FPS 474
Last 100 training episodes: mean/median reward -54.85/-68.09, min/max -100.0/7.0
Policy entropy: 2.290, Critic Loss: 0.107, Actor Loss -0.179

Updates 16940, training timesteps 1355280, FPS 474
Last 100 training episodes: mean/median reward -55.28/-66.34, min/max -100.0/8.8
Policy entropy: 2.287, Critic Loss: 0.205, Actor Loss -0.332

Updates 16950, training timesteps 1356080, FPS 474
Last 100 training episodes: mean/median reward -58.63/-69.83, min/max -100.0/5.4
Policy entropy: 2.290, Critic Loss: 0.091, Actor Loss 0.053

Updates 16960, training timesteps 1356880, FPS 474
Last 100 training episodes: mean/median reward -54.26/-68.09, min/max -100.0/6.7
Policy entropy: 2.285, Critic Loss: 0.109, Actor Loss 0.042

Updates 16970, training timesteps 1357680, FPS 474
Last 100 training episodes: mean/median reward -52.27/-58.38, min/max -100.0/5.1
Policy entropy: 2.284, Critic Loss: 0.174, Actor Loss 0.003

Updates 16980, training timesteps 1358480, FPS 474
Last 100 training episodes: mean/median reward -56.31/-68.09, min/max -100.0/14.6
Policy entropy: 2.285, Critic Loss: 0.175, Actor Loss 0.077

Updates 16990, training timesteps 1359280, FPS 474
Last 100 training episodes: mean/median reward -54.70/-63.02, min/max -100.0/14.6
Policy entropy: 2.286, Critic Loss: 0.168, Actor Loss -0.364

Updates 17000, training timesteps 1360080, FPS 474
Last 100 training episodes: mean/median reward -59.58/-69.83, min/max -100.0/3.6
Policy entropy: 2.289, Critic Loss: 0.078, Actor Loss -0.042

Updates 17010, training timesteps 1360880, FPS 474
Last 100 training episodes: mean/median reward -56.77/-61.45, min/max -100.0/3.2
Policy entropy: 2.288, Critic Loss: 0.131, Actor Loss -0.402

Updates 17020, training timesteps 1361680, FPS 474
Last 100 training episodes: mean/median reward -55.69/-59.87, min/max -100.0/3.4
Policy entropy: 2.287, Critic Loss: 0.047, Actor Loss 0.346

Updates 17030, training timesteps 1362480, FPS 474
Last 100 training episodes: mean/median reward -54.12/-59.87, min/max -100.0/4.9
Policy entropy: 2.289, Critic Loss: 0.187, Actor Loss 0.108

Updates 17040, training timesteps 1363280, FPS 474
Last 100 training episodes: mean/median reward -52.99/-59.87, min/max -100.0/4.9
Policy entropy: 2.288, Critic Loss: 0.089, Actor Loss 0.011

Updates 17050, training timesteps 1364080, FPS 474
Last 100 training episodes: mean/median reward -53.34/-56.88, min/max -100.0/5.7
Policy entropy: 2.288, Critic Loss: 0.256, Actor Loss -0.058

Updates 17060, training timesteps 1364880, FPS 474
Last 100 training episodes: mean/median reward -52.74/-63.02, min/max -100.0/5.7
Policy entropy: 2.286, Critic Loss: 0.097, Actor Loss 0.021

Updates 17070, training timesteps 1365680, FPS 474
Last 100 training episodes: mean/median reward -53.01/-63.02, min/max -100.0/4.2
Policy entropy: 2.287, Critic Loss: 0.158, Actor Loss 0.276

Updates 17080, training timesteps 1366480, FPS 474
Last 100 training episodes: mean/median reward -53.37/-66.34, min/max -100.0/3.1
Policy entropy: 2.288, Critic Loss: 0.058, Actor Loss 0.349

Updates 17090, training timesteps 1367280, FPS 474
Last 100 training episodes: mean/median reward -54.80/-63.02, min/max -100.0/6.0
Policy entropy: 2.285, Critic Loss: 0.182, Actor Loss 0.350

Updates 17100, training timesteps 1368080, FPS 474
Last 100 training episodes: mean/median reward -52.37/-57.57, min/max -100.0/5.1
Policy entropy: 2.287, Critic Loss: 0.140, Actor Loss -0.267

Updates 17110, training timesteps 1368880, FPS 474
Last 100 training episodes: mean/median reward -53.65/-58.45, min/max -100.0/4.4
Policy entropy: 2.286, Critic Loss: 0.054, Actor Loss 0.083

Updates 17120, training timesteps 1369680, FPS 474
Last 100 training episodes: mean/median reward -50.01/-63.02, min/max -100.0/4.9
Policy entropy: 2.287, Critic Loss: 0.093, Actor Loss 0.203

Updates 17130, training timesteps 1370480, FPS 474
Last 100 training episodes: mean/median reward -59.75/-71.67, min/max -100.0/3.1
Policy entropy: 2.288, Critic Loss: 0.085, Actor Loss 0.049

Updates 17140, training timesteps 1371280, FPS 474
Last 100 training episodes: mean/median reward -57.01/-69.83, min/max -100.0/4.9
Policy entropy: 2.287, Critic Loss: 0.078, Actor Loss 0.302

Updates 17150, training timesteps 1372080, FPS 474
Last 100 training episodes: mean/median reward -53.45/-66.34, min/max -100.0/0.9
Policy entropy: 2.289, Critic Loss: 0.104, Actor Loss 0.109

Updates 17160, training timesteps 1372880, FPS 474
Last 100 training episodes: mean/median reward -52.07/-59.87, min/max -100.0/13.2
Policy entropy: 2.289, Critic Loss: 0.087, Actor Loss -0.145

Updates 17170, training timesteps 1373680, FPS 474
Last 100 training episodes: mean/median reward -53.14/-61.45, min/max -100.0/6.0
Policy entropy: 2.286, Critic Loss: 0.099, Actor Loss 0.223

Updates 17180, training timesteps 1374480, FPS 474
Last 100 training episodes: mean/median reward -54.70/-66.34, min/max -100.0/6.3
Policy entropy: 2.288, Critic Loss: 0.164, Actor Loss -0.154

Updates 17190, training timesteps 1375280, FPS 474
Last 100 training episodes: mean/median reward -51.23/-57.32, min/max -100.0/12.1
Policy entropy: 2.287, Critic Loss: 0.191, Actor Loss 0.057

Updates 17200, training timesteps 1376080, FPS 474
Last 100 training episodes: mean/median reward -56.07/-71.67, min/max -100.0/12.1
Policy entropy: 2.287, Critic Loss: 0.336, Actor Loss -0.804

Updates 17210, training timesteps 1376880, FPS 474
Last 100 training episodes: mean/median reward -52.17/-66.34, min/max -100.0/7.7
Policy entropy: 2.285, Critic Loss: 0.132, Actor Loss 0.011

Updates 17220, training timesteps 1377680, FPS 474
Last 100 training episodes: mean/median reward -52.48/-63.02, min/max -100.0/5.1
Policy entropy: 2.287, Critic Loss: 0.096, Actor Loss 0.074

Updates 17230, training timesteps 1378480, FPS 474
Last 100 training episodes: mean/median reward -51.68/-61.61, min/max -100.0/4.5
Policy entropy: 2.286, Critic Loss: 0.083, Actor Loss -0.051

Updates 17240, training timesteps 1379280, FPS 474
Last 100 training episodes: mean/median reward -50.35/-61.45, min/max -100.0/7.0
Policy entropy: 2.288, Critic Loss: 0.124, Actor Loss -0.010

Updates 17250, training timesteps 1380080, FPS 474
Last 100 training episodes: mean/median reward -48.52/-50.05, min/max -100.0/7.0
Policy entropy: 2.287, Critic Loss: 0.075, Actor Loss -0.133

Updates 17260, training timesteps 1380880, FPS 474
Last 100 training episodes: mean/median reward -50.37/-54.04, min/max -100.0/6.6
Policy entropy: 2.281, Critic Loss: 0.130, Actor Loss -0.312

Updates 17270, training timesteps 1381680, FPS 474
Last 100 training episodes: mean/median reward -53.12/-63.02, min/max -100.0/4.6
Policy entropy: 2.287, Critic Loss: 0.100, Actor Loss -0.206

Updates 17280, training timesteps 1382480, FPS 474
Last 100 training episodes: mean/median reward -63.63/-73.51, min/max -100.0/5.7
Policy entropy: 2.288, Critic Loss: 0.096, Actor Loss 0.415

Updates 17290, training timesteps 1383280, FPS 474
Last 100 training episodes: mean/median reward -58.99/-75.44, min/max -100.0/7.4
Policy entropy: 2.287, Critic Loss: 0.107, Actor Loss 0.058

Updates 17300, training timesteps 1384080, FPS 474
Last 100 training episodes: mean/median reward -47.73/-50.05, min/max -100.0/5.4
Policy entropy: 2.289, Critic Loss: 0.114, Actor Loss 0.173

Updates 17310, training timesteps 1384880, FPS 474
Last 100 training episodes: mean/median reward -45.46/-51.33, min/max -100.0/6.0
Policy entropy: 2.289, Critic Loss: 0.126, Actor Loss -0.015

Updates 17320, training timesteps 1385680, FPS 474
Last 100 training episodes: mean/median reward -46.09/-50.05, min/max -100.0/4.9
Policy entropy: 2.288, Critic Loss: 0.251, Actor Loss -0.475

Updates 17330, training timesteps 1386480, FPS 474
Last 100 training episodes: mean/median reward -57.13/-64.68, min/max -100.0/5.7
Policy entropy: 2.287, Critic Loss: 0.126, Actor Loss -0.257

Updates 17340, training timesteps 1387280, FPS 474
Last 100 training episodes: mean/median reward -55.77/-64.68, min/max -100.0/14.8
Policy entropy: 2.288, Critic Loss: 0.185, Actor Loss -0.105

Updates 17350, training timesteps 1388080, FPS 474
Last 100 training episodes: mean/median reward -48.83/-56.88, min/max -100.0/12.4
Policy entropy: 2.288, Critic Loss: 0.108, Actor Loss -0.088

Updates 17360, training timesteps 1388880, FPS 474
Last 100 training episodes: mean/median reward -50.45/-58.38, min/max -100.0/9.6
Policy entropy: 2.286, Critic Loss: 0.122, Actor Loss 0.079

Updates 17370, training timesteps 1389680, FPS 474
Last 100 training episodes: mean/median reward -54.88/-66.34, min/max -100.0/9.8
Policy entropy: 2.289, Critic Loss: 0.192, Actor Loss -0.426

Updates 17380, training timesteps 1390480, FPS 474
Last 100 training episodes: mean/median reward -61.03/-73.51, min/max -100.0/3.4
Policy entropy: 2.288, Critic Loss: 0.128, Actor Loss 0.018

Updates 17390, training timesteps 1391280, FPS 474
Last 100 training episodes: mean/median reward -57.73/-69.83, min/max -100.0/4.6
Policy entropy: 2.289, Critic Loss: 0.286, Actor Loss -0.455

Updates 17400, training timesteps 1392080, FPS 474
Last 100 training episodes: mean/median reward -52.91/-61.45, min/max -100.0/15.1
Policy entropy: 2.289, Critic Loss: 0.110, Actor Loss 0.241

Updates 17410, training timesteps 1392880, FPS 474
Last 100 training episodes: mean/median reward -51.21/-58.38, min/max -100.0/11.1
Policy entropy: 2.290, Critic Loss: 0.180, Actor Loss 0.361

Updates 17420, training timesteps 1393680, FPS 474
Last 100 training episodes: mean/median reward -54.59/-62.95, min/max -100.0/11.1
Policy entropy: 2.289, Critic Loss: 0.219, Actor Loss -0.173

Updates 17430, training timesteps 1394480, FPS 474
Last 100 training episodes: mean/median reward -49.32/-55.46, min/max -100.0/7.4
Policy entropy: 2.290, Critic Loss: 0.149, Actor Loss -0.089

Updates 17440, training timesteps 1395280, FPS 474
Last 100 training episodes: mean/median reward -57.85/-69.83, min/max -100.0/7.4
Policy entropy: 2.286, Critic Loss: 0.118, Actor Loss 0.170

Updates 17450, training timesteps 1396080, FPS 474
Last 100 training episodes: mean/median reward -50.47/-54.11, min/max -100.0/7.4
Policy entropy: 2.278, Critic Loss: 0.138, Actor Loss 0.047

Updates 17460, training timesteps 1396880, FPS 474
Last 100 training episodes: mean/median reward -54.62/-64.68, min/max -100.0/6.6
Policy entropy: 2.286, Critic Loss: 0.087, Actor Loss 0.063

Updates 17470, training timesteps 1397680, FPS 474
Last 100 training episodes: mean/median reward -50.39/-59.87, min/max -100.0/9.6
Policy entropy: 2.288, Critic Loss: 0.317, Actor Loss -0.929

Updates 17480, training timesteps 1398480, FPS 474
Last 100 training episodes: mean/median reward -60.01/-68.09, min/max -100.0/6.6
Policy entropy: 2.284, Critic Loss: 0.199, Actor Loss -0.119

Updates 17490, training timesteps 1399280, FPS 474
Last 100 training episodes: mean/median reward -52.37/-55.46, min/max -100.0/4.9
Policy entropy: 2.287, Critic Loss: 0.205, Actor Loss 0.340

Updates 17500, training timesteps 1400080, FPS 474
Last 100 training episodes: mean/median reward -56.58/-66.34, min/max -100.0/6.0
Policy entropy: 2.288, Critic Loss: 0.122, Actor Loss 0.063

Updates 17510, training timesteps 1400880, FPS 474
Last 100 training episodes: mean/median reward -56.51/-64.68, min/max -100.0/4.6
Policy entropy: 2.287, Critic Loss: 0.081, Actor Loss 0.106

Updates 17520, training timesteps 1401680, FPS 474
Last 100 training episodes: mean/median reward -59.83/-69.83, min/max -100.0/11.2
Policy entropy: 2.287, Critic Loss: 0.076, Actor Loss -0.006

Updates 17530, training timesteps 1402480, FPS 474
Last 100 training episodes: mean/median reward -48.01/-50.05, min/max -100.0/11.2
Policy entropy: 2.291, Critic Loss: 0.110, Actor Loss -0.110

Updates 17540, training timesteps 1403280, FPS 474
Last 100 training episodes: mean/median reward -50.23/-56.88, min/max -100.0/6.2
Policy entropy: 2.288, Critic Loss: 0.180, Actor Loss -0.204

Updates 17550, training timesteps 1404080, FPS 474
Last 100 training episodes: mean/median reward -59.02/-69.83, min/max -100.0/3.4
Policy entropy: 2.290, Critic Loss: 0.179, Actor Loss -0.411

Updates 17560, training timesteps 1404880, FPS 474
Last 100 training episodes: mean/median reward -48.22/-59.87, min/max -100.0/6.0
Policy entropy: 2.287, Critic Loss: 0.130, Actor Loss -0.138

Updates 17570, training timesteps 1405680, FPS 474
Last 100 training episodes: mean/median reward -59.71/-71.67, min/max -100.0/6.4
Policy entropy: 2.288, Critic Loss: 0.179, Actor Loss -0.294

Updates 17580, training timesteps 1406480, FPS 474
Last 100 training episodes: mean/median reward -50.64/-61.45, min/max -100.0/16.5
Policy entropy: 2.287, Critic Loss: 0.156, Actor Loss -0.026

Updates 17590, training timesteps 1407280, FPS 474
Last 100 training episodes: mean/median reward -52.20/-64.68, min/max -100.0/4.5
Policy entropy: 2.287, Critic Loss: 0.172, Actor Loss -0.197

Updates 17600, training timesteps 1408080, FPS 474
Last 100 training episodes: mean/median reward -54.36/-66.34, min/max -100.0/14.6
Policy entropy: 2.288, Critic Loss: 0.098, Actor Loss 0.448

Updates 17610, training timesteps 1408880, FPS 474
Last 100 training episodes: mean/median reward -52.38/-66.34, min/max -100.0/14.6
Policy entropy: 2.287, Critic Loss: 0.052, Actor Loss 0.356

Updates 17620, training timesteps 1409680, FPS 474
Last 100 training episodes: mean/median reward -56.54/-66.34, min/max -100.0/6.3
Policy entropy: 2.289, Critic Loss: 0.103, Actor Loss -0.278

Updates 17630, training timesteps 1410480, FPS 474
Last 100 training episodes: mean/median reward -55.66/-66.34, min/max -100.0/4.4
Policy entropy: 2.291, Critic Loss: 0.305, Actor Loss -0.685

Updates 17640, training timesteps 1411280, FPS 474
Last 100 training episodes: mean/median reward -53.83/-63.02, min/max -100.0/5.1
Policy entropy: 2.288, Critic Loss: 0.125, Actor Loss -0.540

Updates 17650, training timesteps 1412080, FPS 474
Last 100 training episodes: mean/median reward -58.07/-66.34, min/max -100.0/7.4
Policy entropy: 2.289, Critic Loss: 0.133, Actor Loss -0.240

Updates 17660, training timesteps 1412880, FPS 474
Last 100 training episodes: mean/median reward -55.63/-68.09, min/max -100.0/11.3
Policy entropy: 2.288, Critic Loss: 0.123, Actor Loss -0.096

Updates 17670, training timesteps 1413680, FPS 474
Last 100 training episodes: mean/median reward -57.30/-71.67, min/max -100.0/6.3
Policy entropy: 2.287, Critic Loss: 0.071, Actor Loss 0.140

Updates 17680, training timesteps 1414480, FPS 474
Last 100 training episodes: mean/median reward -53.07/-59.87, min/max -100.0/3.1
Policy entropy: 2.288, Critic Loss: 0.245, Actor Loss -0.632

Updates 17690, training timesteps 1415280, FPS 474
Last 100 training episodes: mean/median reward -57.10/-66.34, min/max -100.0/0.0
Policy entropy: 2.289, Critic Loss: 0.060, Actor Loss 0.079

Updates 17700, training timesteps 1416080, FPS 474
Last 100 training episodes: mean/median reward -54.66/-69.83, min/max -100.0/5.1
Policy entropy: 2.290, Critic Loss: 0.143, Actor Loss 0.162

Updates 17710, training timesteps 1416880, FPS 474
Last 100 training episodes: mean/median reward -52.35/-63.02, min/max -100.0/6.0
Policy entropy: 2.288, Critic Loss: 0.253, Actor Loss -0.483

Updates 17720, training timesteps 1417680, FPS 474
Last 100 training episodes: mean/median reward -49.57/-52.69, min/max -100.0/5.7
Policy entropy: 2.291, Critic Loss: 0.168, Actor Loss -0.593

Updates 17730, training timesteps 1418480, FPS 474
Last 100 training episodes: mean/median reward -54.78/-66.34, min/max -100.0/4.9
Policy entropy: 2.290, Critic Loss: 0.184, Actor Loss -0.006

Updates 17740, training timesteps 1419280, FPS 474
Last 100 training episodes: mean/median reward -47.02/-51.33, min/max -100.0/4.9
Policy entropy: 2.290, Critic Loss: 0.144, Actor Loss -0.234

Updates 17750, training timesteps 1420080, FPS 474
Last 100 training episodes: mean/median reward -55.52/-69.83, min/max -100.0/2.4
Policy entropy: 2.290, Critic Loss: 0.181, Actor Loss -0.721

Updates 17760, training timesteps 1420880, FPS 474
Last 100 training episodes: mean/median reward -51.11/-56.88, min/max -100.0/6.7
Policy entropy: 2.289, Critic Loss: 0.212, Actor Loss -0.248

Updates 17770, training timesteps 1421680, FPS 474
Last 100 training episodes: mean/median reward -51.02/-55.46, min/max -100.0/6.6
Policy entropy: 2.287, Critic Loss: 0.097, Actor Loss 0.122

Updates 17780, training timesteps 1422480, FPS 474
Last 100 training episodes: mean/median reward -50.90/-56.88, min/max -100.0/7.4
Policy entropy: 2.286, Critic Loss: 0.087, Actor Loss 0.189

Updates 17790, training timesteps 1423280, FPS 474
Last 100 training episodes: mean/median reward -51.76/-61.45, min/max -100.0/7.4
Policy entropy: 2.286, Critic Loss: 0.108, Actor Loss 0.111

Updates 17800, training timesteps 1424080, FPS 474
Last 100 training episodes: mean/median reward -58.95/-69.83, min/max -100.0/6.6
Policy entropy: 2.288, Critic Loss: 0.246, Actor Loss -0.438

Updates 17810, training timesteps 1424880, FPS 474
Last 100 training episodes: mean/median reward -50.61/-62.21, min/max -100.0/8.6
Policy entropy: 2.289, Critic Loss: 0.161, Actor Loss -0.214

Updates 17820, training timesteps 1425680, FPS 474
Last 100 training episodes: mean/median reward -54.50/-63.02, min/max -100.0/8.6
Policy entropy: 2.286, Critic Loss: 0.120, Actor Loss 0.100

Updates 17830, training timesteps 1426480, FPS 474
Last 100 training episodes: mean/median reward -56.16/-69.83, min/max -100.0/0.0
Policy entropy: 2.287, Critic Loss: 0.083, Actor Loss 0.363

Updates 17840, training timesteps 1427280, FPS 474
Last 100 training episodes: mean/median reward -60.87/-69.83, min/max -100.0/0.0
Policy entropy: 2.289, Critic Loss: 0.079, Actor Loss 0.100

Updates 17850, training timesteps 1428080, FPS 474
Last 100 training episodes: mean/median reward -56.28/-63.02, min/max -100.0/3.1
Policy entropy: 2.289, Critic Loss: 0.118, Actor Loss -0.296

Updates 17860, training timesteps 1428880, FPS 474
Last 100 training episodes: mean/median reward -52.98/-61.45, min/max -100.0/6.0
Policy entropy: 2.286, Critic Loss: 0.205, Actor Loss -0.402

Updates 17870, training timesteps 1429680, FPS 474
Last 100 training episodes: mean/median reward -50.33/-59.87, min/max -100.0/6.0
Policy entropy: 2.290, Critic Loss: 0.194, Actor Loss -0.376

Updates 17880, training timesteps 1430480, FPS 474
Last 100 training episodes: mean/median reward -51.45/-61.45, min/max -100.0/5.4
Policy entropy: 2.289, Critic Loss: 0.088, Actor Loss 0.045

Updates 17890, training timesteps 1431280, FPS 474
Last 100 training episodes: mean/median reward -54.09/-63.02, min/max -100.0/5.4
Policy entropy: 2.288, Critic Loss: 0.103, Actor Loss -0.023

Updates 17900, training timesteps 1432080, FPS 474
Last 100 training episodes: mean/median reward -50.37/-56.88, min/max -100.0/0.4
Policy entropy: 2.287, Critic Loss: 0.263, Actor Loss -0.510

